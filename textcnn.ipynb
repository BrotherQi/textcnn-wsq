{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "#import data_helpers\n",
    "#from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def listtext(path):\n",
    "    list_name = []\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.isdir(file_path):\n",
    "            listdir(file_path, list_name)\n",
    "        else:\n",
    "            list_name.append(file_path)\n",
    "    list_text = []\n",
    "    for files in list_name:\n",
    "        file = open(files,'r')\n",
    "        list_text.append(file.readlines()[0])\n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/md1/a503wangsiqi'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "\n",
    "path_neg = current_path + '/acllmdb/aclImdb/train/neg'\n",
    "path_pos = current_path + '/acllmdb/aclImdb/train/pos'\n",
    "#path_unsup = current_path + 'F:/acllmdb/aclImdb/train/unsup'\n",
    "\n",
    "pos_reviews = listtext(path_pos)\n",
    "neg_reviews = listtext(path_neg)\n",
    "#unsup_reviews = listtext(path_unsup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "#tf.flags.DEFINE_string(\"positive_data_file\", path_pos, \"Data source for the positive data.\")\n",
    "#tf.flags.DEFINE_string(\"negative_data_file\", path_neg, \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", True, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = load_data_and_labels(pos_reviews, neg_reviews)\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, y_train, vocab_processor, x_dev, y_dev\n",
    "\n",
    "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with tf.device('/cpu:0'), sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                \n",
    "                #with tf.device('/cpu:0'):\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            #batches_dev = data_helpers.batch_iter(\n",
    "            #    list(zip(x_dev, y_dev)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout 输入输出以及保留率\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional) 保存l2的正则损失\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer 初始化词向量\n",
    "        #with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)#？\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):#卷积层的定义 卷积核大小\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                #卷积核的四个维度分别是卷积单词个数 词向量的维度  输入层的维度 输出的卷积维度\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")#卷积核的初始化\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],#把卷积之后的多维度向量 池化到一个维度\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)#将全连接层加入l2正则化\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(pos_reviews, neg_reviews):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    '''\n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    '''\n",
    "    x_text = pos_reviews + neg_reviews\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in pos_reviews]\n",
    "    negative_labels = [[1, 0] for _ in neg_reviews]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocabulary Size: 80918\n",
      "Train/Dev split: 22500/2500\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /mnt/md1/a503wangsiqi/runs/1531388756\n",
      "\n",
      "2018-07-12T17:46:01.170226: step 1, loss 3.38377, acc 0.578125\n",
      "2018-07-12T17:46:02.776477: step 2, loss 2.78712, acc 0.5\n",
      "2018-07-12T17:46:04.435440: step 3, loss 3.32275, acc 0.46875\n",
      "2018-07-12T17:46:06.163252: step 4, loss 3.93689, acc 0.40625\n",
      "2018-07-12T17:46:07.802600: step 5, loss 2.63558, acc 0.453125\n",
      "2018-07-12T17:46:09.469681: step 6, loss 2.89114, acc 0.46875\n",
      "2018-07-12T17:46:11.077595: step 7, loss 2.47334, acc 0.484375\n",
      "2018-07-12T17:46:12.647650: step 8, loss 2.37013, acc 0.578125\n",
      "2018-07-12T17:46:14.258295: step 9, loss 2.59968, acc 0.59375\n",
      "2018-07-12T17:46:15.932547: step 10, loss 2.44276, acc 0.5625\n",
      "2018-07-12T17:46:17.539364: step 11, loss 2.40714, acc 0.53125\n",
      "2018-07-12T17:46:19.153440: step 12, loss 2.76499, acc 0.5\n",
      "2018-07-12T17:46:20.775634: step 13, loss 2.98999, acc 0.46875\n",
      "2018-07-12T17:46:22.422938: step 14, loss 3.96928, acc 0.4375\n",
      "2018-07-12T17:46:24.015327: step 15, loss 1.8794, acc 0.671875\n",
      "2018-07-12T17:46:25.610728: step 16, loss 2.3768, acc 0.546875\n",
      "2018-07-12T17:46:27.186188: step 17, loss 2.57774, acc 0.421875\n",
      "2018-07-12T17:46:28.804129: step 18, loss 1.73899, acc 0.671875\n",
      "2018-07-12T17:46:30.470932: step 19, loss 2.31499, acc 0.546875\n",
      "2018-07-12T17:46:32.067513: step 20, loss 2.97982, acc 0.4375\n",
      "2018-07-12T17:46:33.703135: step 21, loss 2.71136, acc 0.53125\n",
      "2018-07-12T17:46:35.255526: step 22, loss 1.37586, acc 0.625\n",
      "2018-07-12T17:46:36.869070: step 23, loss 2.05244, acc 0.515625\n",
      "2018-07-12T17:46:38.472194: step 24, loss 2.70378, acc 0.484375\n",
      "2018-07-12T17:46:40.085356: step 25, loss 2.55437, acc 0.578125\n",
      "2018-07-12T17:46:41.745391: step 26, loss 2.67212, acc 0.53125\n",
      "2018-07-12T17:46:43.401134: step 27, loss 2.26609, acc 0.5\n",
      "2018-07-12T17:46:44.999195: step 28, loss 2.21766, acc 0.53125\n",
      "2018-07-12T17:46:46.589551: step 29, loss 1.70697, acc 0.625\n",
      "2018-07-12T17:46:48.207965: step 30, loss 2.85738, acc 0.4375\n",
      "2018-07-12T17:46:49.829845: step 31, loss 2.92217, acc 0.53125\n",
      "2018-07-12T17:46:51.421978: step 32, loss 1.7456, acc 0.546875\n",
      "2018-07-12T17:46:53.050213: step 33, loss 2.91263, acc 0.515625\n",
      "2018-07-12T17:46:54.685491: step 34, loss 3.01254, acc 0.46875\n",
      "2018-07-12T17:46:56.291675: step 35, loss 2.33385, acc 0.5625\n",
      "2018-07-12T17:46:57.954553: step 36, loss 2.10432, acc 0.625\n",
      "2018-07-12T17:46:59.563211: step 37, loss 2.41043, acc 0.546875\n",
      "2018-07-12T17:47:01.201923: step 38, loss 1.91865, acc 0.59375\n",
      "2018-07-12T17:47:02.826832: step 39, loss 2.41396, acc 0.53125\n",
      "2018-07-12T17:47:04.406646: step 40, loss 2.2015, acc 0.53125\n",
      "2018-07-12T17:47:06.059868: step 41, loss 2.29724, acc 0.59375\n",
      "2018-07-12T17:47:07.691343: step 42, loss 2.97968, acc 0.484375\n",
      "2018-07-12T17:47:09.251889: step 43, loss 1.67073, acc 0.5625\n",
      "2018-07-12T17:47:10.854743: step 44, loss 2.56685, acc 0.578125\n",
      "2018-07-12T17:47:12.450246: step 45, loss 2.09789, acc 0.5625\n",
      "2018-07-12T17:47:14.029470: step 46, loss 2.29537, acc 0.4375\n",
      "2018-07-12T17:47:15.600698: step 47, loss 2.07056, acc 0.53125\n",
      "2018-07-12T17:47:17.210293: step 48, loss 2.4328, acc 0.5\n",
      "2018-07-12T17:47:18.832828: step 49, loss 1.90666, acc 0.546875\n",
      "2018-07-12T17:47:20.403010: step 50, loss 2.31426, acc 0.453125\n",
      "2018-07-12T17:47:21.968164: step 51, loss 1.73955, acc 0.59375\n",
      "2018-07-12T17:47:23.492850: step 52, loss 2.19298, acc 0.546875\n",
      "2018-07-12T17:47:25.099030: step 53, loss 2.26798, acc 0.53125\n",
      "2018-07-12T17:47:26.698594: step 54, loss 1.89285, acc 0.625\n",
      "2018-07-12T17:47:28.288115: step 55, loss 1.59826, acc 0.609375\n",
      "2018-07-12T17:47:29.826035: step 56, loss 1.44816, acc 0.671875\n",
      "2018-07-12T17:47:31.452364: step 57, loss 2.12657, acc 0.515625\n",
      "2018-07-12T17:47:33.119855: step 58, loss 2.3492, acc 0.53125\n",
      "2018-07-12T17:47:34.771092: step 59, loss 3.02372, acc 0.453125\n",
      "2018-07-12T17:47:36.495686: step 60, loss 3.06361, acc 0.359375\n",
      "2018-07-12T17:47:38.148732: step 61, loss 1.96138, acc 0.59375\n",
      "2018-07-12T17:47:39.751169: step 62, loss 2.29881, acc 0.546875\n",
      "2018-07-12T17:47:41.324000: step 63, loss 2.58173, acc 0.484375\n",
      "2018-07-12T17:47:42.893851: step 64, loss 1.66252, acc 0.59375\n",
      "2018-07-12T17:47:44.531829: step 65, loss 1.75359, acc 0.59375\n",
      "2018-07-12T17:47:46.167126: step 66, loss 1.57126, acc 0.640625\n",
      "2018-07-12T17:47:47.821930: step 67, loss 1.98044, acc 0.5\n",
      "2018-07-12T17:47:49.451985: step 68, loss 1.8433, acc 0.65625\n",
      "2018-07-12T17:47:51.059145: step 69, loss 2.45733, acc 0.4375\n",
      "2018-07-12T17:47:52.631697: step 70, loss 1.66012, acc 0.578125\n",
      "2018-07-12T17:47:54.215248: step 71, loss 2.35909, acc 0.453125\n",
      "2018-07-12T17:47:55.796600: step 72, loss 2.40538, acc 0.46875\n",
      "2018-07-12T17:47:57.446555: step 73, loss 2.0901, acc 0.546875\n",
      "2018-07-12T17:47:59.073456: step 74, loss 2.08397, acc 0.5\n",
      "2018-07-12T17:48:00.661527: step 75, loss 2.18581, acc 0.5625\n",
      "2018-07-12T17:48:02.223845: step 76, loss 2.05975, acc 0.515625\n",
      "2018-07-12T17:48:03.776865: step 77, loss 2.5733, acc 0.34375\n",
      "2018-07-12T17:48:05.401809: step 78, loss 2.18517, acc 0.453125\n",
      "2018-07-12T17:48:07.022956: step 79, loss 2.38258, acc 0.515625\n",
      "2018-07-12T17:48:08.595024: step 80, loss 2.37944, acc 0.453125\n",
      "2018-07-12T17:48:10.232180: step 81, loss 2.05641, acc 0.515625\n",
      "2018-07-12T17:48:11.850695: step 82, loss 2.7788, acc 0.421875\n",
      "2018-07-12T17:48:13.423206: step 83, loss 1.62203, acc 0.5625\n",
      "2018-07-12T17:48:15.018019: step 84, loss 2.09514, acc 0.53125\n",
      "2018-07-12T17:48:16.602228: step 85, loss 1.42456, acc 0.578125\n",
      "2018-07-12T17:48:18.233370: step 86, loss 2.29727, acc 0.5\n",
      "2018-07-12T17:48:19.899262: step 87, loss 1.23779, acc 0.625\n",
      "2018-07-12T17:48:21.582901: step 88, loss 1.96203, acc 0.484375\n",
      "2018-07-12T17:48:23.274186: step 89, loss 2.43226, acc 0.5\n",
      "2018-07-12T17:48:24.875191: step 90, loss 1.87432, acc 0.515625\n",
      "2018-07-12T17:48:26.438315: step 91, loss 1.82623, acc 0.546875\n",
      "2018-07-12T17:48:28.026025: step 92, loss 1.90037, acc 0.515625\n",
      "2018-07-12T17:48:29.624566: step 93, loss 2.53876, acc 0.5625\n",
      "2018-07-12T17:48:31.248933: step 94, loss 1.93833, acc 0.46875\n",
      "2018-07-12T17:48:32.839269: step 95, loss 1.84817, acc 0.609375\n",
      "2018-07-12T17:48:34.472670: step 96, loss 1.60635, acc 0.53125\n",
      "2018-07-12T17:48:36.099860: step 97, loss 1.57658, acc 0.484375\n",
      "2018-07-12T17:48:37.748570: step 98, loss 1.92115, acc 0.46875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T17:48:39.365839: step 99, loss 1.45037, acc 0.671875\n",
      "2018-07-12T17:48:40.976723: step 100, loss 1.28639, acc 0.609375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T17:48:58.886172: step 100, loss 0.687164, acc 0.6244\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-100\n",
      "\n",
      "2018-07-12T17:49:01.311526: step 101, loss 1.45864, acc 0.546875\n",
      "2018-07-12T17:49:02.870113: step 102, loss 2.05803, acc 0.546875\n",
      "2018-07-12T17:49:04.453861: step 103, loss 2.6602, acc 0.453125\n",
      "2018-07-12T17:49:06.108705: step 104, loss 1.49275, acc 0.625\n",
      "2018-07-12T17:49:07.683405: step 105, loss 1.50004, acc 0.59375\n",
      "2018-07-12T17:49:09.251419: step 106, loss 1.57666, acc 0.53125\n",
      "2018-07-12T17:49:10.799875: step 107, loss 1.77096, acc 0.5\n",
      "2018-07-12T17:49:12.413569: step 108, loss 1.74562, acc 0.59375\n",
      "2018-07-12T17:49:14.039733: step 109, loss 2.69513, acc 0.375\n",
      "2018-07-12T17:49:15.586603: step 110, loss 1.99697, acc 0.5625\n",
      "2018-07-12T17:49:17.138935: step 111, loss 1.59183, acc 0.609375\n",
      "2018-07-12T17:49:18.780608: step 112, loss 1.74298, acc 0.53125\n",
      "2018-07-12T17:49:20.359403: step 113, loss 2.0394, acc 0.484375\n",
      "2018-07-12T17:49:21.935679: step 114, loss 2.21221, acc 0.53125\n",
      "2018-07-12T17:49:23.531034: step 115, loss 2.26594, acc 0.453125\n",
      "2018-07-12T17:49:25.187988: step 116, loss 1.92437, acc 0.515625\n",
      "2018-07-12T17:49:26.771899: step 117, loss 1.68311, acc 0.6875\n",
      "2018-07-12T17:49:28.419246: step 118, loss 1.6142, acc 0.5625\n",
      "2018-07-12T17:49:30.023417: step 119, loss 1.1992, acc 0.578125\n",
      "2018-07-12T17:49:31.591210: step 120, loss 1.78474, acc 0.5625\n",
      "2018-07-12T17:49:33.177849: step 121, loss 1.66726, acc 0.5625\n",
      "2018-07-12T17:49:34.786926: step 122, loss 1.9028, acc 0.484375\n",
      "2018-07-12T17:49:36.368485: step 123, loss 1.30669, acc 0.546875\n",
      "2018-07-12T17:49:37.988775: step 124, loss 1.87211, acc 0.5625\n",
      "2018-07-12T17:49:39.602001: step 125, loss 1.77507, acc 0.53125\n",
      "2018-07-12T17:49:41.173293: step 126, loss 2.00741, acc 0.484375\n",
      "2018-07-12T17:49:42.765826: step 127, loss 1.56441, acc 0.53125\n",
      "2018-07-12T17:49:44.358462: step 128, loss 1.74319, acc 0.515625\n",
      "2018-07-12T17:49:45.932380: step 129, loss 1.65595, acc 0.59375\n",
      "2018-07-12T17:49:47.511420: step 130, loss 1.32311, acc 0.671875\n",
      "2018-07-12T17:49:49.075830: step 131, loss 1.48878, acc 0.546875\n",
      "2018-07-12T17:49:50.635167: step 132, loss 1.56989, acc 0.609375\n",
      "2018-07-12T17:49:52.220042: step 133, loss 1.84348, acc 0.4375\n",
      "2018-07-12T17:49:53.788549: step 134, loss 1.6496, acc 0.5625\n",
      "2018-07-12T17:49:55.327494: step 135, loss 1.32989, acc 0.578125\n",
      "2018-07-12T17:49:56.882233: step 136, loss 1.68958, acc 0.5625\n",
      "2018-07-12T17:49:58.482782: step 137, loss 1.35143, acc 0.578125\n",
      "2018-07-12T17:50:00.070029: step 138, loss 1.17563, acc 0.6875\n",
      "2018-07-12T17:50:01.654033: step 139, loss 1.88261, acc 0.53125\n",
      "2018-07-12T17:50:03.174355: step 140, loss 1.58732, acc 0.59375\n",
      "2018-07-12T17:50:04.758934: step 141, loss 1.67702, acc 0.515625\n",
      "2018-07-12T17:50:06.398544: step 142, loss 1.60333, acc 0.484375\n",
      "2018-07-12T17:50:08.021181: step 143, loss 1.32761, acc 0.578125\n",
      "2018-07-12T17:50:09.659649: step 144, loss 0.993944, acc 0.6875\n",
      "2018-07-12T17:50:11.222957: step 145, loss 1.71665, acc 0.5625\n",
      "2018-07-12T17:50:12.778086: step 146, loss 1.84313, acc 0.453125\n",
      "2018-07-12T17:50:14.345342: step 147, loss 1.50863, acc 0.546875\n",
      "2018-07-12T17:50:15.908260: step 148, loss 1.885, acc 0.5\n",
      "2018-07-12T17:50:17.508413: step 149, loss 1.88516, acc 0.515625\n",
      "2018-07-12T17:50:19.171666: step 150, loss 1.46333, acc 0.515625\n",
      "2018-07-12T17:50:20.762540: step 151, loss 1.7834, acc 0.515625\n",
      "2018-07-12T17:50:22.369395: step 152, loss 1.98031, acc 0.59375\n",
      "2018-07-12T17:50:23.978032: step 153, loss 1.50233, acc 0.53125\n",
      "2018-07-12T17:50:25.573713: step 154, loss 1.56805, acc 0.5\n",
      "2018-07-12T17:50:27.177213: step 155, loss 1.3769, acc 0.546875\n",
      "2018-07-12T17:50:28.792382: step 156, loss 1.93349, acc 0.5\n",
      "2018-07-12T17:50:30.420344: step 157, loss 1.73977, acc 0.5\n",
      "2018-07-12T17:50:32.080125: step 158, loss 1.63512, acc 0.5625\n",
      "2018-07-12T17:50:33.668265: step 159, loss 1.85654, acc 0.5625\n",
      "2018-07-12T17:50:35.312741: step 160, loss 1.16277, acc 0.65625\n",
      "2018-07-12T17:50:36.965126: step 161, loss 1.56929, acc 0.515625\n",
      "2018-07-12T17:50:38.571388: step 162, loss 1.48307, acc 0.578125\n",
      "2018-07-12T17:50:40.199323: step 163, loss 1.34059, acc 0.609375\n",
      "2018-07-12T17:50:41.807256: step 164, loss 1.239, acc 0.515625\n",
      "2018-07-12T17:50:43.398106: step 165, loss 0.920206, acc 0.640625\n",
      "2018-07-12T17:50:44.980354: step 166, loss 1.60908, acc 0.578125\n",
      "2018-07-12T17:50:46.525428: step 167, loss 1.75665, acc 0.515625\n",
      "2018-07-12T17:50:48.159689: step 168, loss 1.21989, acc 0.625\n",
      "2018-07-12T17:50:49.761905: step 169, loss 1.52793, acc 0.578125\n",
      "2018-07-12T17:50:51.387925: step 170, loss 1.07756, acc 0.578125\n",
      "2018-07-12T17:50:52.959390: step 171, loss 1.62131, acc 0.46875\n",
      "2018-07-12T17:50:54.583638: step 172, loss 1.35147, acc 0.59375\n",
      "2018-07-12T17:50:56.222456: step 173, loss 1.6535, acc 0.46875\n",
      "2018-07-12T17:50:57.823338: step 174, loss 1.46242, acc 0.5625\n",
      "2018-07-12T17:50:59.414252: step 175, loss 1.65941, acc 0.515625\n",
      "2018-07-12T17:51:01.060068: step 176, loss 1.30074, acc 0.65625\n",
      "2018-07-12T17:51:02.667397: step 177, loss 1.87289, acc 0.578125\n",
      "2018-07-12T17:51:04.253647: step 178, loss 1.21958, acc 0.515625\n",
      "2018-07-12T17:51:05.886906: step 179, loss 1.3585, acc 0.546875\n",
      "2018-07-12T17:51:07.502477: step 180, loss 0.958179, acc 0.6875\n",
      "2018-07-12T17:51:09.105043: step 181, loss 1.36846, acc 0.546875\n",
      "2018-07-12T17:51:10.703839: step 182, loss 1.17052, acc 0.640625\n",
      "2018-07-12T17:51:12.371185: step 183, loss 1.28656, acc 0.59375\n",
      "2018-07-12T17:51:13.973802: step 184, loss 0.953356, acc 0.703125\n",
      "2018-07-12T17:51:15.556250: step 185, loss 1.0258, acc 0.6875\n",
      "2018-07-12T17:51:17.142744: step 186, loss 1.56322, acc 0.546875\n",
      "2018-07-12T17:51:18.719680: step 187, loss 1.07112, acc 0.5625\n",
      "2018-07-12T17:51:20.266477: step 188, loss 1.31661, acc 0.609375\n",
      "2018-07-12T17:51:21.901974: step 189, loss 1.22164, acc 0.609375\n",
      "2018-07-12T17:51:23.564663: step 190, loss 1.06496, acc 0.609375\n",
      "2018-07-12T17:51:25.211334: step 191, loss 1.92399, acc 0.453125\n",
      "2018-07-12T17:51:26.815853: step 192, loss 1.19421, acc 0.625\n",
      "2018-07-12T17:51:28.435167: step 193, loss 1.1735, acc 0.609375\n",
      "2018-07-12T17:51:29.984941: step 194, loss 1.12216, acc 0.546875\n",
      "2018-07-12T17:51:31.581962: step 195, loss 1.35936, acc 0.546875\n",
      "2018-07-12T17:51:33.235958: step 196, loss 1.3208, acc 0.625\n",
      "2018-07-12T17:51:34.860356: step 197, loss 1.28248, acc 0.515625\n",
      "2018-07-12T17:51:36.456097: step 198, loss 1.33002, acc 0.5625\n",
      "2018-07-12T17:51:38.035754: step 199, loss 1.16062, acc 0.609375\n",
      "2018-07-12T17:51:39.645530: step 200, loss 1.52324, acc 0.453125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T17:51:57.384558: step 200, loss 0.547151, acc 0.706\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-200\n",
      "\n",
      "2018-07-12T17:52:00.166269: step 201, loss 1.33241, acc 0.5625\n",
      "2018-07-12T17:52:01.792285: step 202, loss 1.26399, acc 0.640625\n",
      "2018-07-12T17:52:03.355627: step 203, loss 1.24106, acc 0.578125\n",
      "2018-07-12T17:52:04.919618: step 204, loss 1.20467, acc 0.625\n",
      "2018-07-12T17:52:06.505819: step 205, loss 1.26234, acc 0.609375\n",
      "2018-07-12T17:52:08.121536: step 206, loss 1.56293, acc 0.515625\n",
      "2018-07-12T17:52:09.649797: step 207, loss 1.65774, acc 0.53125\n",
      "2018-07-12T17:52:11.192266: step 208, loss 1.27622, acc 0.515625\n",
      "2018-07-12T17:52:12.858640: step 209, loss 1.05165, acc 0.609375\n",
      "2018-07-12T17:52:14.463655: step 210, loss 0.84515, acc 0.703125\n",
      "2018-07-12T17:52:16.028314: step 211, loss 1.16981, acc 0.609375\n",
      "2018-07-12T17:52:17.595653: step 212, loss 1.04187, acc 0.65625\n",
      "2018-07-12T17:52:19.210290: step 213, loss 0.997167, acc 0.609375\n",
      "2018-07-12T17:52:20.832199: step 214, loss 1.02888, acc 0.6875\n",
      "2018-07-12T17:52:22.520358: step 215, loss 1.09214, acc 0.578125\n",
      "2018-07-12T17:52:24.086214: step 216, loss 1.40356, acc 0.5625\n",
      "2018-07-12T17:52:25.711149: step 217, loss 1.07208, acc 0.640625\n",
      "2018-07-12T17:52:27.305879: step 218, loss 1.27033, acc 0.515625\n",
      "2018-07-12T17:52:28.892676: step 219, loss 1.40014, acc 0.515625\n",
      "2018-07-12T17:52:30.476934: step 220, loss 1.3235, acc 0.515625\n",
      "2018-07-12T17:52:32.032431: step 221, loss 1.08777, acc 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T17:52:33.610796: step 222, loss 1.16518, acc 0.578125\n",
      "2018-07-12T17:52:35.231420: step 223, loss 1.21825, acc 0.46875\n",
      "2018-07-12T17:52:36.821966: step 224, loss 1.13007, acc 0.59375\n",
      "2018-07-12T17:52:38.375180: step 225, loss 0.93358, acc 0.625\n",
      "2018-07-12T17:52:39.963701: step 226, loss 1.27526, acc 0.59375\n",
      "2018-07-12T17:52:41.592602: step 227, loss 1.14842, acc 0.578125\n",
      "2018-07-12T17:52:43.266977: step 228, loss 1.18726, acc 0.5625\n",
      "2018-07-12T17:52:44.892515: step 229, loss 1.33516, acc 0.546875\n",
      "2018-07-12T17:52:46.486666: step 230, loss 1.03039, acc 0.625\n",
      "2018-07-12T17:52:48.047188: step 231, loss 1.29622, acc 0.59375\n",
      "2018-07-12T17:52:49.652710: step 232, loss 1.49041, acc 0.53125\n",
      "2018-07-12T17:52:51.238569: step 233, loss 1.19465, acc 0.59375\n",
      "2018-07-12T17:52:52.786497: step 234, loss 0.9275, acc 0.671875\n",
      "2018-07-12T17:52:54.329133: step 235, loss 1.03828, acc 0.6875\n",
      "2018-07-12T17:52:55.952853: step 236, loss 1.2379, acc 0.5625\n",
      "2018-07-12T17:52:57.554345: step 237, loss 0.968163, acc 0.59375\n",
      "2018-07-12T17:52:59.194214: step 238, loss 1.29649, acc 0.609375\n",
      "2018-07-12T17:53:00.797403: step 239, loss 1.02454, acc 0.640625\n",
      "2018-07-12T17:53:02.378389: step 240, loss 1.42119, acc 0.46875\n",
      "2018-07-12T17:53:03.945734: step 241, loss 1.26591, acc 0.546875\n",
      "2018-07-12T17:53:05.610072: step 242, loss 1.01083, acc 0.546875\n",
      "2018-07-12T17:53:07.233505: step 243, loss 0.987981, acc 0.578125\n",
      "2018-07-12T17:53:08.821544: step 244, loss 1.34341, acc 0.625\n",
      "2018-07-12T17:53:10.338934: step 245, loss 1.026, acc 0.671875\n",
      "2018-07-12T17:53:11.927741: step 246, loss 0.894651, acc 0.609375\n",
      "2018-07-12T17:53:13.491938: step 247, loss 0.932088, acc 0.5625\n",
      "2018-07-12T17:53:15.098246: step 248, loss 0.975788, acc 0.65625\n",
      "2018-07-12T17:53:16.661821: step 249, loss 1.26413, acc 0.5\n",
      "2018-07-12T17:53:18.279109: step 250, loss 1.08721, acc 0.65625\n",
      "2018-07-12T17:53:19.918317: step 251, loss 1.09802, acc 0.453125\n",
      "2018-07-12T17:53:21.556515: step 252, loss 1.19099, acc 0.515625\n",
      "2018-07-12T17:53:23.117336: step 253, loss 1.04736, acc 0.5625\n",
      "2018-07-12T17:53:24.730352: step 254, loss 1.06733, acc 0.578125\n",
      "2018-07-12T17:53:26.391454: step 255, loss 0.871283, acc 0.65625\n",
      "2018-07-12T17:53:28.018387: step 256, loss 1.28793, acc 0.546875\n",
      "2018-07-12T17:53:29.580754: step 257, loss 0.736771, acc 0.6875\n",
      "2018-07-12T17:53:31.106519: step 258, loss 0.811588, acc 0.734375\n",
      "2018-07-12T17:53:32.808469: step 259, loss 0.849669, acc 0.609375\n",
      "2018-07-12T17:53:34.508062: step 260, loss 1.08043, acc 0.578125\n",
      "2018-07-12T17:53:36.108484: step 261, loss 1.01487, acc 0.5625\n",
      "2018-07-12T17:53:37.681536: step 262, loss 1.00713, acc 0.59375\n",
      "2018-07-12T17:53:39.263738: step 263, loss 1.082, acc 0.59375\n",
      "2018-07-12T17:53:40.858277: step 264, loss 0.727962, acc 0.625\n",
      "2018-07-12T17:53:42.525350: step 265, loss 0.753356, acc 0.703125\n",
      "2018-07-12T17:53:44.062370: step 266, loss 0.93812, acc 0.640625\n",
      "2018-07-12T17:53:45.624251: step 267, loss 0.923557, acc 0.609375\n",
      "2018-07-12T17:53:47.170159: step 268, loss 1.20365, acc 0.484375\n",
      "2018-07-12T17:53:48.735046: step 269, loss 1.12111, acc 0.546875\n",
      "2018-07-12T17:53:50.314348: step 270, loss 1.01661, acc 0.59375\n",
      "2018-07-12T17:53:51.900407: step 271, loss 1.23761, acc 0.578125\n",
      "2018-07-12T17:53:53.494842: step 272, loss 1.0506, acc 0.609375\n",
      "2018-07-12T17:53:55.082698: step 273, loss 0.754308, acc 0.640625\n",
      "2018-07-12T17:53:56.687960: step 274, loss 0.880982, acc 0.546875\n",
      "2018-07-12T17:53:58.301312: step 275, loss 0.894588, acc 0.625\n",
      "2018-07-12T17:53:59.929192: step 276, loss 0.94476, acc 0.625\n",
      "2018-07-12T17:54:01.518779: step 277, loss 0.771806, acc 0.625\n",
      "2018-07-12T17:54:03.169008: step 278, loss 0.964992, acc 0.609375\n",
      "2018-07-12T17:54:04.747973: step 279, loss 1.01356, acc 0.5625\n",
      "2018-07-12T17:54:06.358987: step 280, loss 0.914495, acc 0.65625\n",
      "2018-07-12T17:54:08.072791: step 281, loss 0.779965, acc 0.703125\n",
      "2018-07-12T17:54:09.700848: step 282, loss 0.83454, acc 0.609375\n",
      "2018-07-12T17:54:11.297737: step 283, loss 1.03755, acc 0.578125\n",
      "2018-07-12T17:54:12.972451: step 284, loss 1.21995, acc 0.5625\n",
      "2018-07-12T17:54:14.598390: step 285, loss 0.944451, acc 0.640625\n",
      "2018-07-12T17:54:16.256688: step 286, loss 0.773901, acc 0.65625\n",
      "2018-07-12T17:54:17.863159: step 287, loss 0.88022, acc 0.59375\n",
      "2018-07-12T17:54:19.464768: step 288, loss 0.800784, acc 0.65625\n",
      "2018-07-12T17:54:21.128995: step 289, loss 1.04989, acc 0.625\n",
      "2018-07-12T17:54:22.815602: step 290, loss 0.919572, acc 0.65625\n",
      "2018-07-12T17:54:24.377409: step 291, loss 1.12322, acc 0.546875\n",
      "2018-07-12T17:54:25.983600: step 292, loss 0.755797, acc 0.625\n",
      "2018-07-12T17:54:27.619109: step 293, loss 0.973927, acc 0.515625\n",
      "2018-07-12T17:54:29.210999: step 294, loss 0.746586, acc 0.671875\n",
      "2018-07-12T17:54:30.862335: step 295, loss 0.863265, acc 0.640625\n",
      "2018-07-12T17:54:32.540508: step 296, loss 0.623105, acc 0.796875\n",
      "2018-07-12T17:54:34.172974: step 297, loss 0.974617, acc 0.578125\n",
      "2018-07-12T17:54:35.850671: step 298, loss 0.865114, acc 0.59375\n",
      "2018-07-12T17:54:37.531062: step 299, loss 0.818278, acc 0.59375\n",
      "2018-07-12T17:54:39.132241: step 300, loss 0.699421, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T17:54:57.301380: step 300, loss 0.588696, acc 0.6572\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-300\n",
      "\n",
      "2018-07-12T17:54:59.878202: step 301, loss 0.88222, acc 0.59375\n",
      "2018-07-12T17:55:01.454388: step 302, loss 0.769878, acc 0.703125\n",
      "2018-07-12T17:55:03.086693: step 303, loss 0.744761, acc 0.65625\n",
      "2018-07-12T17:55:04.680358: step 304, loss 0.727718, acc 0.609375\n",
      "2018-07-12T17:55:06.299734: step 305, loss 0.708173, acc 0.640625\n",
      "2018-07-12T17:55:07.928673: step 306, loss 0.809669, acc 0.609375\n",
      "2018-07-12T17:55:09.530359: step 307, loss 0.899514, acc 0.546875\n",
      "2018-07-12T17:55:11.143790: step 308, loss 1.0711, acc 0.4375\n",
      "2018-07-12T17:55:12.832610: step 309, loss 0.707601, acc 0.59375\n",
      "2018-07-12T17:55:14.503279: step 310, loss 0.864709, acc 0.65625\n",
      "2018-07-12T17:55:16.177862: step 311, loss 0.980796, acc 0.5625\n",
      "2018-07-12T17:55:17.776329: step 312, loss 0.857949, acc 0.546875\n",
      "2018-07-12T17:55:19.469408: step 313, loss 0.880294, acc 0.640625\n",
      "2018-07-12T17:55:21.064058: step 314, loss 1.03633, acc 0.546875\n",
      "2018-07-12T17:55:22.657239: step 315, loss 0.571398, acc 0.6875\n",
      "2018-07-12T17:55:24.252092: step 316, loss 0.843704, acc 0.671875\n",
      "2018-07-12T17:55:25.832752: step 317, loss 0.907623, acc 0.515625\n",
      "2018-07-12T17:55:27.421463: step 318, loss 0.899089, acc 0.59375\n",
      "2018-07-12T17:55:29.173130: step 319, loss 0.755458, acc 0.6875\n",
      "2018-07-12T17:55:30.795137: step 320, loss 0.780814, acc 0.671875\n",
      "2018-07-12T17:55:32.421033: step 321, loss 0.613184, acc 0.78125\n",
      "2018-07-12T17:55:34.123997: step 322, loss 0.831947, acc 0.671875\n",
      "2018-07-12T17:55:35.743306: step 323, loss 0.813904, acc 0.625\n",
      "2018-07-12T17:55:37.394975: step 324, loss 0.828097, acc 0.578125\n",
      "2018-07-12T17:55:38.963984: step 325, loss 1.07657, acc 0.5\n",
      "2018-07-12T17:55:40.532859: step 326, loss 0.9547, acc 0.625\n",
      "2018-07-12T17:55:42.105467: step 327, loss 1.15222, acc 0.5625\n",
      "2018-07-12T17:55:43.719235: step 328, loss 0.8307, acc 0.625\n",
      "2018-07-12T17:55:45.361308: step 329, loss 0.907086, acc 0.59375\n",
      "2018-07-12T17:55:46.923252: step 330, loss 0.920904, acc 0.625\n",
      "2018-07-12T17:55:48.558965: step 331, loss 1.16138, acc 0.5\n",
      "2018-07-12T17:55:50.168481: step 332, loss 0.645349, acc 0.65625\n",
      "2018-07-12T17:55:51.771438: step 333, loss 0.876758, acc 0.546875\n",
      "2018-07-12T17:55:53.381402: step 334, loss 0.582403, acc 0.75\n",
      "2018-07-12T17:55:55.066780: step 335, loss 0.675846, acc 0.671875\n",
      "2018-07-12T17:55:56.661407: step 336, loss 0.898712, acc 0.59375\n",
      "2018-07-12T17:55:58.346091: step 337, loss 0.738045, acc 0.6875\n",
      "2018-07-12T17:55:59.983441: step 338, loss 0.845758, acc 0.5625\n",
      "2018-07-12T17:56:01.751602: step 339, loss 0.848233, acc 0.734375\n",
      "2018-07-12T17:56:03.328774: step 340, loss 0.882888, acc 0.59375\n",
      "2018-07-12T17:56:05.012241: step 341, loss 0.889755, acc 0.59375\n",
      "2018-07-12T17:56:06.618014: step 342, loss 0.861073, acc 0.546875\n",
      "2018-07-12T17:56:08.245246: step 343, loss 0.612821, acc 0.6875\n",
      "2018-07-12T17:56:09.883881: step 344, loss 1.21184, acc 0.46875\n",
      "2018-07-12T17:56:11.523951: step 345, loss 0.664053, acc 0.671875\n",
      "2018-07-12T17:56:13.150844: step 346, loss 0.991046, acc 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T17:56:14.735724: step 347, loss 0.684853, acc 0.671875\n",
      "2018-07-12T17:56:16.419611: step 348, loss 0.904254, acc 0.578125\n",
      "2018-07-12T17:56:18.064365: step 349, loss 0.624791, acc 0.65625\n",
      "2018-07-12T17:56:19.623259: step 350, loss 0.70719, acc 0.65625\n",
      "2018-07-12T17:56:21.212027: step 351, loss 0.727171, acc 0.71875\n",
      "2018-07-12T17:56:22.210823: step 352, loss 0.793633, acc 0.638889\n",
      "2018-07-12T17:56:23.775781: step 353, loss 0.741915, acc 0.59375\n",
      "2018-07-12T17:56:25.340623: step 354, loss 0.792622, acc 0.609375\n",
      "2018-07-12T17:56:26.922775: step 355, loss 0.784125, acc 0.59375\n",
      "2018-07-12T17:56:28.547276: step 356, loss 0.765951, acc 0.59375\n",
      "2018-07-12T17:56:30.221228: step 357, loss 0.782151, acc 0.625\n",
      "2018-07-12T17:56:31.796198: step 358, loss 0.649875, acc 0.65625\n",
      "2018-07-12T17:56:33.408385: step 359, loss 0.745584, acc 0.625\n",
      "2018-07-12T17:56:35.063988: step 360, loss 0.787494, acc 0.65625\n",
      "2018-07-12T17:56:36.716089: step 361, loss 0.875559, acc 0.59375\n",
      "2018-07-12T17:56:38.311898: step 362, loss 0.68786, acc 0.625\n",
      "2018-07-12T17:56:39.868692: step 363, loss 0.675226, acc 0.65625\n",
      "2018-07-12T17:56:41.442652: step 364, loss 0.700478, acc 0.671875\n",
      "2018-07-12T17:56:42.998422: step 365, loss 0.626022, acc 0.71875\n",
      "2018-07-12T17:56:44.587127: step 366, loss 0.438066, acc 0.765625\n",
      "2018-07-12T17:56:46.162158: step 367, loss 0.660539, acc 0.6875\n",
      "2018-07-12T17:56:47.710698: step 368, loss 0.713372, acc 0.703125\n",
      "2018-07-12T17:56:49.297328: step 369, loss 0.582282, acc 0.703125\n",
      "2018-07-12T17:56:50.884115: step 370, loss 0.602773, acc 0.703125\n",
      "2018-07-12T17:56:52.572066: step 371, loss 0.74673, acc 0.625\n",
      "2018-07-12T17:56:54.178099: step 372, loss 0.640874, acc 0.625\n",
      "2018-07-12T17:56:55.717068: step 373, loss 0.621327, acc 0.65625\n",
      "2018-07-12T17:56:57.338498: step 374, loss 0.586343, acc 0.71875\n",
      "2018-07-12T17:56:58.904848: step 375, loss 0.542275, acc 0.765625\n",
      "2018-07-12T17:57:00.504687: step 376, loss 0.687286, acc 0.703125\n",
      "2018-07-12T17:57:02.109761: step 377, loss 0.675757, acc 0.640625\n",
      "2018-07-12T17:57:03.696544: step 378, loss 0.787334, acc 0.5625\n",
      "2018-07-12T17:57:05.283451: step 379, loss 0.66791, acc 0.609375\n",
      "2018-07-12T17:57:06.838583: step 380, loss 0.649688, acc 0.6875\n",
      "2018-07-12T17:57:08.369660: step 381, loss 0.69789, acc 0.640625\n",
      "2018-07-12T17:57:09.921809: step 382, loss 0.78398, acc 0.625\n",
      "2018-07-12T17:57:11.495501: step 383, loss 0.846968, acc 0.515625\n",
      "2018-07-12T17:57:13.053144: step 384, loss 0.71528, acc 0.671875\n",
      "2018-07-12T17:57:14.667175: step 385, loss 0.756151, acc 0.640625\n",
      "2018-07-12T17:57:16.263437: step 386, loss 0.490302, acc 0.75\n",
      "2018-07-12T17:57:17.925281: step 387, loss 0.5215, acc 0.8125\n",
      "2018-07-12T17:57:19.549489: step 388, loss 0.678782, acc 0.625\n",
      "2018-07-12T17:57:21.105135: step 389, loss 0.584384, acc 0.75\n",
      "2018-07-12T17:57:22.731042: step 390, loss 0.717885, acc 0.703125\n",
      "2018-07-12T17:57:24.318863: step 391, loss 0.659543, acc 0.65625\n",
      "2018-07-12T17:57:25.909817: step 392, loss 0.616036, acc 0.671875\n",
      "2018-07-12T17:57:27.535913: step 393, loss 0.646765, acc 0.703125\n",
      "2018-07-12T17:57:29.168568: step 394, loss 0.878514, acc 0.5625\n",
      "2018-07-12T17:57:30.827023: step 395, loss 0.902666, acc 0.578125\n",
      "2018-07-12T17:57:32.461846: step 396, loss 0.530995, acc 0.71875\n",
      "2018-07-12T17:57:34.095844: step 397, loss 0.703617, acc 0.609375\n",
      "2018-07-12T17:57:35.689596: step 398, loss 0.642942, acc 0.671875\n",
      "2018-07-12T17:57:37.234085: step 399, loss 0.521622, acc 0.75\n",
      "2018-07-12T17:57:38.831480: step 400, loss 0.560013, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T17:57:56.455286: step 400, loss 0.497485, acc 0.784\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-400\n",
      "\n",
      "2018-07-12T17:57:59.107470: step 401, loss 0.627226, acc 0.625\n",
      "2018-07-12T17:58:00.691430: step 402, loss 0.725254, acc 0.625\n",
      "2018-07-12T17:58:02.269093: step 403, loss 0.488899, acc 0.796875\n",
      "2018-07-12T17:58:03.858639: step 404, loss 0.698407, acc 0.703125\n",
      "2018-07-12T17:58:05.486661: step 405, loss 0.564839, acc 0.734375\n",
      "2018-07-12T17:58:07.127196: step 406, loss 0.607583, acc 0.671875\n",
      "2018-07-12T17:58:08.744221: step 407, loss 0.613769, acc 0.703125\n",
      "2018-07-12T17:58:10.379175: step 408, loss 0.625672, acc 0.671875\n",
      "2018-07-12T17:58:11.954874: step 409, loss 0.753297, acc 0.625\n",
      "2018-07-12T17:58:13.511791: step 410, loss 0.71375, acc 0.53125\n",
      "2018-07-12T17:58:15.056974: step 411, loss 0.611514, acc 0.640625\n",
      "2018-07-12T17:58:16.602177: step 412, loss 0.469651, acc 0.796875\n",
      "2018-07-12T17:58:18.189673: step 413, loss 0.768903, acc 0.53125\n",
      "2018-07-12T17:58:19.826693: step 414, loss 0.658326, acc 0.65625\n",
      "2018-07-12T17:58:21.443117: step 415, loss 0.760923, acc 0.65625\n",
      "2018-07-12T17:58:23.077078: step 416, loss 0.578283, acc 0.6875\n",
      "2018-07-12T17:58:24.684593: step 417, loss 0.610485, acc 0.6875\n",
      "2018-07-12T17:58:26.298480: step 418, loss 0.650189, acc 0.6875\n",
      "2018-07-12T17:58:27.922019: step 419, loss 0.571606, acc 0.734375\n",
      "2018-07-12T17:58:29.485314: step 420, loss 0.676554, acc 0.6875\n",
      "2018-07-12T17:58:31.123501: step 421, loss 0.766101, acc 0.671875\n",
      "2018-07-12T17:58:32.685063: step 422, loss 0.620825, acc 0.625\n",
      "2018-07-12T17:58:34.385297: step 423, loss 0.806974, acc 0.578125\n",
      "2018-07-12T17:58:36.026048: step 424, loss 0.729685, acc 0.640625\n",
      "2018-07-12T17:58:37.626691: step 425, loss 0.593393, acc 0.734375\n",
      "2018-07-12T17:58:39.184242: step 426, loss 0.685513, acc 0.625\n",
      "2018-07-12T17:58:40.718542: step 427, loss 0.687546, acc 0.671875\n",
      "2018-07-12T17:58:42.279094: step 428, loss 0.731636, acc 0.625\n",
      "2018-07-12T17:58:43.925365: step 429, loss 0.521226, acc 0.75\n",
      "2018-07-12T17:58:45.499923: step 430, loss 0.827581, acc 0.609375\n",
      "2018-07-12T17:58:47.053968: step 431, loss 0.705352, acc 0.65625\n",
      "2018-07-12T17:58:48.736543: step 432, loss 0.608531, acc 0.703125\n",
      "2018-07-12T17:58:50.277927: step 433, loss 0.643368, acc 0.6875\n",
      "2018-07-12T17:58:51.842565: step 434, loss 0.657608, acc 0.640625\n",
      "2018-07-12T17:58:53.383522: step 435, loss 0.763719, acc 0.59375\n",
      "2018-07-12T17:58:54.940303: step 436, loss 0.758578, acc 0.625\n",
      "2018-07-12T17:58:56.503386: step 437, loss 0.672464, acc 0.609375\n",
      "2018-07-12T17:58:58.067686: step 438, loss 0.565208, acc 0.71875\n",
      "2018-07-12T17:58:59.634803: step 439, loss 0.503501, acc 0.71875\n",
      "2018-07-12T17:59:01.171187: step 440, loss 0.53717, acc 0.796875\n",
      "2018-07-12T17:59:02.692210: step 441, loss 0.713072, acc 0.625\n",
      "2018-07-12T17:59:04.274175: step 442, loss 0.831694, acc 0.546875\n",
      "2018-07-12T17:59:05.819418: step 443, loss 0.566615, acc 0.6875\n",
      "2018-07-12T17:59:07.402751: step 444, loss 0.740805, acc 0.640625\n",
      "2018-07-12T17:59:09.025844: step 445, loss 0.609676, acc 0.671875\n",
      "2018-07-12T17:59:10.630181: step 446, loss 0.605364, acc 0.71875\n",
      "2018-07-12T17:59:12.246775: step 447, loss 0.764593, acc 0.59375\n",
      "2018-07-12T17:59:13.847386: step 448, loss 0.566005, acc 0.734375\n",
      "2018-07-12T17:59:15.437743: step 449, loss 0.63466, acc 0.625\n",
      "2018-07-12T17:59:17.067985: step 450, loss 0.612958, acc 0.640625\n",
      "2018-07-12T17:59:18.645899: step 451, loss 0.672987, acc 0.703125\n",
      "2018-07-12T17:59:20.232602: step 452, loss 0.564837, acc 0.703125\n",
      "2018-07-12T17:59:21.745593: step 453, loss 0.601333, acc 0.734375\n",
      "2018-07-12T17:59:23.353523: step 454, loss 0.474112, acc 0.6875\n",
      "2018-07-12T17:59:24.942937: step 455, loss 0.614541, acc 0.671875\n",
      "2018-07-12T17:59:26.519369: step 456, loss 0.600162, acc 0.75\n",
      "2018-07-12T17:59:28.126636: step 457, loss 0.708822, acc 0.671875\n",
      "2018-07-12T17:59:29.701995: step 458, loss 0.684268, acc 0.671875\n",
      "2018-07-12T17:59:31.268315: step 459, loss 0.574715, acc 0.75\n",
      "2018-07-12T17:59:32.860390: step 460, loss 0.664167, acc 0.59375\n",
      "2018-07-12T17:59:34.459340: step 461, loss 0.724251, acc 0.640625\n",
      "2018-07-12T17:59:36.034216: step 462, loss 0.804255, acc 0.609375\n",
      "2018-07-12T17:59:37.585698: step 463, loss 0.644869, acc 0.71875\n",
      "2018-07-12T17:59:39.190405: step 464, loss 0.662797, acc 0.609375\n",
      "2018-07-12T17:59:40.751523: step 465, loss 0.69107, acc 0.625\n",
      "2018-07-12T17:59:42.340092: step 466, loss 0.474166, acc 0.75\n",
      "2018-07-12T17:59:43.907755: step 467, loss 0.516517, acc 0.75\n",
      "2018-07-12T17:59:45.529355: step 468, loss 0.76018, acc 0.65625\n",
      "2018-07-12T17:59:47.130886: step 469, loss 0.687106, acc 0.640625\n",
      "2018-07-12T17:59:48.731450: step 470, loss 0.726894, acc 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T17:59:50.320997: step 471, loss 0.665257, acc 0.625\n",
      "2018-07-12T17:59:51.898992: step 472, loss 0.681732, acc 0.640625\n",
      "2018-07-12T17:59:53.494179: step 473, loss 0.622925, acc 0.734375\n",
      "2018-07-12T17:59:55.085303: step 474, loss 0.64529, acc 0.578125\n",
      "2018-07-12T17:59:56.711304: step 475, loss 0.573416, acc 0.71875\n",
      "2018-07-12T17:59:58.312349: step 476, loss 0.603266, acc 0.6875\n",
      "2018-07-12T18:00:00.019616: step 477, loss 0.577599, acc 0.75\n",
      "2018-07-12T18:00:01.679119: step 478, loss 0.537196, acc 0.734375\n",
      "2018-07-12T18:00:03.302616: step 479, loss 0.581242, acc 0.703125\n",
      "2018-07-12T18:00:04.905492: step 480, loss 0.679767, acc 0.6875\n",
      "2018-07-12T18:00:06.494511: step 481, loss 0.568599, acc 0.71875\n",
      "2018-07-12T18:00:08.078277: step 482, loss 0.599387, acc 0.71875\n",
      "2018-07-12T18:00:09.626868: step 483, loss 0.734679, acc 0.65625\n",
      "2018-07-12T18:00:11.179191: step 484, loss 0.607705, acc 0.703125\n",
      "2018-07-12T18:00:12.774258: step 485, loss 0.489923, acc 0.78125\n",
      "2018-07-12T18:00:14.409314: step 486, loss 0.64513, acc 0.65625\n",
      "2018-07-12T18:00:16.063924: step 487, loss 0.519703, acc 0.734375\n",
      "2018-07-12T18:00:17.627624: step 488, loss 0.577805, acc 0.71875\n",
      "2018-07-12T18:00:19.344420: step 489, loss 0.693718, acc 0.65625\n",
      "2018-07-12T18:00:20.914355: step 490, loss 0.761664, acc 0.671875\n",
      "2018-07-12T18:00:22.538432: step 491, loss 0.789226, acc 0.578125\n",
      "2018-07-12T18:00:24.163751: step 492, loss 0.654056, acc 0.65625\n",
      "2018-07-12T18:00:25.719672: step 493, loss 0.618366, acc 0.671875\n",
      "2018-07-12T18:00:27.440705: step 494, loss 0.740763, acc 0.625\n",
      "2018-07-12T18:00:29.111815: step 495, loss 0.664952, acc 0.65625\n",
      "2018-07-12T18:00:30.834773: step 496, loss 0.459695, acc 0.8125\n",
      "2018-07-12T18:00:32.447280: step 497, loss 0.527799, acc 0.765625\n",
      "2018-07-12T18:00:34.018398: step 498, loss 0.77963, acc 0.65625\n",
      "2018-07-12T18:00:35.626340: step 499, loss 0.550266, acc 0.671875\n",
      "2018-07-12T18:00:37.339093: step 500, loss 0.597082, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:00:55.195467: step 500, loss 0.497752, acc 0.7664\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-500\n",
      "\n",
      "2018-07-12T18:00:57.745165: step 501, loss 0.498586, acc 0.734375\n",
      "2018-07-12T18:00:59.292572: step 502, loss 0.595844, acc 0.6875\n",
      "2018-07-12T18:01:00.847643: step 503, loss 0.638058, acc 0.71875\n",
      "2018-07-12T18:01:02.458119: step 504, loss 0.642333, acc 0.6875\n",
      "2018-07-12T18:01:04.045056: step 505, loss 0.675486, acc 0.65625\n",
      "2018-07-12T18:01:05.621708: step 506, loss 0.541703, acc 0.671875\n",
      "2018-07-12T18:01:07.388386: step 507, loss 0.567128, acc 0.765625\n",
      "2018-07-12T18:01:09.039025: step 508, loss 0.715308, acc 0.640625\n",
      "2018-07-12T18:01:10.679179: step 509, loss 0.63128, acc 0.640625\n",
      "2018-07-12T18:01:12.234157: step 510, loss 0.600627, acc 0.75\n",
      "2018-07-12T18:01:13.852176: step 511, loss 0.491048, acc 0.734375\n",
      "2018-07-12T18:01:15.434143: step 512, loss 0.694178, acc 0.6875\n",
      "2018-07-12T18:01:17.056622: step 513, loss 0.510705, acc 0.75\n",
      "2018-07-12T18:01:18.629074: step 514, loss 0.635864, acc 0.703125\n",
      "2018-07-12T18:01:20.210448: step 515, loss 0.50776, acc 0.8125\n",
      "2018-07-12T18:01:21.804451: step 516, loss 0.684573, acc 0.6875\n",
      "2018-07-12T18:01:23.434970: step 517, loss 0.614857, acc 0.703125\n",
      "2018-07-12T18:01:25.047579: step 518, loss 0.559208, acc 0.703125\n",
      "2018-07-12T18:01:26.670330: step 519, loss 0.540738, acc 0.78125\n",
      "2018-07-12T18:01:28.264835: step 520, loss 0.621093, acc 0.640625\n",
      "2018-07-12T18:01:30.135685: step 521, loss 0.530994, acc 0.796875\n",
      "2018-07-12T18:01:31.711996: step 522, loss 0.57072, acc 0.6875\n",
      "2018-07-12T18:01:33.367426: step 523, loss 0.635638, acc 0.65625\n",
      "2018-07-12T18:01:35.050787: step 524, loss 0.673321, acc 0.6875\n",
      "2018-07-12T18:01:36.658027: step 525, loss 0.618089, acc 0.640625\n",
      "2018-07-12T18:01:38.255086: step 526, loss 0.614418, acc 0.75\n",
      "2018-07-12T18:01:39.859392: step 527, loss 0.543286, acc 0.765625\n",
      "2018-07-12T18:01:41.552939: step 528, loss 0.604467, acc 0.625\n",
      "2018-07-12T18:01:43.172645: step 529, loss 0.519172, acc 0.734375\n",
      "2018-07-12T18:01:44.727917: step 530, loss 0.626353, acc 0.609375\n",
      "2018-07-12T18:01:46.299618: step 531, loss 0.588782, acc 0.625\n",
      "2018-07-12T18:01:47.876653: step 532, loss 0.61567, acc 0.671875\n",
      "2018-07-12T18:01:49.546219: step 533, loss 0.591835, acc 0.671875\n",
      "2018-07-12T18:01:51.147893: step 534, loss 0.552246, acc 0.734375\n",
      "2018-07-12T18:01:52.739869: step 535, loss 0.700468, acc 0.640625\n",
      "2018-07-12T18:01:54.314129: step 536, loss 0.645402, acc 0.578125\n",
      "2018-07-12T18:01:55.983681: step 537, loss 0.644686, acc 0.640625\n",
      "2018-07-12T18:01:57.545958: step 538, loss 0.541815, acc 0.734375\n",
      "2018-07-12T18:01:59.130505: step 539, loss 0.585975, acc 0.71875\n",
      "2018-07-12T18:02:00.672062: step 540, loss 0.524567, acc 0.703125\n",
      "2018-07-12T18:02:02.250361: step 541, loss 0.756704, acc 0.609375\n",
      "2018-07-12T18:02:03.834929: step 542, loss 0.638007, acc 0.65625\n",
      "2018-07-12T18:02:05.389769: step 543, loss 0.702117, acc 0.640625\n",
      "2018-07-12T18:02:06.922682: step 544, loss 0.602369, acc 0.65625\n",
      "2018-07-12T18:02:08.588037: step 545, loss 0.657213, acc 0.625\n",
      "2018-07-12T18:02:10.187311: step 546, loss 0.599275, acc 0.640625\n",
      "2018-07-12T18:02:11.782575: step 547, loss 0.531843, acc 0.734375\n",
      "2018-07-12T18:02:13.338431: step 548, loss 0.655252, acc 0.640625\n",
      "2018-07-12T18:02:14.919037: step 549, loss 0.621349, acc 0.671875\n",
      "2018-07-12T18:02:16.532146: step 550, loss 0.639073, acc 0.671875\n",
      "2018-07-12T18:02:18.149863: step 551, loss 0.582718, acc 0.671875\n",
      "2018-07-12T18:02:19.682001: step 552, loss 0.502924, acc 0.75\n",
      "2018-07-12T18:02:21.319877: step 553, loss 0.652055, acc 0.640625\n",
      "2018-07-12T18:02:22.922313: step 554, loss 0.764274, acc 0.578125\n",
      "2018-07-12T18:02:24.524080: step 555, loss 0.610715, acc 0.625\n",
      "2018-07-12T18:02:26.121681: step 556, loss 0.692518, acc 0.65625\n",
      "2018-07-12T18:02:27.678778: step 557, loss 0.542415, acc 0.734375\n",
      "2018-07-12T18:02:29.225661: step 558, loss 0.704525, acc 0.65625\n",
      "2018-07-12T18:02:30.763826: step 559, loss 0.597117, acc 0.703125\n",
      "2018-07-12T18:02:32.338244: step 560, loss 0.720805, acc 0.65625\n",
      "2018-07-12T18:02:33.926546: step 561, loss 0.777532, acc 0.6875\n",
      "2018-07-12T18:02:35.553360: step 562, loss 0.735425, acc 0.59375\n",
      "2018-07-12T18:02:37.102602: step 563, loss 0.554541, acc 0.765625\n",
      "2018-07-12T18:02:38.745309: step 564, loss 0.61248, acc 0.65625\n",
      "2018-07-12T18:02:40.321910: step 565, loss 0.559775, acc 0.703125\n",
      "2018-07-12T18:02:41.883667: step 566, loss 0.637166, acc 0.703125\n",
      "2018-07-12T18:02:43.432143: step 567, loss 0.642181, acc 0.640625\n",
      "2018-07-12T18:02:45.097364: step 568, loss 0.60188, acc 0.734375\n",
      "2018-07-12T18:02:46.713156: step 569, loss 0.509823, acc 0.703125\n",
      "2018-07-12T18:02:48.299485: step 570, loss 0.539459, acc 0.734375\n",
      "2018-07-12T18:02:49.889515: step 571, loss 0.647993, acc 0.65625\n",
      "2018-07-12T18:02:51.459011: step 572, loss 0.671956, acc 0.6875\n",
      "2018-07-12T18:02:53.033981: step 573, loss 0.456264, acc 0.75\n",
      "2018-07-12T18:02:54.704769: step 574, loss 0.613715, acc 0.671875\n",
      "2018-07-12T18:02:56.351807: step 575, loss 0.556005, acc 0.75\n",
      "2018-07-12T18:02:57.992333: step 576, loss 0.50729, acc 0.734375\n",
      "2018-07-12T18:02:59.559663: step 577, loss 0.820552, acc 0.46875\n",
      "2018-07-12T18:03:01.129583: step 578, loss 0.670958, acc 0.6875\n",
      "2018-07-12T18:03:02.703682: step 579, loss 0.716876, acc 0.625\n",
      "2018-07-12T18:03:04.328911: step 580, loss 0.596699, acc 0.625\n",
      "2018-07-12T18:03:05.953318: step 581, loss 0.458067, acc 0.84375\n",
      "2018-07-12T18:03:07.548293: step 582, loss 0.659761, acc 0.609375\n",
      "2018-07-12T18:03:09.091932: step 583, loss 0.490032, acc 0.71875\n",
      "2018-07-12T18:03:10.666977: step 584, loss 0.620901, acc 0.65625\n",
      "2018-07-12T18:03:12.239300: step 585, loss 0.522761, acc 0.765625\n",
      "2018-07-12T18:03:13.850056: step 586, loss 0.673851, acc 0.625\n",
      "2018-07-12T18:03:15.408851: step 587, loss 0.499131, acc 0.71875\n",
      "2018-07-12T18:03:17.041113: step 588, loss 0.432732, acc 0.78125\n",
      "2018-07-12T18:03:18.670990: step 589, loss 0.55695, acc 0.671875\n",
      "2018-07-12T18:03:20.281284: step 590, loss 0.633841, acc 0.703125\n",
      "2018-07-12T18:03:21.919232: step 591, loss 0.663355, acc 0.65625\n",
      "2018-07-12T18:03:23.500816: step 592, loss 0.45567, acc 0.75\n",
      "2018-07-12T18:03:25.042066: step 593, loss 0.639107, acc 0.609375\n",
      "2018-07-12T18:03:26.590318: step 594, loss 0.631722, acc 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:03:28.128361: step 595, loss 0.624718, acc 0.640625\n",
      "2018-07-12T18:03:29.716127: step 596, loss 0.452984, acc 0.796875\n",
      "2018-07-12T18:03:31.290467: step 597, loss 0.594672, acc 0.6875\n",
      "2018-07-12T18:03:32.842823: step 598, loss 0.55036, acc 0.75\n",
      "2018-07-12T18:03:34.463806: step 599, loss 0.536528, acc 0.703125\n",
      "2018-07-12T18:03:36.033471: step 600, loss 0.618044, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:03:53.795109: step 600, loss 0.504358, acc 0.7528\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-600\n",
      "\n",
      "2018-07-12T18:03:56.437225: step 601, loss 0.620071, acc 0.625\n",
      "2018-07-12T18:03:57.998300: step 602, loss 0.511815, acc 0.796875\n",
      "2018-07-12T18:03:59.685375: step 603, loss 0.55314, acc 0.734375\n",
      "2018-07-12T18:04:01.368598: step 604, loss 0.620001, acc 0.734375\n",
      "2018-07-12T18:04:03.013123: step 605, loss 0.62368, acc 0.703125\n",
      "2018-07-12T18:04:04.612248: step 606, loss 0.527583, acc 0.734375\n",
      "2018-07-12T18:04:06.289576: step 607, loss 0.570217, acc 0.703125\n",
      "2018-07-12T18:04:07.898109: step 608, loss 0.481487, acc 0.78125\n",
      "2018-07-12T18:04:09.533779: step 609, loss 0.583324, acc 0.703125\n",
      "2018-07-12T18:04:11.203756: step 610, loss 0.535425, acc 0.703125\n",
      "2018-07-12T18:04:12.839108: step 611, loss 0.58231, acc 0.671875\n",
      "2018-07-12T18:04:14.457897: step 612, loss 0.477934, acc 0.75\n",
      "2018-07-12T18:04:16.137331: step 613, loss 0.567396, acc 0.796875\n",
      "2018-07-12T18:04:17.821316: step 614, loss 0.614204, acc 0.6875\n",
      "2018-07-12T18:04:19.419839: step 615, loss 0.610022, acc 0.640625\n",
      "2018-07-12T18:04:21.006213: step 616, loss 0.572748, acc 0.71875\n",
      "2018-07-12T18:04:22.609881: step 617, loss 0.558105, acc 0.703125\n",
      "2018-07-12T18:04:24.272522: step 618, loss 0.676292, acc 0.640625\n",
      "2018-07-12T18:04:25.996063: step 619, loss 0.519498, acc 0.796875\n",
      "2018-07-12T18:04:27.607988: step 620, loss 0.611424, acc 0.71875\n",
      "2018-07-12T18:04:29.260740: step 621, loss 0.583539, acc 0.671875\n",
      "2018-07-12T18:04:30.891776: step 622, loss 0.577558, acc 0.65625\n",
      "2018-07-12T18:04:32.532790: step 623, loss 0.531009, acc 0.765625\n",
      "2018-07-12T18:04:34.184095: step 624, loss 0.618314, acc 0.65625\n",
      "2018-07-12T18:04:35.818061: step 625, loss 0.504591, acc 0.78125\n",
      "2018-07-12T18:04:37.439255: step 626, loss 0.551359, acc 0.71875\n",
      "2018-07-12T18:04:39.097002: step 627, loss 0.50548, acc 0.734375\n",
      "2018-07-12T18:04:40.739886: step 628, loss 0.489059, acc 0.71875\n",
      "2018-07-12T18:04:42.456740: step 629, loss 0.61879, acc 0.6875\n",
      "2018-07-12T18:04:44.102709: step 630, loss 0.553512, acc 0.671875\n",
      "2018-07-12T18:04:45.793720: step 631, loss 0.561001, acc 0.71875\n",
      "2018-07-12T18:04:47.479768: step 632, loss 0.579358, acc 0.71875\n",
      "2018-07-12T18:04:49.114662: step 633, loss 0.550077, acc 0.71875\n",
      "2018-07-12T18:04:50.784825: step 634, loss 0.604043, acc 0.65625\n",
      "2018-07-12T18:04:52.394823: step 635, loss 0.482736, acc 0.8125\n",
      "2018-07-12T18:04:54.027657: step 636, loss 0.546589, acc 0.671875\n",
      "2018-07-12T18:04:55.676283: step 637, loss 0.629309, acc 0.671875\n",
      "2018-07-12T18:04:57.330095: step 638, loss 0.654103, acc 0.59375\n",
      "2018-07-12T18:04:58.948031: step 639, loss 0.538355, acc 0.734375\n",
      "2018-07-12T18:05:00.598857: step 640, loss 0.51401, acc 0.75\n",
      "2018-07-12T18:05:02.222919: step 641, loss 0.70048, acc 0.5625\n",
      "2018-07-12T18:05:03.860748: step 642, loss 0.548887, acc 0.703125\n",
      "2018-07-12T18:05:05.537808: step 643, loss 0.569985, acc 0.734375\n",
      "2018-07-12T18:05:07.182598: step 644, loss 0.589568, acc 0.6875\n",
      "2018-07-12T18:05:08.793783: step 645, loss 0.601475, acc 0.671875\n",
      "2018-07-12T18:05:10.453962: step 646, loss 0.637661, acc 0.703125\n",
      "2018-07-12T18:05:12.082658: step 647, loss 0.670082, acc 0.71875\n",
      "2018-07-12T18:05:13.703793: step 648, loss 0.620439, acc 0.78125\n",
      "2018-07-12T18:05:15.493739: step 649, loss 0.4771, acc 0.75\n",
      "2018-07-12T18:05:17.159793: step 650, loss 0.507525, acc 0.75\n",
      "2018-07-12T18:05:18.793491: step 651, loss 0.615542, acc 0.640625\n",
      "2018-07-12T18:05:20.353220: step 652, loss 0.756698, acc 0.640625\n",
      "2018-07-12T18:05:22.035771: step 653, loss 0.624845, acc 0.625\n",
      "2018-07-12T18:05:23.657665: step 654, loss 0.68627, acc 0.5625\n",
      "2018-07-12T18:05:25.316865: step 655, loss 0.616497, acc 0.6875\n",
      "2018-07-12T18:05:26.936707: step 656, loss 0.519611, acc 0.78125\n",
      "2018-07-12T18:05:28.605633: step 657, loss 0.504447, acc 0.8125\n",
      "2018-07-12T18:05:30.205553: step 658, loss 0.610109, acc 0.65625\n",
      "2018-07-12T18:05:31.779655: step 659, loss 0.545116, acc 0.71875\n",
      "2018-07-12T18:05:33.370024: step 660, loss 0.527411, acc 0.75\n",
      "2018-07-12T18:05:34.952646: step 661, loss 0.635149, acc 0.71875\n",
      "2018-07-12T18:05:36.570505: step 662, loss 0.539937, acc 0.671875\n",
      "2018-07-12T18:05:38.202018: step 663, loss 0.585325, acc 0.71875\n",
      "2018-07-12T18:05:39.811370: step 664, loss 0.629533, acc 0.65625\n",
      "2018-07-12T18:05:41.461884: step 665, loss 0.573771, acc 0.625\n",
      "2018-07-12T18:05:43.090114: step 666, loss 0.571299, acc 0.75\n",
      "2018-07-12T18:05:44.748266: step 667, loss 0.54368, acc 0.71875\n",
      "2018-07-12T18:05:46.360408: step 668, loss 0.508584, acc 0.71875\n",
      "2018-07-12T18:05:48.056942: step 669, loss 0.524338, acc 0.75\n",
      "2018-07-12T18:05:49.622429: step 670, loss 0.554933, acc 0.6875\n",
      "2018-07-12T18:05:51.216971: step 671, loss 0.585954, acc 0.640625\n",
      "2018-07-12T18:05:52.848124: step 672, loss 0.438197, acc 0.796875\n",
      "2018-07-12T18:05:54.524259: step 673, loss 0.559294, acc 0.6875\n",
      "2018-07-12T18:05:56.118096: step 674, loss 0.547746, acc 0.71875\n",
      "2018-07-12T18:05:57.764812: step 675, loss 0.541532, acc 0.75\n",
      "2018-07-12T18:05:59.437221: step 676, loss 0.467646, acc 0.765625\n",
      "2018-07-12T18:06:01.132616: step 677, loss 0.57735, acc 0.71875\n",
      "2018-07-12T18:06:02.837752: step 678, loss 0.523691, acc 0.71875\n",
      "2018-07-12T18:06:04.521485: step 679, loss 0.556992, acc 0.734375\n",
      "2018-07-12T18:06:06.273383: step 680, loss 0.633879, acc 0.59375\n",
      "2018-07-12T18:06:08.117741: step 681, loss 0.540428, acc 0.6875\n",
      "2018-07-12T18:06:09.958928: step 682, loss 0.528168, acc 0.75\n",
      "2018-07-12T18:06:11.531141: step 683, loss 0.50967, acc 0.765625\n",
      "2018-07-12T18:06:13.127484: step 684, loss 0.599704, acc 0.71875\n",
      "2018-07-12T18:06:14.704584: step 685, loss 0.529919, acc 0.78125\n",
      "2018-07-12T18:06:16.314860: step 686, loss 0.586027, acc 0.703125\n",
      "2018-07-12T18:06:17.960397: step 687, loss 0.642698, acc 0.671875\n",
      "2018-07-12T18:06:19.574125: step 688, loss 0.59314, acc 0.6875\n",
      "2018-07-12T18:06:21.251596: step 689, loss 0.652578, acc 0.734375\n",
      "2018-07-12T18:06:22.912448: step 690, loss 0.454371, acc 0.71875\n",
      "2018-07-12T18:06:24.543809: step 691, loss 0.607588, acc 0.6875\n",
      "2018-07-12T18:06:26.254930: step 692, loss 0.631764, acc 0.703125\n",
      "2018-07-12T18:06:27.935016: step 693, loss 0.661662, acc 0.71875\n",
      "2018-07-12T18:06:29.591208: step 694, loss 0.49773, acc 0.71875\n",
      "2018-07-12T18:06:31.163227: step 695, loss 0.550908, acc 0.71875\n",
      "2018-07-12T18:06:32.799724: step 696, loss 0.484217, acc 0.796875\n",
      "2018-07-12T18:06:34.615983: step 697, loss 0.679753, acc 0.59375\n",
      "2018-07-12T18:06:36.263787: step 698, loss 0.500452, acc 0.703125\n",
      "2018-07-12T18:06:37.850548: step 699, loss 0.647819, acc 0.65625\n",
      "2018-07-12T18:06:39.433155: step 700, loss 0.523362, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:06:57.343395: step 700, loss 0.491553, acc 0.7652\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-700\n",
      "\n",
      "2018-07-12T18:06:59.988372: step 701, loss 0.580722, acc 0.734375\n",
      "2018-07-12T18:07:01.560272: step 702, loss 0.579272, acc 0.671875\n",
      "2018-07-12T18:07:03.189413: step 703, loss 0.505142, acc 0.734375\n",
      "2018-07-12T18:07:04.218421: step 704, loss 0.546174, acc 0.694444\n",
      "2018-07-12T18:07:05.899305: step 705, loss 0.50047, acc 0.734375\n",
      "2018-07-12T18:07:07.524630: step 706, loss 0.512536, acc 0.734375\n",
      "2018-07-12T18:07:09.152899: step 707, loss 0.568019, acc 0.71875\n",
      "2018-07-12T18:07:10.867448: step 708, loss 0.57967, acc 0.671875\n",
      "2018-07-12T18:07:12.502832: step 709, loss 0.581303, acc 0.609375\n",
      "2018-07-12T18:07:14.098342: step 710, loss 0.475804, acc 0.734375\n",
      "2018-07-12T18:07:15.670032: step 711, loss 0.551445, acc 0.71875\n",
      "2018-07-12T18:07:17.336403: step 712, loss 0.584967, acc 0.65625\n",
      "2018-07-12T18:07:18.901301: step 713, loss 0.644156, acc 0.671875\n",
      "2018-07-12T18:07:20.529002: step 714, loss 0.531423, acc 0.71875\n",
      "2018-07-12T18:07:22.155768: step 715, loss 0.490483, acc 0.703125\n",
      "2018-07-12T18:07:23.756976: step 716, loss 0.412296, acc 0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:07:25.409773: step 717, loss 0.478824, acc 0.8125\n",
      "2018-07-12T18:07:27.089400: step 718, loss 0.654994, acc 0.65625\n",
      "2018-07-12T18:07:28.764743: step 719, loss 0.563109, acc 0.734375\n",
      "2018-07-12T18:07:30.430449: step 720, loss 0.48957, acc 0.71875\n",
      "2018-07-12T18:07:32.112599: step 721, loss 0.407333, acc 0.8125\n",
      "2018-07-12T18:07:33.736916: step 722, loss 0.575546, acc 0.6875\n",
      "2018-07-12T18:07:35.333985: step 723, loss 0.382979, acc 0.859375\n",
      "2018-07-12T18:07:36.932495: step 724, loss 0.691829, acc 0.65625\n",
      "2018-07-12T18:07:38.628510: step 725, loss 0.63613, acc 0.640625\n",
      "2018-07-12T18:07:40.318900: step 726, loss 0.511815, acc 0.71875\n",
      "2018-07-12T18:07:41.943497: step 727, loss 0.645244, acc 0.71875\n",
      "2018-07-12T18:07:43.537191: step 728, loss 0.593788, acc 0.71875\n",
      "2018-07-12T18:07:45.154691: step 729, loss 0.449664, acc 0.75\n",
      "2018-07-12T18:07:46.804548: step 730, loss 0.523357, acc 0.765625\n",
      "2018-07-12T18:07:48.523630: step 731, loss 0.554679, acc 0.6875\n",
      "2018-07-12T18:07:50.169670: step 732, loss 0.616026, acc 0.640625\n",
      "2018-07-12T18:07:51.742747: step 733, loss 0.457608, acc 0.78125\n",
      "2018-07-12T18:07:53.299121: step 734, loss 0.529545, acc 0.71875\n",
      "2018-07-12T18:07:54.900627: step 735, loss 0.543338, acc 0.734375\n",
      "2018-07-12T18:07:56.532167: step 736, loss 0.484574, acc 0.765625\n",
      "2018-07-12T18:07:58.129303: step 737, loss 0.488668, acc 0.765625\n",
      "2018-07-12T18:07:59.794399: step 738, loss 0.508928, acc 0.734375\n",
      "2018-07-12T18:08:01.403380: step 739, loss 0.599745, acc 0.734375\n",
      "2018-07-12T18:08:03.024297: step 740, loss 0.457728, acc 0.765625\n",
      "2018-07-12T18:08:04.596424: step 741, loss 0.554269, acc 0.703125\n",
      "2018-07-12T18:08:06.199492: step 742, loss 0.627868, acc 0.65625\n",
      "2018-07-12T18:08:07.836220: step 743, loss 0.500113, acc 0.734375\n",
      "2018-07-12T18:08:09.506639: step 744, loss 0.576854, acc 0.671875\n",
      "2018-07-12T18:08:11.121816: step 745, loss 0.500774, acc 0.671875\n",
      "2018-07-12T18:08:12.798491: step 746, loss 0.568706, acc 0.703125\n",
      "2018-07-12T18:08:14.458616: step 747, loss 0.441758, acc 0.765625\n",
      "2018-07-12T18:08:16.082377: step 748, loss 0.595015, acc 0.6875\n",
      "2018-07-12T18:08:17.698110: step 749, loss 0.571797, acc 0.6875\n",
      "2018-07-12T18:08:19.329127: step 750, loss 0.515133, acc 0.796875\n",
      "2018-07-12T18:08:20.956804: step 751, loss 0.43991, acc 0.765625\n",
      "2018-07-12T18:08:22.582022: step 752, loss 0.517952, acc 0.765625\n",
      "2018-07-12T18:08:24.185456: step 753, loss 0.588186, acc 0.703125\n",
      "2018-07-12T18:08:25.799927: step 754, loss 0.672476, acc 0.640625\n",
      "2018-07-12T18:08:27.418315: step 755, loss 0.530379, acc 0.703125\n",
      "2018-07-12T18:08:29.064523: step 756, loss 0.423612, acc 0.765625\n",
      "2018-07-12T18:08:30.710307: step 757, loss 0.633852, acc 0.671875\n",
      "2018-07-12T18:08:32.325250: step 758, loss 0.494459, acc 0.796875\n",
      "2018-07-12T18:08:33.950138: step 759, loss 0.585113, acc 0.75\n",
      "2018-07-12T18:08:35.551413: step 760, loss 0.56349, acc 0.734375\n",
      "2018-07-12T18:08:37.132504: step 761, loss 0.592103, acc 0.65625\n",
      "2018-07-12T18:08:38.738141: step 762, loss 0.487539, acc 0.765625\n",
      "2018-07-12T18:08:40.443262: step 763, loss 0.581445, acc 0.640625\n",
      "2018-07-12T18:08:42.047157: step 764, loss 0.662856, acc 0.703125\n",
      "2018-07-12T18:08:43.608590: step 765, loss 0.53792, acc 0.703125\n",
      "2018-07-12T18:08:45.219976: step 766, loss 0.496468, acc 0.765625\n",
      "2018-07-12T18:08:46.813304: step 767, loss 0.564465, acc 0.65625\n",
      "2018-07-12T18:08:48.413583: step 768, loss 0.535827, acc 0.75\n",
      "2018-07-12T18:08:50.163978: step 769, loss 0.574659, acc 0.6875\n",
      "2018-07-12T18:08:51.819481: step 770, loss 0.606157, acc 0.703125\n",
      "2018-07-12T18:08:53.443422: step 771, loss 0.563811, acc 0.75\n",
      "2018-07-12T18:08:55.090150: step 772, loss 0.663518, acc 0.640625\n",
      "2018-07-12T18:08:56.812232: step 773, loss 0.487667, acc 0.765625\n",
      "2018-07-12T18:08:58.466476: step 774, loss 0.548469, acc 0.6875\n",
      "2018-07-12T18:09:00.040792: step 775, loss 0.495018, acc 0.78125\n",
      "2018-07-12T18:09:01.668461: step 776, loss 0.541249, acc 0.71875\n",
      "2018-07-12T18:09:03.294199: step 777, loss 0.537497, acc 0.734375\n",
      "2018-07-12T18:09:04.870826: step 778, loss 0.551349, acc 0.703125\n",
      "2018-07-12T18:09:06.523207: step 779, loss 0.500806, acc 0.78125\n",
      "2018-07-12T18:09:08.087463: step 780, loss 0.54649, acc 0.75\n",
      "2018-07-12T18:09:09.679960: step 781, loss 0.667721, acc 0.6875\n",
      "2018-07-12T18:09:11.324190: step 782, loss 0.571274, acc 0.6875\n",
      "2018-07-12T18:09:13.013201: step 783, loss 0.513505, acc 0.703125\n",
      "2018-07-12T18:09:14.671439: step 784, loss 0.57469, acc 0.765625\n",
      "2018-07-12T18:09:16.267280: step 785, loss 0.564323, acc 0.6875\n",
      "2018-07-12T18:09:17.935926: step 786, loss 0.443324, acc 0.828125\n",
      "2018-07-12T18:09:19.560899: step 787, loss 0.64635, acc 0.71875\n",
      "2018-07-12T18:09:21.250941: step 788, loss 0.555543, acc 0.65625\n",
      "2018-07-12T18:09:22.977882: step 789, loss 0.494796, acc 0.75\n",
      "2018-07-12T18:09:24.660937: step 790, loss 0.510357, acc 0.734375\n",
      "2018-07-12T18:09:26.335137: step 791, loss 0.664872, acc 0.71875\n",
      "2018-07-12T18:09:28.028825: step 792, loss 0.541122, acc 0.703125\n",
      "2018-07-12T18:09:29.633284: step 793, loss 0.428432, acc 0.78125\n",
      "2018-07-12T18:09:31.252378: step 794, loss 0.463384, acc 0.75\n",
      "2018-07-12T18:09:32.894281: step 795, loss 0.613487, acc 0.703125\n",
      "2018-07-12T18:09:34.566927: step 796, loss 0.490846, acc 0.703125\n",
      "2018-07-12T18:09:36.327877: step 797, loss 0.537846, acc 0.703125\n",
      "2018-07-12T18:09:37.904166: step 798, loss 0.613877, acc 0.6875\n",
      "2018-07-12T18:09:39.545410: step 799, loss 0.495724, acc 0.703125\n",
      "2018-07-12T18:09:41.211270: step 800, loss 0.579743, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:09:59.434691: step 800, loss 0.470531, acc 0.7988\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-800\n",
      "\n",
      "2018-07-12T18:10:02.022024: step 801, loss 0.51577, acc 0.71875\n",
      "2018-07-12T18:10:03.623513: step 802, loss 0.54464, acc 0.734375\n",
      "2018-07-12T18:10:05.278472: step 803, loss 0.491245, acc 0.71875\n",
      "2018-07-12T18:10:06.903306: step 804, loss 0.644346, acc 0.609375\n",
      "2018-07-12T18:10:08.507687: step 805, loss 0.610502, acc 0.71875\n",
      "2018-07-12T18:10:10.136486: step 806, loss 0.55313, acc 0.703125\n",
      "2018-07-12T18:10:11.760446: step 807, loss 0.491496, acc 0.796875\n",
      "2018-07-12T18:10:13.349731: step 808, loss 0.550565, acc 0.703125\n",
      "2018-07-12T18:10:14.933110: step 809, loss 0.521467, acc 0.75\n",
      "2018-07-12T18:10:16.565934: step 810, loss 0.494629, acc 0.71875\n",
      "2018-07-12T18:10:18.154444: step 811, loss 0.597078, acc 0.59375\n",
      "2018-07-12T18:10:19.673987: step 812, loss 0.570574, acc 0.703125\n",
      "2018-07-12T18:10:21.230786: step 813, loss 0.596666, acc 0.734375\n",
      "2018-07-12T18:10:22.902478: step 814, loss 0.586905, acc 0.703125\n",
      "2018-07-12T18:10:24.529087: step 815, loss 0.564057, acc 0.71875\n",
      "2018-07-12T18:10:26.154944: step 816, loss 0.55174, acc 0.65625\n",
      "2018-07-12T18:10:27.762656: step 817, loss 0.504907, acc 0.765625\n",
      "2018-07-12T18:10:29.395847: step 818, loss 0.498892, acc 0.703125\n",
      "2018-07-12T18:10:31.021276: step 819, loss 0.511717, acc 0.75\n",
      "2018-07-12T18:10:32.652149: step 820, loss 0.595467, acc 0.703125\n",
      "2018-07-12T18:10:34.274636: step 821, loss 0.532708, acc 0.734375\n",
      "2018-07-12T18:10:35.859720: step 822, loss 0.580678, acc 0.703125\n",
      "2018-07-12T18:10:37.457833: step 823, loss 0.604934, acc 0.703125\n",
      "2018-07-12T18:10:39.050259: step 824, loss 0.561767, acc 0.703125\n",
      "2018-07-12T18:10:40.758403: step 825, loss 0.611736, acc 0.71875\n",
      "2018-07-12T18:10:42.370219: step 826, loss 0.485891, acc 0.765625\n",
      "2018-07-12T18:10:44.016067: step 827, loss 0.580818, acc 0.6875\n",
      "2018-07-12T18:10:45.702921: step 828, loss 0.538296, acc 0.703125\n",
      "2018-07-12T18:10:47.338071: step 829, loss 0.521871, acc 0.71875\n",
      "2018-07-12T18:10:48.961392: step 830, loss 0.502196, acc 0.734375\n",
      "2018-07-12T18:10:50.551100: step 831, loss 0.623045, acc 0.6875\n",
      "2018-07-12T18:10:52.232252: step 832, loss 0.624305, acc 0.65625\n",
      "2018-07-12T18:10:53.864423: step 833, loss 0.519383, acc 0.6875\n",
      "2018-07-12T18:10:55.419383: step 834, loss 0.517173, acc 0.765625\n",
      "2018-07-12T18:10:57.028544: step 835, loss 0.486295, acc 0.765625\n",
      "2018-07-12T18:10:58.756329: step 836, loss 0.606207, acc 0.65625\n",
      "2018-07-12T18:11:00.446608: step 837, loss 0.659916, acc 0.65625\n",
      "2018-07-12T18:11:02.103904: step 838, loss 0.574551, acc 0.6875\n",
      "2018-07-12T18:11:03.718891: step 839, loss 0.632944, acc 0.671875\n",
      "2018-07-12T18:11:05.340073: step 840, loss 0.501084, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:11:07.033148: step 841, loss 0.501259, acc 0.703125\n",
      "2018-07-12T18:11:08.763032: step 842, loss 0.470654, acc 0.828125\n",
      "2018-07-12T18:11:10.401969: step 843, loss 0.565398, acc 0.6875\n",
      "2018-07-12T18:11:12.059345: step 844, loss 0.536788, acc 0.734375\n",
      "2018-07-12T18:11:13.700766: step 845, loss 0.451512, acc 0.828125\n",
      "2018-07-12T18:11:15.317765: step 846, loss 0.545279, acc 0.703125\n",
      "2018-07-12T18:11:16.898983: step 847, loss 0.572511, acc 0.75\n",
      "2018-07-12T18:11:18.504944: step 848, loss 0.459154, acc 0.796875\n",
      "2018-07-12T18:11:20.223764: step 849, loss 0.475344, acc 0.78125\n",
      "2018-07-12T18:11:21.987199: step 850, loss 0.442488, acc 0.78125\n",
      "2018-07-12T18:11:23.667825: step 851, loss 0.549296, acc 0.765625\n",
      "2018-07-12T18:11:25.208580: step 852, loss 0.530036, acc 0.6875\n",
      "2018-07-12T18:11:26.808886: step 853, loss 0.480505, acc 0.78125\n",
      "2018-07-12T18:11:28.435885: step 854, loss 0.637157, acc 0.71875\n",
      "2018-07-12T18:11:30.068721: step 855, loss 0.616768, acc 0.734375\n",
      "2018-07-12T18:11:31.809235: step 856, loss 0.59393, acc 0.6875\n",
      "2018-07-12T18:11:33.470628: step 857, loss 0.445381, acc 0.8125\n",
      "2018-07-12T18:11:35.133508: step 858, loss 0.512747, acc 0.71875\n",
      "2018-07-12T18:11:36.750100: step 859, loss 0.663611, acc 0.703125\n",
      "2018-07-12T18:11:38.400838: step 860, loss 0.590951, acc 0.65625\n",
      "2018-07-12T18:11:40.049533: step 861, loss 0.603165, acc 0.671875\n",
      "2018-07-12T18:11:41.682009: step 862, loss 0.59881, acc 0.640625\n",
      "2018-07-12T18:11:43.285188: step 863, loss 0.579787, acc 0.6875\n",
      "2018-07-12T18:11:44.887270: step 864, loss 0.632883, acc 0.578125\n",
      "2018-07-12T18:11:46.489290: step 865, loss 0.539605, acc 0.765625\n",
      "2018-07-12T18:11:48.126432: step 866, loss 0.430758, acc 0.765625\n",
      "2018-07-12T18:11:49.687141: step 867, loss 0.569429, acc 0.703125\n",
      "2018-07-12T18:11:51.350272: step 868, loss 0.472912, acc 0.765625\n",
      "2018-07-12T18:11:53.102971: step 869, loss 0.505076, acc 0.703125\n",
      "2018-07-12T18:11:54.725479: step 870, loss 0.517528, acc 0.765625\n",
      "2018-07-12T18:11:56.312703: step 871, loss 0.519282, acc 0.734375\n",
      "2018-07-12T18:11:57.926325: step 872, loss 0.530097, acc 0.65625\n",
      "2018-07-12T18:11:59.511757: step 873, loss 0.466484, acc 0.765625\n",
      "2018-07-12T18:12:01.109655: step 874, loss 0.445287, acc 0.78125\n",
      "2018-07-12T18:12:02.724088: step 875, loss 0.530907, acc 0.703125\n",
      "2018-07-12T18:12:04.369041: step 876, loss 0.467912, acc 0.71875\n",
      "2018-07-12T18:12:06.007141: step 877, loss 0.616852, acc 0.6875\n",
      "2018-07-12T18:12:07.598163: step 878, loss 0.531271, acc 0.828125\n",
      "2018-07-12T18:12:09.204838: step 879, loss 0.523056, acc 0.78125\n",
      "2018-07-12T18:12:10.939023: step 880, loss 0.565048, acc 0.671875\n",
      "2018-07-12T18:12:12.566412: step 881, loss 0.580058, acc 0.6875\n",
      "2018-07-12T18:12:14.183113: step 882, loss 0.432162, acc 0.828125\n",
      "2018-07-12T18:12:15.790013: step 883, loss 0.41036, acc 0.84375\n",
      "2018-07-12T18:12:17.430084: step 884, loss 0.546886, acc 0.75\n",
      "2018-07-12T18:12:19.086657: step 885, loss 0.455241, acc 0.703125\n",
      "2018-07-12T18:12:20.710970: step 886, loss 0.557094, acc 0.75\n",
      "2018-07-12T18:12:22.301334: step 887, loss 0.486601, acc 0.75\n",
      "2018-07-12T18:12:23.982024: step 888, loss 0.805878, acc 0.65625\n",
      "2018-07-12T18:12:25.586562: step 889, loss 0.44444, acc 0.765625\n",
      "2018-07-12T18:12:27.176802: step 890, loss 0.433436, acc 0.8125\n",
      "2018-07-12T18:12:28.781633: step 891, loss 0.533719, acc 0.71875\n",
      "2018-07-12T18:12:30.384455: step 892, loss 0.398524, acc 0.8125\n",
      "2018-07-12T18:12:32.074548: step 893, loss 0.469387, acc 0.75\n",
      "2018-07-12T18:12:33.681866: step 894, loss 0.633403, acc 0.65625\n",
      "2018-07-12T18:12:35.279490: step 895, loss 0.657767, acc 0.609375\n",
      "2018-07-12T18:12:36.904821: step 896, loss 0.640927, acc 0.625\n",
      "2018-07-12T18:12:38.586879: step 897, loss 0.570483, acc 0.734375\n",
      "2018-07-12T18:12:40.263698: step 898, loss 0.581594, acc 0.671875\n",
      "2018-07-12T18:12:41.899172: step 899, loss 0.547637, acc 0.734375\n",
      "2018-07-12T18:12:43.555831: step 900, loss 0.514165, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:13:01.647604: step 900, loss 0.511609, acc 0.7344\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-900\n",
      "\n",
      "2018-07-12T18:13:04.185973: step 901, loss 0.576639, acc 0.65625\n",
      "2018-07-12T18:13:05.756733: step 902, loss 0.564701, acc 0.71875\n",
      "2018-07-12T18:13:07.355512: step 903, loss 0.507353, acc 0.75\n",
      "2018-07-12T18:13:09.035915: step 904, loss 0.670182, acc 0.75\n",
      "2018-07-12T18:13:10.676018: step 905, loss 0.449983, acc 0.765625\n",
      "2018-07-12T18:13:12.372675: step 906, loss 0.611407, acc 0.71875\n",
      "2018-07-12T18:13:14.093986: step 907, loss 0.543992, acc 0.734375\n",
      "2018-07-12T18:13:15.696288: step 908, loss 0.54114, acc 0.71875\n",
      "2018-07-12T18:13:17.335579: step 909, loss 0.619133, acc 0.640625\n",
      "2018-07-12T18:13:19.046075: step 910, loss 0.45657, acc 0.75\n",
      "2018-07-12T18:13:20.707411: step 911, loss 0.583157, acc 0.65625\n",
      "2018-07-12T18:13:22.327793: step 912, loss 0.590401, acc 0.6875\n",
      "2018-07-12T18:13:23.936591: step 913, loss 0.518025, acc 0.75\n",
      "2018-07-12T18:13:25.607383: step 914, loss 0.492159, acc 0.78125\n",
      "2018-07-12T18:13:27.304882: step 915, loss 0.616623, acc 0.6875\n",
      "2018-07-12T18:13:28.951890: step 916, loss 0.457666, acc 0.75\n",
      "2018-07-12T18:13:30.648890: step 917, loss 0.481273, acc 0.703125\n",
      "2018-07-12T18:13:32.299747: step 918, loss 0.570114, acc 0.734375\n",
      "2018-07-12T18:13:33.875645: step 919, loss 0.468961, acc 0.765625\n",
      "2018-07-12T18:13:35.478952: step 920, loss 0.488485, acc 0.765625\n",
      "2018-07-12T18:13:37.054629: step 921, loss 0.646524, acc 0.640625\n",
      "2018-07-12T18:13:38.687357: step 922, loss 0.512728, acc 0.71875\n",
      "2018-07-12T18:13:40.339708: step 923, loss 0.673375, acc 0.578125\n",
      "2018-07-12T18:13:41.983913: step 924, loss 0.474512, acc 0.796875\n",
      "2018-07-12T18:13:43.615108: step 925, loss 0.62377, acc 0.640625\n",
      "2018-07-12T18:13:45.300869: step 926, loss 0.529444, acc 0.71875\n",
      "2018-07-12T18:13:46.949776: step 927, loss 0.541626, acc 0.703125\n",
      "2018-07-12T18:13:48.589961: step 928, loss 0.626989, acc 0.71875\n",
      "2018-07-12T18:13:50.225027: step 929, loss 0.466115, acc 0.8125\n",
      "2018-07-12T18:13:51.872578: step 930, loss 0.471037, acc 0.734375\n",
      "2018-07-12T18:13:53.520562: step 931, loss 0.635887, acc 0.703125\n",
      "2018-07-12T18:13:55.133740: step 932, loss 0.620911, acc 0.6875\n",
      "2018-07-12T18:13:56.768402: step 933, loss 0.545419, acc 0.75\n",
      "2018-07-12T18:13:58.462408: step 934, loss 0.620412, acc 0.625\n",
      "2018-07-12T18:14:00.153808: step 935, loss 0.469631, acc 0.765625\n",
      "2018-07-12T18:14:01.777846: step 936, loss 0.530236, acc 0.75\n",
      "2018-07-12T18:14:03.440622: step 937, loss 0.502297, acc 0.765625\n",
      "2018-07-12T18:14:05.074330: step 938, loss 0.503976, acc 0.796875\n",
      "2018-07-12T18:14:06.709015: step 939, loss 0.455382, acc 0.78125\n",
      "2018-07-12T18:14:08.360730: step 940, loss 0.558948, acc 0.71875\n",
      "2018-07-12T18:14:09.999915: step 941, loss 0.474293, acc 0.78125\n",
      "2018-07-12T18:14:11.625006: step 942, loss 0.53362, acc 0.75\n",
      "2018-07-12T18:14:13.317661: step 943, loss 0.639162, acc 0.65625\n",
      "2018-07-12T18:14:14.948683: step 944, loss 0.49321, acc 0.765625\n",
      "2018-07-12T18:14:16.589514: step 945, loss 0.484886, acc 0.75\n",
      "2018-07-12T18:14:18.211750: step 946, loss 0.557542, acc 0.703125\n",
      "2018-07-12T18:14:19.906118: step 947, loss 0.527974, acc 0.734375\n",
      "2018-07-12T18:14:21.577828: step 948, loss 0.492817, acc 0.78125\n",
      "2018-07-12T18:14:23.230396: step 949, loss 0.533165, acc 0.71875\n",
      "2018-07-12T18:14:24.852138: step 950, loss 0.636312, acc 0.734375\n",
      "2018-07-12T18:14:26.473836: step 951, loss 0.574845, acc 0.71875\n",
      "2018-07-12T18:14:28.147625: step 952, loss 0.654781, acc 0.65625\n",
      "2018-07-12T18:14:29.786146: step 953, loss 0.522954, acc 0.71875\n",
      "2018-07-12T18:14:31.442966: step 954, loss 0.646671, acc 0.609375\n",
      "2018-07-12T18:14:33.096348: step 955, loss 0.590305, acc 0.734375\n",
      "2018-07-12T18:14:34.728349: step 956, loss 0.633704, acc 0.671875\n",
      "2018-07-12T18:14:36.345170: step 957, loss 0.559731, acc 0.671875\n",
      "2018-07-12T18:14:37.988385: step 958, loss 0.480951, acc 0.828125\n",
      "2018-07-12T18:14:39.705533: step 959, loss 0.536003, acc 0.734375\n",
      "2018-07-12T18:14:41.367418: step 960, loss 0.448515, acc 0.78125\n",
      "2018-07-12T18:14:43.000982: step 961, loss 0.509921, acc 0.765625\n",
      "2018-07-12T18:14:44.748838: step 962, loss 0.525449, acc 0.78125\n",
      "2018-07-12T18:14:46.391506: step 963, loss 0.553993, acc 0.734375\n",
      "2018-07-12T18:14:48.044317: step 964, loss 0.503231, acc 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:14:49.667229: step 965, loss 0.561102, acc 0.71875\n",
      "2018-07-12T18:14:51.315466: step 966, loss 0.476286, acc 0.734375\n",
      "2018-07-12T18:14:52.910344: step 967, loss 0.559142, acc 0.75\n",
      "2018-07-12T18:14:54.637293: step 968, loss 0.628674, acc 0.6875\n",
      "2018-07-12T18:14:56.298569: step 969, loss 0.444398, acc 0.828125\n",
      "2018-07-12T18:14:57.958908: step 970, loss 0.582163, acc 0.703125\n",
      "2018-07-12T18:14:59.640288: step 971, loss 0.504507, acc 0.734375\n",
      "2018-07-12T18:15:01.303316: step 972, loss 0.424679, acc 0.75\n",
      "2018-07-12T18:15:02.944724: step 973, loss 0.37324, acc 0.875\n",
      "2018-07-12T18:15:04.579652: step 974, loss 0.529567, acc 0.734375\n",
      "2018-07-12T18:15:06.189894: step 975, loss 0.586397, acc 0.71875\n",
      "2018-07-12T18:15:07.898487: step 976, loss 0.410079, acc 0.78125\n",
      "2018-07-12T18:15:09.546628: step 977, loss 0.655207, acc 0.703125\n",
      "2018-07-12T18:15:11.203241: step 978, loss 0.403189, acc 0.8125\n",
      "2018-07-12T18:15:12.847329: step 979, loss 0.378558, acc 0.859375\n",
      "2018-07-12T18:15:14.509218: step 980, loss 0.565355, acc 0.734375\n",
      "2018-07-12T18:15:16.116307: step 981, loss 0.603416, acc 0.671875\n",
      "2018-07-12T18:15:17.759807: step 982, loss 0.728584, acc 0.625\n",
      "2018-07-12T18:15:19.390111: step 983, loss 0.532022, acc 0.75\n",
      "2018-07-12T18:15:21.045315: step 984, loss 0.537321, acc 0.71875\n",
      "2018-07-12T18:15:22.680742: step 985, loss 0.512862, acc 0.734375\n",
      "2018-07-12T18:15:24.516142: step 986, loss 0.603297, acc 0.65625\n",
      "2018-07-12T18:15:26.134036: step 987, loss 0.443745, acc 0.796875\n",
      "2018-07-12T18:15:27.791565: step 988, loss 0.742556, acc 0.640625\n",
      "2018-07-12T18:15:29.466642: step 989, loss 0.540092, acc 0.71875\n",
      "2018-07-12T18:15:31.178873: step 990, loss 0.45529, acc 0.8125\n",
      "2018-07-12T18:15:32.793680: step 991, loss 0.450347, acc 0.78125\n",
      "2018-07-12T18:15:34.387155: step 992, loss 0.430408, acc 0.78125\n",
      "2018-07-12T18:15:36.090131: step 993, loss 0.604662, acc 0.671875\n",
      "2018-07-12T18:15:37.751176: step 994, loss 0.533939, acc 0.734375\n",
      "2018-07-12T18:15:39.413036: step 995, loss 0.635076, acc 0.65625\n",
      "2018-07-12T18:15:41.022739: step 996, loss 0.518211, acc 0.734375\n",
      "2018-07-12T18:15:42.634649: step 997, loss 0.476742, acc 0.78125\n",
      "2018-07-12T18:15:44.279657: step 998, loss 0.541245, acc 0.75\n",
      "2018-07-12T18:15:46.006862: step 999, loss 0.526645, acc 0.71875\n",
      "2018-07-12T18:15:47.635200: step 1000, loss 0.590489, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:16:05.470819: step 1000, loss 0.464326, acc 0.7964\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1000\n",
      "\n",
      "2018-07-12T18:16:07.978533: step 1001, loss 0.551978, acc 0.765625\n",
      "2018-07-12T18:16:09.590516: step 1002, loss 0.410613, acc 0.78125\n",
      "2018-07-12T18:16:11.187855: step 1003, loss 0.534001, acc 0.71875\n",
      "2018-07-12T18:16:12.793578: step 1004, loss 0.561939, acc 0.734375\n",
      "2018-07-12T18:16:14.463852: step 1005, loss 0.444296, acc 0.78125\n",
      "2018-07-12T18:16:16.082453: step 1006, loss 0.438597, acc 0.765625\n",
      "2018-07-12T18:16:17.754972: step 1007, loss 0.449852, acc 0.828125\n",
      "2018-07-12T18:16:19.379252: step 1008, loss 0.550308, acc 0.6875\n",
      "2018-07-12T18:16:21.026456: step 1009, loss 0.617772, acc 0.6875\n",
      "2018-07-12T18:16:22.683485: step 1010, loss 0.419402, acc 0.8125\n",
      "2018-07-12T18:16:24.333394: step 1011, loss 0.469542, acc 0.734375\n",
      "2018-07-12T18:16:26.009302: step 1012, loss 0.544609, acc 0.734375\n",
      "2018-07-12T18:16:27.701140: step 1013, loss 0.413989, acc 0.8125\n",
      "2018-07-12T18:16:29.473412: step 1014, loss 0.490384, acc 0.71875\n",
      "2018-07-12T18:16:31.126371: step 1015, loss 0.512339, acc 0.734375\n",
      "2018-07-12T18:16:32.832333: step 1016, loss 0.59104, acc 0.703125\n",
      "2018-07-12T18:16:34.479962: step 1017, loss 0.57228, acc 0.71875\n",
      "2018-07-12T18:16:36.061788: step 1018, loss 0.630003, acc 0.609375\n",
      "2018-07-12T18:16:37.647786: step 1019, loss 0.474518, acc 0.765625\n",
      "2018-07-12T18:16:39.309844: step 1020, loss 0.463478, acc 0.765625\n",
      "2018-07-12T18:16:40.918788: step 1021, loss 0.683397, acc 0.65625\n",
      "2018-07-12T18:16:42.518743: step 1022, loss 0.791833, acc 0.609375\n",
      "2018-07-12T18:16:44.081154: step 1023, loss 0.419366, acc 0.796875\n",
      "2018-07-12T18:16:45.715384: step 1024, loss 0.70678, acc 0.640625\n",
      "2018-07-12T18:16:47.346831: step 1025, loss 0.447072, acc 0.84375\n",
      "2018-07-12T18:16:49.028760: step 1026, loss 0.623722, acc 0.703125\n",
      "2018-07-12T18:16:50.665799: step 1027, loss 0.563115, acc 0.6875\n",
      "2018-07-12T18:16:52.309148: step 1028, loss 0.532322, acc 0.703125\n",
      "2018-07-12T18:16:54.027402: step 1029, loss 0.503782, acc 0.734375\n",
      "2018-07-12T18:16:55.631015: step 1030, loss 0.474897, acc 0.703125\n",
      "2018-07-12T18:16:57.284367: step 1031, loss 0.41707, acc 0.828125\n",
      "2018-07-12T18:16:58.921319: step 1032, loss 0.51142, acc 0.71875\n",
      "2018-07-12T18:17:00.488453: step 1033, loss 0.589844, acc 0.78125\n",
      "2018-07-12T18:17:02.166093: step 1034, loss 0.477474, acc 0.765625\n",
      "2018-07-12T18:17:03.801694: step 1035, loss 0.552301, acc 0.6875\n",
      "2018-07-12T18:17:05.431287: step 1036, loss 0.399832, acc 0.796875\n",
      "2018-07-12T18:17:07.093624: step 1037, loss 0.506924, acc 0.796875\n",
      "2018-07-12T18:17:08.715081: step 1038, loss 0.469623, acc 0.828125\n",
      "2018-07-12T18:17:10.427331: step 1039, loss 0.462091, acc 0.765625\n",
      "2018-07-12T18:17:12.123273: step 1040, loss 0.526342, acc 0.765625\n",
      "2018-07-12T18:17:13.769101: step 1041, loss 0.475524, acc 0.75\n",
      "2018-07-12T18:17:15.497545: step 1042, loss 0.484517, acc 0.703125\n",
      "2018-07-12T18:17:17.240682: step 1043, loss 0.518412, acc 0.734375\n",
      "2018-07-12T18:17:18.846517: step 1044, loss 0.514334, acc 0.703125\n",
      "2018-07-12T18:17:20.468253: step 1045, loss 0.560929, acc 0.8125\n",
      "2018-07-12T18:17:22.102754: step 1046, loss 0.511755, acc 0.734375\n",
      "2018-07-12T18:17:23.785334: step 1047, loss 0.466801, acc 0.75\n",
      "2018-07-12T18:17:25.443041: step 1048, loss 0.462624, acc 0.765625\n",
      "2018-07-12T18:17:27.008462: step 1049, loss 0.549645, acc 0.71875\n",
      "2018-07-12T18:17:28.733474: step 1050, loss 0.634397, acc 0.75\n",
      "2018-07-12T18:17:30.359584: step 1051, loss 0.604645, acc 0.640625\n",
      "2018-07-12T18:17:32.003354: step 1052, loss 0.523465, acc 0.75\n",
      "2018-07-12T18:17:33.579141: step 1053, loss 0.537662, acc 0.734375\n",
      "2018-07-12T18:17:35.277131: step 1054, loss 0.470431, acc 0.6875\n",
      "2018-07-12T18:17:36.960317: step 1055, loss 0.458336, acc 0.765625\n",
      "2018-07-12T18:17:38.061258: step 1056, loss 0.665244, acc 0.722222\n",
      "2018-07-12T18:17:39.685324: step 1057, loss 0.49529, acc 0.75\n",
      "2018-07-12T18:17:41.311583: step 1058, loss 0.554181, acc 0.65625\n",
      "2018-07-12T18:17:42.989391: step 1059, loss 0.550737, acc 0.71875\n",
      "2018-07-12T18:17:44.627900: step 1060, loss 0.517117, acc 0.6875\n",
      "2018-07-12T18:17:46.312915: step 1061, loss 0.566437, acc 0.8125\n",
      "2018-07-12T18:17:47.974634: step 1062, loss 0.556325, acc 0.640625\n",
      "2018-07-12T18:17:49.604056: step 1063, loss 0.522098, acc 0.765625\n",
      "2018-07-12T18:17:51.220133: step 1064, loss 0.493357, acc 0.78125\n",
      "2018-07-12T18:17:52.893611: step 1065, loss 0.496516, acc 0.71875\n",
      "2018-07-12T18:17:54.552288: step 1066, loss 0.522418, acc 0.6875\n",
      "2018-07-12T18:17:56.266411: step 1067, loss 0.425736, acc 0.78125\n",
      "2018-07-12T18:17:57.947201: step 1068, loss 0.366262, acc 0.875\n",
      "2018-07-12T18:17:59.542645: step 1069, loss 0.392366, acc 0.765625\n",
      "2018-07-12T18:18:01.120426: step 1070, loss 0.432593, acc 0.765625\n",
      "2018-07-12T18:18:02.785313: step 1071, loss 0.421872, acc 0.765625\n",
      "2018-07-12T18:18:04.404725: step 1072, loss 0.456849, acc 0.78125\n",
      "2018-07-12T18:18:06.068051: step 1073, loss 0.39805, acc 0.84375\n",
      "2018-07-12T18:18:07.821341: step 1074, loss 0.41068, acc 0.75\n",
      "2018-07-12T18:18:09.470268: step 1075, loss 0.525648, acc 0.703125\n",
      "2018-07-12T18:18:11.140939: step 1076, loss 0.527795, acc 0.75\n",
      "2018-07-12T18:18:12.773085: step 1077, loss 0.602099, acc 0.71875\n",
      "2018-07-12T18:18:14.411669: step 1078, loss 0.577588, acc 0.703125\n",
      "2018-07-12T18:18:16.061235: step 1079, loss 0.523657, acc 0.71875\n",
      "2018-07-12T18:18:17.735028: step 1080, loss 0.633656, acc 0.71875\n",
      "2018-07-12T18:18:19.462963: step 1081, loss 0.49083, acc 0.78125\n",
      "2018-07-12T18:18:21.086094: step 1082, loss 0.599401, acc 0.640625\n",
      "2018-07-12T18:18:22.696536: step 1083, loss 0.40228, acc 0.875\n",
      "2018-07-12T18:18:24.422875: step 1084, loss 0.417029, acc 0.78125\n",
      "2018-07-12T18:18:26.116815: step 1085, loss 0.492971, acc 0.71875\n",
      "2018-07-12T18:18:27.724360: step 1086, loss 0.452229, acc 0.8125\n",
      "2018-07-12T18:18:29.301052: step 1087, loss 0.434466, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:18:30.906816: step 1088, loss 0.396123, acc 0.796875\n",
      "2018-07-12T18:18:32.627084: step 1089, loss 0.456225, acc 0.765625\n",
      "2018-07-12T18:18:34.280993: step 1090, loss 0.477263, acc 0.765625\n",
      "2018-07-12T18:18:35.903371: step 1091, loss 0.49758, acc 0.734375\n",
      "2018-07-12T18:18:37.582029: step 1092, loss 0.478169, acc 0.8125\n",
      "2018-07-12T18:18:39.269511: step 1093, loss 0.425057, acc 0.84375\n",
      "2018-07-12T18:18:40.978141: step 1094, loss 0.520028, acc 0.75\n",
      "2018-07-12T18:18:42.637279: step 1095, loss 0.497013, acc 0.75\n",
      "2018-07-12T18:18:44.333684: step 1096, loss 0.456296, acc 0.78125\n",
      "2018-07-12T18:18:45.964833: step 1097, loss 0.580527, acc 0.734375\n",
      "2018-07-12T18:18:47.629931: step 1098, loss 0.489078, acc 0.78125\n",
      "2018-07-12T18:18:49.305924: step 1099, loss 0.41124, acc 0.734375\n",
      "2018-07-12T18:18:50.944991: step 1100, loss 0.3955, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:19:08.694919: step 1100, loss 0.442903, acc 0.8116\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1100\n",
      "\n",
      "2018-07-12T18:19:11.402293: step 1101, loss 0.411799, acc 0.796875\n",
      "2018-07-12T18:19:13.025973: step 1102, loss 0.412955, acc 0.75\n",
      "2018-07-12T18:19:14.688776: step 1103, loss 0.388506, acc 0.765625\n",
      "2018-07-12T18:19:16.279296: step 1104, loss 0.418022, acc 0.796875\n",
      "2018-07-12T18:19:17.930536: step 1105, loss 0.414656, acc 0.796875\n",
      "2018-07-12T18:19:19.560820: step 1106, loss 0.50442, acc 0.78125\n",
      "2018-07-12T18:19:21.256102: step 1107, loss 0.446281, acc 0.796875\n",
      "2018-07-12T18:19:22.927675: step 1108, loss 0.407815, acc 0.8125\n",
      "2018-07-12T18:19:24.603198: step 1109, loss 0.421249, acc 0.8125\n",
      "2018-07-12T18:19:26.298409: step 1110, loss 0.537026, acc 0.734375\n",
      "2018-07-12T18:19:27.938390: step 1111, loss 0.470839, acc 0.78125\n",
      "2018-07-12T18:19:29.558835: step 1112, loss 0.504442, acc 0.8125\n",
      "2018-07-12T18:19:31.173426: step 1113, loss 0.344101, acc 0.84375\n",
      "2018-07-12T18:19:32.845586: step 1114, loss 0.499269, acc 0.71875\n",
      "2018-07-12T18:19:34.517561: step 1115, loss 0.551107, acc 0.671875\n",
      "2018-07-12T18:19:36.131832: step 1116, loss 0.612734, acc 0.71875\n",
      "2018-07-12T18:19:37.861733: step 1117, loss 0.453331, acc 0.734375\n",
      "2018-07-12T18:19:39.527954: step 1118, loss 0.473101, acc 0.78125\n",
      "2018-07-12T18:19:41.136050: step 1119, loss 0.422278, acc 0.8125\n",
      "2018-07-12T18:19:42.798001: step 1120, loss 0.459976, acc 0.765625\n",
      "2018-07-12T18:19:44.426400: step 1121, loss 0.472553, acc 0.78125\n",
      "2018-07-12T18:19:46.024824: step 1122, loss 0.45507, acc 0.765625\n",
      "2018-07-12T18:19:47.692273: step 1123, loss 0.470038, acc 0.765625\n",
      "2018-07-12T18:19:49.449827: step 1124, loss 0.443029, acc 0.828125\n",
      "2018-07-12T18:19:51.095756: step 1125, loss 0.425651, acc 0.796875\n",
      "2018-07-12T18:19:52.673749: step 1126, loss 0.436708, acc 0.78125\n",
      "2018-07-12T18:19:54.289932: step 1127, loss 0.612612, acc 0.6875\n",
      "2018-07-12T18:19:55.990658: step 1128, loss 0.404634, acc 0.8125\n",
      "2018-07-12T18:19:57.618860: step 1129, loss 0.460797, acc 0.796875\n",
      "2018-07-12T18:19:59.313742: step 1130, loss 0.516143, acc 0.765625\n",
      "2018-07-12T18:20:00.949965: step 1131, loss 0.368043, acc 0.828125\n",
      "2018-07-12T18:20:02.579539: step 1132, loss 0.394726, acc 0.828125\n",
      "2018-07-12T18:20:04.194557: step 1133, loss 0.558314, acc 0.671875\n",
      "2018-07-12T18:20:05.834559: step 1134, loss 0.504232, acc 0.71875\n",
      "2018-07-12T18:20:07.539233: step 1135, loss 0.610917, acc 0.65625\n",
      "2018-07-12T18:20:09.159138: step 1136, loss 0.343931, acc 0.859375\n",
      "2018-07-12T18:20:10.775989: step 1137, loss 0.531063, acc 0.75\n",
      "2018-07-12T18:20:12.403259: step 1138, loss 0.54042, acc 0.859375\n",
      "2018-07-12T18:20:14.080430: step 1139, loss 0.615235, acc 0.6875\n",
      "2018-07-12T18:20:15.796787: step 1140, loss 0.482567, acc 0.8125\n",
      "2018-07-12T18:20:17.385912: step 1141, loss 0.532294, acc 0.75\n",
      "2018-07-12T18:20:19.011840: step 1142, loss 0.390091, acc 0.84375\n",
      "2018-07-12T18:20:20.634689: step 1143, loss 0.501501, acc 0.6875\n",
      "2018-07-12T18:20:22.344518: step 1144, loss 0.533215, acc 0.765625\n",
      "2018-07-12T18:20:23.989359: step 1145, loss 0.5086, acc 0.765625\n",
      "2018-07-12T18:20:25.573837: step 1146, loss 0.434154, acc 0.796875\n",
      "2018-07-12T18:20:27.175563: step 1147, loss 0.52561, acc 0.734375\n",
      "2018-07-12T18:20:28.853979: step 1148, loss 0.523931, acc 0.796875\n",
      "2018-07-12T18:20:30.459864: step 1149, loss 0.589373, acc 0.71875\n",
      "2018-07-12T18:20:32.046459: step 1150, loss 0.432842, acc 0.8125\n",
      "2018-07-12T18:20:33.766086: step 1151, loss 0.456636, acc 0.75\n",
      "2018-07-12T18:20:35.414572: step 1152, loss 0.449625, acc 0.765625\n",
      "2018-07-12T18:20:37.070982: step 1153, loss 0.545317, acc 0.78125\n",
      "2018-07-12T18:20:38.676161: step 1154, loss 0.446149, acc 0.71875\n",
      "2018-07-12T18:20:40.302481: step 1155, loss 0.440076, acc 0.796875\n",
      "2018-07-12T18:20:41.942849: step 1156, loss 0.512361, acc 0.71875\n",
      "2018-07-12T18:20:43.592042: step 1157, loss 0.525508, acc 0.78125\n",
      "2018-07-12T18:20:45.243071: step 1158, loss 0.615898, acc 0.75\n",
      "2018-07-12T18:20:46.876094: step 1159, loss 0.471371, acc 0.8125\n",
      "2018-07-12T18:20:48.584426: step 1160, loss 0.508269, acc 0.734375\n",
      "2018-07-12T18:20:50.267371: step 1161, loss 0.594711, acc 0.734375\n",
      "2018-07-12T18:20:51.951181: step 1162, loss 0.515214, acc 0.734375\n",
      "2018-07-12T18:20:53.566344: step 1163, loss 0.355995, acc 0.828125\n",
      "2018-07-12T18:20:55.203375: step 1164, loss 0.411269, acc 0.859375\n",
      "2018-07-12T18:20:56.818716: step 1165, loss 0.486666, acc 0.765625\n",
      "2018-07-12T18:20:58.434493: step 1166, loss 0.418017, acc 0.8125\n",
      "2018-07-12T18:21:00.082078: step 1167, loss 0.524457, acc 0.796875\n",
      "2018-07-12T18:21:01.710996: step 1168, loss 0.561672, acc 0.734375\n",
      "2018-07-12T18:21:03.342309: step 1169, loss 0.512486, acc 0.75\n",
      "2018-07-12T18:21:04.975398: step 1170, loss 0.524353, acc 0.78125\n",
      "2018-07-12T18:21:06.632826: step 1171, loss 0.416403, acc 0.75\n",
      "2018-07-12T18:21:08.255734: step 1172, loss 0.502658, acc 0.734375\n",
      "2018-07-12T18:21:09.883290: step 1173, loss 0.622261, acc 0.671875\n",
      "2018-07-12T18:21:11.481715: step 1174, loss 0.574373, acc 0.734375\n",
      "2018-07-12T18:21:13.107643: step 1175, loss 0.552259, acc 0.765625\n",
      "2018-07-12T18:21:14.749305: step 1176, loss 0.409168, acc 0.828125\n",
      "2018-07-12T18:21:16.435489: step 1177, loss 0.485556, acc 0.671875\n",
      "2018-07-12T18:21:18.080570: step 1178, loss 0.555316, acc 0.78125\n",
      "2018-07-12T18:21:19.679838: step 1179, loss 0.610026, acc 0.703125\n",
      "2018-07-12T18:21:21.271488: step 1180, loss 0.436015, acc 0.734375\n",
      "2018-07-12T18:21:22.905998: step 1181, loss 0.420444, acc 0.84375\n",
      "2018-07-12T18:21:24.626495: step 1182, loss 0.476919, acc 0.734375\n",
      "2018-07-12T18:21:26.254366: step 1183, loss 0.49669, acc 0.765625\n",
      "2018-07-12T18:21:27.849692: step 1184, loss 0.730448, acc 0.6875\n",
      "2018-07-12T18:21:29.533543: step 1185, loss 0.607739, acc 0.71875\n",
      "2018-07-12T18:21:31.199587: step 1186, loss 0.410479, acc 0.8125\n",
      "2018-07-12T18:21:32.820012: step 1187, loss 0.398919, acc 0.84375\n",
      "2018-07-12T18:21:34.425379: step 1188, loss 0.368752, acc 0.828125\n",
      "2018-07-12T18:21:36.099526: step 1189, loss 0.591245, acc 0.703125\n",
      "2018-07-12T18:21:37.720999: step 1190, loss 0.689706, acc 0.703125\n",
      "2018-07-12T18:21:39.340299: step 1191, loss 0.566804, acc 0.734375\n",
      "2018-07-12T18:21:40.954262: step 1192, loss 0.447282, acc 0.796875\n",
      "2018-07-12T18:21:42.568324: step 1193, loss 0.563788, acc 0.6875\n",
      "2018-07-12T18:21:44.249158: step 1194, loss 0.650154, acc 0.6875\n",
      "2018-07-12T18:21:45.913935: step 1195, loss 0.521603, acc 0.78125\n",
      "2018-07-12T18:21:47.512818: step 1196, loss 0.343086, acc 0.875\n",
      "2018-07-12T18:21:49.211651: step 1197, loss 0.503386, acc 0.765625\n",
      "2018-07-12T18:21:50.848910: step 1198, loss 0.540746, acc 0.734375\n",
      "2018-07-12T18:21:52.501014: step 1199, loss 0.613845, acc 0.609375\n",
      "2018-07-12T18:21:54.167930: step 1200, loss 0.504972, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:22:12.061878: step 1200, loss 0.414973, acc 0.8276\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1200\n",
      "\n",
      "2018-07-12T18:22:14.632574: step 1201, loss 0.579669, acc 0.71875\n",
      "2018-07-12T18:22:16.245171: step 1202, loss 0.554714, acc 0.78125\n",
      "2018-07-12T18:22:17.871141: step 1203, loss 0.590407, acc 0.671875\n",
      "2018-07-12T18:22:19.477876: step 1204, loss 0.323968, acc 0.875\n",
      "2018-07-12T18:22:21.196361: step 1205, loss 0.53404, acc 0.734375\n",
      "2018-07-12T18:22:22.875408: step 1206, loss 0.443404, acc 0.765625\n",
      "2018-07-12T18:22:24.556718: step 1207, loss 0.355041, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:22:26.205138: step 1208, loss 0.622155, acc 0.671875\n",
      "2018-07-12T18:22:27.825126: step 1209, loss 0.445714, acc 0.8125\n",
      "2018-07-12T18:22:29.460048: step 1210, loss 0.479189, acc 0.734375\n",
      "2018-07-12T18:22:31.124931: step 1211, loss 0.533359, acc 0.75\n",
      "2018-07-12T18:22:32.722017: step 1212, loss 0.36245, acc 0.828125\n",
      "2018-07-12T18:22:34.402739: step 1213, loss 0.549659, acc 0.75\n",
      "2018-07-12T18:22:36.049879: step 1214, loss 0.520149, acc 0.75\n",
      "2018-07-12T18:22:37.659106: step 1215, loss 0.539643, acc 0.75\n",
      "2018-07-12T18:22:39.223395: step 1216, loss 0.442484, acc 0.796875\n",
      "2018-07-12T18:22:40.826591: step 1217, loss 0.339429, acc 0.859375\n",
      "2018-07-12T18:22:42.419469: step 1218, loss 0.57518, acc 0.703125\n",
      "2018-07-12T18:22:44.090479: step 1219, loss 0.707368, acc 0.609375\n",
      "2018-07-12T18:22:45.707069: step 1220, loss 0.547009, acc 0.703125\n",
      "2018-07-12T18:22:47.290126: step 1221, loss 0.504404, acc 0.765625\n",
      "2018-07-12T18:22:48.904713: step 1222, loss 0.414142, acc 0.84375\n",
      "2018-07-12T18:22:50.523681: step 1223, loss 0.617988, acc 0.703125\n",
      "2018-07-12T18:22:52.190786: step 1224, loss 0.520585, acc 0.734375\n",
      "2018-07-12T18:22:53.826780: step 1225, loss 0.391583, acc 0.796875\n",
      "2018-07-12T18:22:55.389379: step 1226, loss 0.382025, acc 0.84375\n",
      "2018-07-12T18:22:57.069923: step 1227, loss 0.61581, acc 0.71875\n",
      "2018-07-12T18:22:58.829425: step 1228, loss 0.460939, acc 0.78125\n",
      "2018-07-12T18:23:00.421758: step 1229, loss 0.491611, acc 0.71875\n",
      "2018-07-12T18:23:02.081251: step 1230, loss 0.438413, acc 0.78125\n",
      "2018-07-12T18:23:03.699840: step 1231, loss 0.494997, acc 0.734375\n",
      "2018-07-12T18:23:05.299771: step 1232, loss 0.493814, acc 0.796875\n",
      "2018-07-12T18:23:06.989813: step 1233, loss 0.411924, acc 0.8125\n",
      "2018-07-12T18:23:08.686464: step 1234, loss 0.48392, acc 0.765625\n",
      "2018-07-12T18:23:10.318838: step 1235, loss 0.574439, acc 0.75\n",
      "2018-07-12T18:23:11.915863: step 1236, loss 0.592181, acc 0.734375\n",
      "2018-07-12T18:23:13.522615: step 1237, loss 0.538693, acc 0.734375\n",
      "2018-07-12T18:23:15.169814: step 1238, loss 0.376282, acc 0.859375\n",
      "2018-07-12T18:23:16.810370: step 1239, loss 0.490243, acc 0.71875\n",
      "2018-07-12T18:23:18.424669: step 1240, loss 0.410792, acc 0.796875\n",
      "2018-07-12T18:23:20.067971: step 1241, loss 0.647286, acc 0.703125\n",
      "2018-07-12T18:23:21.663944: step 1242, loss 0.576903, acc 0.75\n",
      "2018-07-12T18:23:23.321047: step 1243, loss 0.412914, acc 0.75\n",
      "2018-07-12T18:23:24.924727: step 1244, loss 0.372266, acc 0.796875\n",
      "2018-07-12T18:23:26.580157: step 1245, loss 0.484292, acc 0.671875\n",
      "2018-07-12T18:23:28.234885: step 1246, loss 0.56772, acc 0.6875\n",
      "2018-07-12T18:23:29.869393: step 1247, loss 0.536766, acc 0.75\n",
      "2018-07-12T18:23:31.455596: step 1248, loss 0.396994, acc 0.8125\n",
      "2018-07-12T18:23:33.101362: step 1249, loss 0.550747, acc 0.703125\n",
      "2018-07-12T18:23:34.719405: step 1250, loss 0.423112, acc 0.796875\n",
      "2018-07-12T18:23:36.418483: step 1251, loss 0.521914, acc 0.703125\n",
      "2018-07-12T18:23:38.054153: step 1252, loss 0.608193, acc 0.765625\n",
      "2018-07-12T18:23:39.714777: step 1253, loss 0.593476, acc 0.734375\n",
      "2018-07-12T18:23:41.333306: step 1254, loss 0.580199, acc 0.703125\n",
      "2018-07-12T18:23:43.027753: step 1255, loss 0.491955, acc 0.75\n",
      "2018-07-12T18:23:44.699933: step 1256, loss 0.433149, acc 0.796875\n",
      "2018-07-12T18:23:46.304096: step 1257, loss 0.479296, acc 0.734375\n",
      "2018-07-12T18:23:47.926893: step 1258, loss 0.467217, acc 0.78125\n",
      "2018-07-12T18:23:49.508121: step 1259, loss 0.550414, acc 0.71875\n",
      "2018-07-12T18:23:51.122031: step 1260, loss 0.692323, acc 0.65625\n",
      "2018-07-12T18:23:52.768242: step 1261, loss 0.718781, acc 0.734375\n",
      "2018-07-12T18:23:54.388724: step 1262, loss 0.616219, acc 0.65625\n",
      "2018-07-12T18:23:56.061138: step 1263, loss 0.43017, acc 0.796875\n",
      "2018-07-12T18:23:57.669127: step 1264, loss 0.337632, acc 0.90625\n",
      "2018-07-12T18:23:59.343868: step 1265, loss 0.525085, acc 0.703125\n",
      "2018-07-12T18:24:01.030929: step 1266, loss 0.458711, acc 0.78125\n",
      "2018-07-12T18:24:02.718582: step 1267, loss 0.534691, acc 0.765625\n",
      "2018-07-12T18:24:04.443208: step 1268, loss 0.460432, acc 0.765625\n",
      "2018-07-12T18:24:06.121805: step 1269, loss 0.427605, acc 0.796875\n",
      "2018-07-12T18:24:07.728486: step 1270, loss 0.631523, acc 0.671875\n",
      "2018-07-12T18:24:09.357994: step 1271, loss 0.550173, acc 0.734375\n",
      "2018-07-12T18:24:11.082284: step 1272, loss 0.564628, acc 0.765625\n",
      "2018-07-12T18:24:12.745498: step 1273, loss 0.406632, acc 0.84375\n",
      "2018-07-12T18:24:14.418849: step 1274, loss 0.3932, acc 0.84375\n",
      "2018-07-12T18:24:16.064627: step 1275, loss 0.457084, acc 0.765625\n",
      "2018-07-12T18:24:17.711635: step 1276, loss 0.673543, acc 0.65625\n",
      "2018-07-12T18:24:19.320329: step 1277, loss 0.445352, acc 0.8125\n",
      "2018-07-12T18:24:20.966131: step 1278, loss 0.432935, acc 0.765625\n",
      "2018-07-12T18:24:22.611516: step 1279, loss 0.43883, acc 0.84375\n",
      "2018-07-12T18:24:24.224228: step 1280, loss 0.382175, acc 0.796875\n",
      "2018-07-12T18:24:25.846196: step 1281, loss 0.526814, acc 0.71875\n",
      "2018-07-12T18:24:27.420295: step 1282, loss 0.456562, acc 0.765625\n",
      "2018-07-12T18:24:29.029313: step 1283, loss 0.54653, acc 0.734375\n",
      "2018-07-12T18:24:30.616942: step 1284, loss 0.367568, acc 0.875\n",
      "2018-07-12T18:24:32.243775: step 1285, loss 0.542547, acc 0.765625\n",
      "2018-07-12T18:24:33.877530: step 1286, loss 0.32046, acc 0.84375\n",
      "2018-07-12T18:24:35.476355: step 1287, loss 0.412465, acc 0.84375\n",
      "2018-07-12T18:24:37.071587: step 1288, loss 0.414984, acc 0.78125\n",
      "2018-07-12T18:24:38.694803: step 1289, loss 0.422044, acc 0.78125\n",
      "2018-07-12T18:24:40.302540: step 1290, loss 0.389683, acc 0.796875\n",
      "2018-07-12T18:24:41.917207: step 1291, loss 0.577279, acc 0.65625\n",
      "2018-07-12T18:24:43.552173: step 1292, loss 0.479286, acc 0.78125\n",
      "2018-07-12T18:24:45.230432: step 1293, loss 0.393613, acc 0.859375\n",
      "2018-07-12T18:24:46.893427: step 1294, loss 0.493994, acc 0.765625\n",
      "2018-07-12T18:24:48.534437: step 1295, loss 0.43899, acc 0.828125\n",
      "2018-07-12T18:24:50.125787: step 1296, loss 0.481349, acc 0.78125\n",
      "2018-07-12T18:24:51.857866: step 1297, loss 0.470563, acc 0.796875\n",
      "2018-07-12T18:24:53.443350: step 1298, loss 0.539777, acc 0.734375\n",
      "2018-07-12T18:24:55.033828: step 1299, loss 0.42983, acc 0.828125\n",
      "2018-07-12T18:24:56.690086: step 1300, loss 0.390855, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:25:14.429569: step 1300, loss 0.398773, acc 0.8356\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1300\n",
      "\n",
      "2018-07-12T18:25:16.950724: step 1301, loss 0.352699, acc 0.84375\n",
      "2018-07-12T18:25:18.581985: step 1302, loss 0.558525, acc 0.703125\n",
      "2018-07-12T18:25:20.241631: step 1303, loss 0.407022, acc 0.8125\n",
      "2018-07-12T18:25:21.838619: step 1304, loss 0.356876, acc 0.890625\n",
      "2018-07-12T18:25:23.439085: step 1305, loss 0.527887, acc 0.6875\n",
      "2018-07-12T18:25:25.111199: step 1306, loss 0.407835, acc 0.796875\n",
      "2018-07-12T18:25:26.748641: step 1307, loss 0.630866, acc 0.625\n",
      "2018-07-12T18:25:28.333010: step 1308, loss 0.514009, acc 0.765625\n",
      "2018-07-12T18:25:30.032577: step 1309, loss 0.569176, acc 0.6875\n",
      "2018-07-12T18:25:31.659249: step 1310, loss 0.594355, acc 0.6875\n",
      "2018-07-12T18:25:33.340974: step 1311, loss 0.503683, acc 0.765625\n",
      "2018-07-12T18:25:35.026004: step 1312, loss 0.464667, acc 0.75\n",
      "2018-07-12T18:25:36.686604: step 1313, loss 0.539378, acc 0.71875\n",
      "2018-07-12T18:25:38.337130: step 1314, loss 0.505165, acc 0.796875\n",
      "2018-07-12T18:25:39.955386: step 1315, loss 0.379587, acc 0.796875\n",
      "2018-07-12T18:25:41.602609: step 1316, loss 0.515523, acc 0.71875\n",
      "2018-07-12T18:25:43.236770: step 1317, loss 0.577707, acc 0.71875\n",
      "2018-07-12T18:25:44.860529: step 1318, loss 0.290964, acc 0.84375\n",
      "2018-07-12T18:25:46.467564: step 1319, loss 0.326212, acc 0.8125\n",
      "2018-07-12T18:25:48.132621: step 1320, loss 0.54178, acc 0.765625\n",
      "2018-07-12T18:25:49.733870: step 1321, loss 0.502212, acc 0.71875\n",
      "2018-07-12T18:25:51.384641: step 1322, loss 0.348606, acc 0.84375\n",
      "2018-07-12T18:25:53.001470: step 1323, loss 0.437737, acc 0.734375\n",
      "2018-07-12T18:25:54.637106: step 1324, loss 0.330797, acc 0.859375\n",
      "2018-07-12T18:25:56.280597: step 1325, loss 0.424249, acc 0.796875\n",
      "2018-07-12T18:25:57.840312: step 1326, loss 0.523159, acc 0.734375\n",
      "2018-07-12T18:25:59.460137: step 1327, loss 0.492842, acc 0.765625\n",
      "2018-07-12T18:26:01.131820: step 1328, loss 0.519301, acc 0.734375\n",
      "2018-07-12T18:26:02.709101: step 1329, loss 0.678548, acc 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:26:04.285675: step 1330, loss 0.42492, acc 0.859375\n",
      "2018-07-12T18:26:05.941827: step 1331, loss 0.629464, acc 0.703125\n",
      "2018-07-12T18:26:07.615593: step 1332, loss 0.518426, acc 0.765625\n",
      "2018-07-12T18:26:09.217733: step 1333, loss 0.399012, acc 0.8125\n",
      "2018-07-12T18:26:10.859753: step 1334, loss 0.488786, acc 0.765625\n",
      "2018-07-12T18:26:12.464341: step 1335, loss 0.445497, acc 0.78125\n",
      "2018-07-12T18:26:14.083381: step 1336, loss 0.338593, acc 0.84375\n",
      "2018-07-12T18:26:15.699625: step 1337, loss 0.400505, acc 0.796875\n",
      "2018-07-12T18:26:17.332206: step 1338, loss 0.562227, acc 0.6875\n",
      "2018-07-12T18:26:18.883819: step 1339, loss 0.534376, acc 0.734375\n",
      "2018-07-12T18:26:20.552662: step 1340, loss 0.39258, acc 0.78125\n",
      "2018-07-12T18:26:22.181087: step 1341, loss 0.454973, acc 0.71875\n",
      "2018-07-12T18:26:23.802251: step 1342, loss 0.410181, acc 0.84375\n",
      "2018-07-12T18:26:25.411143: step 1343, loss 0.577956, acc 0.703125\n",
      "2018-07-12T18:26:27.070199: step 1344, loss 0.539161, acc 0.765625\n",
      "2018-07-12T18:26:28.714648: step 1345, loss 0.528341, acc 0.75\n",
      "2018-07-12T18:26:30.327574: step 1346, loss 0.298628, acc 0.890625\n",
      "2018-07-12T18:26:31.892683: step 1347, loss 0.377609, acc 0.84375\n",
      "2018-07-12T18:26:33.519271: step 1348, loss 0.435602, acc 0.796875\n",
      "2018-07-12T18:26:35.192405: step 1349, loss 0.419749, acc 0.78125\n",
      "2018-07-12T18:26:36.941623: step 1350, loss 0.499101, acc 0.828125\n",
      "2018-07-12T18:26:38.572463: step 1351, loss 0.461414, acc 0.78125\n",
      "2018-07-12T18:26:40.205856: step 1352, loss 0.372597, acc 0.84375\n",
      "2018-07-12T18:26:41.812273: step 1353, loss 0.497504, acc 0.734375\n",
      "2018-07-12T18:26:43.407281: step 1354, loss 0.404165, acc 0.8125\n",
      "2018-07-12T18:26:44.974690: step 1355, loss 0.423055, acc 0.796875\n",
      "2018-07-12T18:26:46.599561: step 1356, loss 0.359466, acc 0.875\n",
      "2018-07-12T18:26:48.170245: step 1357, loss 0.478363, acc 0.8125\n",
      "2018-07-12T18:26:49.868023: step 1358, loss 0.447993, acc 0.8125\n",
      "2018-07-12T18:26:51.518062: step 1359, loss 0.414671, acc 0.796875\n",
      "2018-07-12T18:26:53.165145: step 1360, loss 0.476122, acc 0.765625\n",
      "2018-07-12T18:26:54.761623: step 1361, loss 0.508759, acc 0.765625\n",
      "2018-07-12T18:26:56.421821: step 1362, loss 0.433572, acc 0.796875\n",
      "2018-07-12T18:26:57.996359: step 1363, loss 0.613015, acc 0.65625\n",
      "2018-07-12T18:26:59.617209: step 1364, loss 0.435993, acc 0.796875\n",
      "2018-07-12T18:27:01.274545: step 1365, loss 0.548064, acc 0.734375\n",
      "2018-07-12T18:27:02.967960: step 1366, loss 0.502938, acc 0.71875\n",
      "2018-07-12T18:27:04.614054: step 1367, loss 0.443189, acc 0.84375\n",
      "2018-07-12T18:27:06.202205: step 1368, loss 0.480614, acc 0.796875\n",
      "2018-07-12T18:27:07.770985: step 1369, loss 0.46428, acc 0.78125\n",
      "2018-07-12T18:27:09.434862: step 1370, loss 0.490609, acc 0.828125\n",
      "2018-07-12T18:27:11.068465: step 1371, loss 0.438018, acc 0.8125\n",
      "2018-07-12T18:27:12.706552: step 1372, loss 0.417467, acc 0.765625\n",
      "2018-07-12T18:27:14.296475: step 1373, loss 0.633677, acc 0.765625\n",
      "2018-07-12T18:27:15.942428: step 1374, loss 0.460886, acc 0.78125\n",
      "2018-07-12T18:27:17.622750: step 1375, loss 0.510038, acc 0.71875\n",
      "2018-07-12T18:27:19.239804: step 1376, loss 0.487954, acc 0.75\n",
      "2018-07-12T18:27:20.830323: step 1377, loss 0.431276, acc 0.796875\n",
      "2018-07-12T18:27:22.517832: step 1378, loss 0.378394, acc 0.84375\n",
      "2018-07-12T18:27:24.097737: step 1379, loss 0.557939, acc 0.703125\n",
      "2018-07-12T18:27:25.725795: step 1380, loss 0.502504, acc 0.78125\n",
      "2018-07-12T18:27:27.289967: step 1381, loss 0.409111, acc 0.84375\n",
      "2018-07-12T18:27:28.904106: step 1382, loss 0.400515, acc 0.828125\n",
      "2018-07-12T18:27:30.526588: step 1383, loss 0.51584, acc 0.765625\n",
      "2018-07-12T18:27:32.148738: step 1384, loss 0.381475, acc 0.875\n",
      "2018-07-12T18:27:33.743797: step 1385, loss 0.51511, acc 0.71875\n",
      "2018-07-12T18:27:35.329181: step 1386, loss 0.478963, acc 0.71875\n",
      "2018-07-12T18:27:37.014830: step 1387, loss 0.398543, acc 0.828125\n",
      "2018-07-12T18:27:38.645025: step 1388, loss 0.425871, acc 0.796875\n",
      "2018-07-12T18:27:40.249444: step 1389, loss 0.382032, acc 0.84375\n",
      "2018-07-12T18:27:41.859266: step 1390, loss 0.423746, acc 0.796875\n",
      "2018-07-12T18:27:43.471745: step 1391, loss 0.389782, acc 0.828125\n",
      "2018-07-12T18:27:45.154614: step 1392, loss 0.4664, acc 0.734375\n",
      "2018-07-12T18:27:46.712440: step 1393, loss 0.54495, acc 0.734375\n",
      "2018-07-12T18:27:48.352669: step 1394, loss 0.504851, acc 0.765625\n",
      "2018-07-12T18:27:49.948626: step 1395, loss 0.355237, acc 0.828125\n",
      "2018-07-12T18:27:51.539180: step 1396, loss 0.519162, acc 0.796875\n",
      "2018-07-12T18:27:53.218240: step 1397, loss 0.498742, acc 0.8125\n",
      "2018-07-12T18:27:54.863072: step 1398, loss 0.528402, acc 0.671875\n",
      "2018-07-12T18:27:56.529633: step 1399, loss 0.383444, acc 0.828125\n",
      "2018-07-12T18:27:58.276147: step 1400, loss 0.51879, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:28:16.293870: step 1400, loss 0.429628, acc 0.808\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1400\n",
      "\n",
      "2018-07-12T18:28:18.911006: step 1401, loss 0.638257, acc 0.71875\n",
      "2018-07-12T18:28:20.532125: step 1402, loss 0.396337, acc 0.828125\n",
      "2018-07-12T18:28:22.119402: step 1403, loss 0.477132, acc 0.75\n",
      "2018-07-12T18:28:23.715656: step 1404, loss 0.490562, acc 0.71875\n",
      "2018-07-12T18:28:25.287820: step 1405, loss 0.438283, acc 0.78125\n",
      "2018-07-12T18:28:26.858941: step 1406, loss 0.459933, acc 0.734375\n",
      "2018-07-12T18:28:28.413385: step 1407, loss 0.598952, acc 0.75\n",
      "2018-07-12T18:28:29.440291: step 1408, loss 0.434325, acc 0.805556\n",
      "2018-07-12T18:28:31.096768: step 1409, loss 0.491077, acc 0.734375\n",
      "2018-07-12T18:28:32.788538: step 1410, loss 0.296757, acc 0.859375\n",
      "2018-07-12T18:28:34.483428: step 1411, loss 0.399962, acc 0.84375\n",
      "2018-07-12T18:28:36.085394: step 1412, loss 0.73538, acc 0.75\n",
      "2018-07-12T18:28:37.710260: step 1413, loss 0.505076, acc 0.734375\n",
      "2018-07-12T18:28:39.331543: step 1414, loss 0.539891, acc 0.75\n",
      "2018-07-12T18:28:40.941119: step 1415, loss 0.365295, acc 0.796875\n",
      "2018-07-12T18:28:42.608598: step 1416, loss 0.565526, acc 0.71875\n",
      "2018-07-12T18:28:44.232430: step 1417, loss 0.331881, acc 0.859375\n",
      "2018-07-12T18:28:45.865869: step 1418, loss 0.317397, acc 0.84375\n",
      "2018-07-12T18:28:47.525940: step 1419, loss 0.582453, acc 0.765625\n",
      "2018-07-12T18:28:49.114990: step 1420, loss 0.39737, acc 0.78125\n",
      "2018-07-12T18:28:50.732165: step 1421, loss 0.424131, acc 0.765625\n",
      "2018-07-12T18:28:52.367131: step 1422, loss 0.373521, acc 0.890625\n",
      "2018-07-12T18:28:53.986726: step 1423, loss 0.378179, acc 0.828125\n",
      "2018-07-12T18:28:55.602469: step 1424, loss 0.319155, acc 0.859375\n",
      "2018-07-12T18:28:57.323905: step 1425, loss 0.435485, acc 0.75\n",
      "2018-07-12T18:28:58.943666: step 1426, loss 0.41041, acc 0.8125\n",
      "2018-07-12T18:29:00.510715: step 1427, loss 0.526831, acc 0.75\n",
      "2018-07-12T18:29:02.132425: step 1428, loss 0.382555, acc 0.859375\n",
      "2018-07-12T18:29:03.807182: step 1429, loss 0.476424, acc 0.734375\n",
      "2018-07-12T18:29:05.405587: step 1430, loss 0.445048, acc 0.84375\n",
      "2018-07-12T18:29:07.046505: step 1431, loss 0.348717, acc 0.8125\n",
      "2018-07-12T18:29:08.709447: step 1432, loss 0.392878, acc 0.75\n",
      "2018-07-12T18:29:10.327842: step 1433, loss 0.355321, acc 0.8125\n",
      "2018-07-12T18:29:11.972141: step 1434, loss 0.397387, acc 0.84375\n",
      "2018-07-12T18:29:13.547132: step 1435, loss 0.501274, acc 0.75\n",
      "2018-07-12T18:29:15.207598: step 1436, loss 0.412562, acc 0.796875\n",
      "2018-07-12T18:29:16.773005: step 1437, loss 0.336284, acc 0.875\n",
      "2018-07-12T18:29:18.390675: step 1438, loss 0.612934, acc 0.734375\n",
      "2018-07-12T18:29:20.004306: step 1439, loss 0.572003, acc 0.765625\n",
      "2018-07-12T18:29:21.631788: step 1440, loss 0.406579, acc 0.84375\n",
      "2018-07-12T18:29:23.226007: step 1441, loss 0.50832, acc 0.765625\n",
      "2018-07-12T18:29:24.859817: step 1442, loss 0.431807, acc 0.765625\n",
      "2018-07-12T18:29:26.457162: step 1443, loss 0.421114, acc 0.84375\n",
      "2018-07-12T18:29:28.152452: step 1444, loss 0.30494, acc 0.84375\n",
      "2018-07-12T18:29:29.777715: step 1445, loss 0.380229, acc 0.828125\n",
      "2018-07-12T18:29:31.444145: step 1446, loss 0.451363, acc 0.765625\n",
      "2018-07-12T18:29:33.062002: step 1447, loss 0.445897, acc 0.84375\n",
      "2018-07-12T18:29:34.688591: step 1448, loss 0.360449, acc 0.828125\n",
      "2018-07-12T18:29:36.319002: step 1449, loss 0.551967, acc 0.75\n",
      "2018-07-12T18:29:37.963705: step 1450, loss 0.442901, acc 0.8125\n",
      "2018-07-12T18:29:39.598591: step 1451, loss 0.460761, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:29:41.224720: step 1452, loss 0.385329, acc 0.828125\n",
      "2018-07-12T18:29:42.778603: step 1453, loss 0.587194, acc 0.6875\n",
      "2018-07-12T18:29:44.354691: step 1454, loss 0.410397, acc 0.828125\n",
      "2018-07-12T18:29:45.969442: step 1455, loss 0.505094, acc 0.734375\n",
      "2018-07-12T18:29:47.612070: step 1456, loss 0.538824, acc 0.75\n",
      "2018-07-12T18:29:49.203652: step 1457, loss 0.320194, acc 0.84375\n",
      "2018-07-12T18:29:50.926704: step 1458, loss 0.417605, acc 0.78125\n",
      "2018-07-12T18:29:52.651282: step 1459, loss 0.459074, acc 0.75\n",
      "2018-07-12T18:29:54.289277: step 1460, loss 0.591051, acc 0.65625\n",
      "2018-07-12T18:29:55.868237: step 1461, loss 0.388907, acc 0.8125\n",
      "2018-07-12T18:29:57.463733: step 1462, loss 0.356642, acc 0.859375\n",
      "2018-07-12T18:29:59.137195: step 1463, loss 0.445167, acc 0.75\n",
      "2018-07-12T18:30:00.774254: step 1464, loss 0.424342, acc 0.84375\n",
      "2018-07-12T18:30:02.403150: step 1465, loss 0.492135, acc 0.8125\n",
      "2018-07-12T18:30:04.027579: step 1466, loss 0.461896, acc 0.78125\n",
      "2018-07-12T18:30:05.647685: step 1467, loss 0.487744, acc 0.734375\n",
      "2018-07-12T18:30:07.239216: step 1468, loss 0.381968, acc 0.875\n",
      "2018-07-12T18:30:08.848405: step 1469, loss 0.599243, acc 0.703125\n",
      "2018-07-12T18:30:10.501226: step 1470, loss 0.528985, acc 0.765625\n",
      "2018-07-12T18:30:12.146393: step 1471, loss 0.268908, acc 0.921875\n",
      "2018-07-12T18:30:13.840519: step 1472, loss 0.426552, acc 0.765625\n",
      "2018-07-12T18:30:15.441660: step 1473, loss 0.459523, acc 0.765625\n",
      "2018-07-12T18:30:17.076021: step 1474, loss 0.477293, acc 0.796875\n",
      "2018-07-12T18:30:18.709831: step 1475, loss 0.5645, acc 0.75\n",
      "2018-07-12T18:30:20.305101: step 1476, loss 0.40819, acc 0.84375\n",
      "2018-07-12T18:30:21.908411: step 1477, loss 0.338336, acc 0.859375\n",
      "2018-07-12T18:30:23.547657: step 1478, loss 0.390527, acc 0.828125\n",
      "2018-07-12T18:30:25.161200: step 1479, loss 0.436483, acc 0.78125\n",
      "2018-07-12T18:30:26.858764: step 1480, loss 0.329804, acc 0.875\n",
      "2018-07-12T18:30:28.507564: step 1481, loss 0.448283, acc 0.796875\n",
      "2018-07-12T18:30:30.165198: step 1482, loss 0.317945, acc 0.828125\n",
      "2018-07-12T18:30:31.826809: step 1483, loss 0.404191, acc 0.8125\n",
      "2018-07-12T18:30:33.459182: step 1484, loss 0.497929, acc 0.8125\n",
      "2018-07-12T18:30:35.071439: step 1485, loss 0.450172, acc 0.828125\n",
      "2018-07-12T18:30:36.751426: step 1486, loss 0.332392, acc 0.875\n",
      "2018-07-12T18:30:38.427307: step 1487, loss 0.604388, acc 0.734375\n",
      "2018-07-12T18:30:40.119118: step 1488, loss 0.548735, acc 0.734375\n",
      "2018-07-12T18:30:41.748067: step 1489, loss 0.441104, acc 0.828125\n",
      "2018-07-12T18:30:43.364480: step 1490, loss 0.385756, acc 0.8125\n",
      "2018-07-12T18:30:44.950544: step 1491, loss 0.42954, acc 0.765625\n",
      "2018-07-12T18:30:46.560133: step 1492, loss 0.46043, acc 0.828125\n",
      "2018-07-12T18:30:48.188711: step 1493, loss 0.409758, acc 0.875\n",
      "2018-07-12T18:30:49.796509: step 1494, loss 0.508779, acc 0.75\n",
      "2018-07-12T18:30:51.417614: step 1495, loss 0.58244, acc 0.65625\n",
      "2018-07-12T18:30:53.094772: step 1496, loss 0.345379, acc 0.796875\n",
      "2018-07-12T18:30:54.789183: step 1497, loss 0.57139, acc 0.71875\n",
      "2018-07-12T18:30:56.450056: step 1498, loss 0.369196, acc 0.78125\n",
      "2018-07-12T18:30:58.122770: step 1499, loss 0.375359, acc 0.875\n",
      "2018-07-12T18:30:59.770475: step 1500, loss 0.409996, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:31:17.698413: step 1500, loss 0.373941, acc 0.836\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1500\n",
      "\n",
      "2018-07-12T18:31:20.338496: step 1501, loss 0.471595, acc 0.78125\n",
      "2018-07-12T18:31:21.985040: step 1502, loss 0.414575, acc 0.78125\n",
      "2018-07-12T18:31:23.595952: step 1503, loss 0.403748, acc 0.8125\n",
      "2018-07-12T18:31:25.206464: step 1504, loss 0.520781, acc 0.734375\n",
      "2018-07-12T18:31:26.847062: step 1505, loss 0.399044, acc 0.8125\n",
      "2018-07-12T18:31:28.488809: step 1506, loss 0.453024, acc 0.78125\n",
      "2018-07-12T18:31:30.107724: step 1507, loss 0.372988, acc 0.828125\n",
      "2018-07-12T18:31:31.788258: step 1508, loss 0.382563, acc 0.828125\n",
      "2018-07-12T18:31:33.468571: step 1509, loss 0.441662, acc 0.796875\n",
      "2018-07-12T18:31:35.149304: step 1510, loss 0.380731, acc 0.859375\n",
      "2018-07-12T18:31:36.756551: step 1511, loss 0.472844, acc 0.78125\n",
      "2018-07-12T18:31:38.407080: step 1512, loss 0.368099, acc 0.890625\n",
      "2018-07-12T18:31:40.133492: step 1513, loss 0.374132, acc 0.84375\n",
      "2018-07-12T18:31:41.785921: step 1514, loss 0.600068, acc 0.734375\n",
      "2018-07-12T18:31:43.397910: step 1515, loss 0.569723, acc 0.734375\n",
      "2018-07-12T18:31:45.005023: step 1516, loss 0.413363, acc 0.796875\n",
      "2018-07-12T18:31:46.592654: step 1517, loss 0.436021, acc 0.828125\n",
      "2018-07-12T18:31:48.259383: step 1518, loss 0.459129, acc 0.828125\n",
      "2018-07-12T18:31:49.866924: step 1519, loss 0.401131, acc 0.8125\n",
      "2018-07-12T18:31:51.512189: step 1520, loss 0.65863, acc 0.734375\n",
      "2018-07-12T18:31:53.183554: step 1521, loss 0.427118, acc 0.796875\n",
      "2018-07-12T18:31:54.826989: step 1522, loss 0.520727, acc 0.765625\n",
      "2018-07-12T18:31:56.413958: step 1523, loss 0.347386, acc 0.90625\n",
      "2018-07-12T18:31:58.076748: step 1524, loss 0.464724, acc 0.8125\n",
      "2018-07-12T18:31:59.730329: step 1525, loss 0.436949, acc 0.796875\n",
      "2018-07-12T18:32:01.398083: step 1526, loss 0.461406, acc 0.8125\n",
      "2018-07-12T18:32:02.984689: step 1527, loss 0.511557, acc 0.78125\n",
      "2018-07-12T18:32:04.609293: step 1528, loss 0.356485, acc 0.8125\n",
      "2018-07-12T18:32:06.207693: step 1529, loss 0.462572, acc 0.71875\n",
      "2018-07-12T18:32:07.828658: step 1530, loss 0.439631, acc 0.796875\n",
      "2018-07-12T18:32:09.432745: step 1531, loss 0.350484, acc 0.859375\n",
      "2018-07-12T18:32:11.165637: step 1532, loss 0.467373, acc 0.75\n",
      "2018-07-12T18:32:12.782830: step 1533, loss 0.384223, acc 0.8125\n",
      "2018-07-12T18:32:14.395307: step 1534, loss 0.349949, acc 0.828125\n",
      "2018-07-12T18:32:16.080673: step 1535, loss 0.434651, acc 0.796875\n",
      "2018-07-12T18:32:17.702269: step 1536, loss 0.377381, acc 0.828125\n",
      "2018-07-12T18:32:19.307734: step 1537, loss 0.494915, acc 0.765625\n",
      "2018-07-12T18:32:20.939186: step 1538, loss 0.53552, acc 0.71875\n",
      "2018-07-12T18:32:22.506121: step 1539, loss 0.40036, acc 0.828125\n",
      "2018-07-12T18:32:24.127845: step 1540, loss 0.502367, acc 0.703125\n",
      "2018-07-12T18:32:25.772081: step 1541, loss 0.515427, acc 0.78125\n",
      "2018-07-12T18:32:27.421096: step 1542, loss 0.419274, acc 0.796875\n",
      "2018-07-12T18:32:29.045994: step 1543, loss 0.429814, acc 0.84375\n",
      "2018-07-12T18:32:30.692499: step 1544, loss 0.554907, acc 0.703125\n",
      "2018-07-12T18:32:32.281278: step 1545, loss 0.369328, acc 0.828125\n",
      "2018-07-12T18:32:33.922063: step 1546, loss 0.534059, acc 0.78125\n",
      "2018-07-12T18:32:35.558286: step 1547, loss 0.510204, acc 0.75\n",
      "2018-07-12T18:32:37.234095: step 1548, loss 0.375694, acc 0.828125\n",
      "2018-07-12T18:32:38.935625: step 1549, loss 0.415301, acc 0.75\n",
      "2018-07-12T18:32:40.550731: step 1550, loss 0.47064, acc 0.78125\n",
      "2018-07-12T18:32:42.186982: step 1551, loss 0.501262, acc 0.78125\n",
      "2018-07-12T18:32:43.773153: step 1552, loss 0.424487, acc 0.78125\n",
      "2018-07-12T18:32:45.456412: step 1553, loss 0.357401, acc 0.828125\n",
      "2018-07-12T18:32:47.085760: step 1554, loss 0.437323, acc 0.8125\n",
      "2018-07-12T18:32:48.705528: step 1555, loss 0.393666, acc 0.828125\n",
      "2018-07-12T18:32:50.325067: step 1556, loss 0.371444, acc 0.796875\n",
      "2018-07-12T18:32:51.990370: step 1557, loss 0.413547, acc 0.828125\n",
      "2018-07-12T18:32:53.675926: step 1558, loss 0.48846, acc 0.765625\n",
      "2018-07-12T18:32:55.301327: step 1559, loss 0.26426, acc 0.875\n",
      "2018-07-12T18:32:56.881623: step 1560, loss 0.56049, acc 0.78125\n",
      "2018-07-12T18:32:58.495734: step 1561, loss 0.424996, acc 0.828125\n",
      "2018-07-12T18:33:00.152041: step 1562, loss 0.568921, acc 0.734375\n",
      "2018-07-12T18:33:01.772115: step 1563, loss 0.395234, acc 0.796875\n",
      "2018-07-12T18:33:03.415077: step 1564, loss 0.40106, acc 0.84375\n",
      "2018-07-12T18:33:05.025318: step 1565, loss 0.340805, acc 0.828125\n",
      "2018-07-12T18:33:06.670273: step 1566, loss 0.498169, acc 0.796875\n",
      "2018-07-12T18:33:08.316503: step 1567, loss 0.329407, acc 0.84375\n",
      "2018-07-12T18:33:09.886973: step 1568, loss 0.592441, acc 0.703125\n",
      "2018-07-12T18:33:11.550687: step 1569, loss 0.60671, acc 0.6875\n",
      "2018-07-12T18:33:13.130070: step 1570, loss 0.492009, acc 0.796875\n",
      "2018-07-12T18:33:14.765181: step 1571, loss 0.432044, acc 0.828125\n",
      "2018-07-12T18:33:16.359850: step 1572, loss 0.497245, acc 0.796875\n",
      "2018-07-12T18:33:17.971464: step 1573, loss 0.362499, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:33:19.596827: step 1574, loss 0.49607, acc 0.8125\n",
      "2018-07-12T18:33:21.218586: step 1575, loss 0.43203, acc 0.75\n",
      "2018-07-12T18:33:22.812792: step 1576, loss 0.396904, acc 0.8125\n",
      "2018-07-12T18:33:24.436695: step 1577, loss 0.361935, acc 0.84375\n",
      "2018-07-12T18:33:26.027912: step 1578, loss 0.354108, acc 0.828125\n",
      "2018-07-12T18:33:27.643276: step 1579, loss 0.593588, acc 0.765625\n",
      "2018-07-12T18:33:29.226426: step 1580, loss 0.398922, acc 0.890625\n",
      "2018-07-12T18:33:30.872307: step 1581, loss 0.476974, acc 0.75\n",
      "2018-07-12T18:33:32.516456: step 1582, loss 0.444244, acc 0.8125\n",
      "2018-07-12T18:33:34.181547: step 1583, loss 0.394429, acc 0.875\n",
      "2018-07-12T18:33:35.797363: step 1584, loss 0.349036, acc 0.859375\n",
      "2018-07-12T18:33:37.504065: step 1585, loss 0.342095, acc 0.796875\n",
      "2018-07-12T18:33:39.095756: step 1586, loss 0.452336, acc 0.765625\n",
      "2018-07-12T18:33:40.723345: step 1587, loss 0.586222, acc 0.703125\n",
      "2018-07-12T18:33:42.374536: step 1588, loss 0.360943, acc 0.828125\n",
      "2018-07-12T18:33:43.994846: step 1589, loss 0.437349, acc 0.78125\n",
      "2018-07-12T18:33:45.624604: step 1590, loss 0.383937, acc 0.828125\n",
      "2018-07-12T18:33:47.342395: step 1591, loss 0.387527, acc 0.796875\n",
      "2018-07-12T18:33:48.975785: step 1592, loss 0.413165, acc 0.765625\n",
      "2018-07-12T18:33:50.623155: step 1593, loss 0.479401, acc 0.75\n",
      "2018-07-12T18:33:52.276237: step 1594, loss 0.347663, acc 0.84375\n",
      "2018-07-12T18:33:53.942012: step 1595, loss 0.447724, acc 0.828125\n",
      "2018-07-12T18:33:55.586082: step 1596, loss 0.577963, acc 0.78125\n",
      "2018-07-12T18:33:57.219901: step 1597, loss 0.327155, acc 0.890625\n",
      "2018-07-12T18:33:58.875386: step 1598, loss 0.400075, acc 0.8125\n",
      "2018-07-12T18:34:00.482700: step 1599, loss 0.391558, acc 0.796875\n",
      "2018-07-12T18:34:02.062712: step 1600, loss 0.282644, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:34:19.984520: step 1600, loss 0.367008, acc 0.8412\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1600\n",
      "\n",
      "2018-07-12T18:34:22.467235: step 1601, loss 0.432462, acc 0.828125\n",
      "2018-07-12T18:34:24.076654: step 1602, loss 0.398488, acc 0.796875\n",
      "2018-07-12T18:34:25.667176: step 1603, loss 0.492984, acc 0.765625\n",
      "2018-07-12T18:34:27.288742: step 1604, loss 0.554209, acc 0.765625\n",
      "2018-07-12T18:34:28.957956: step 1605, loss 0.402861, acc 0.796875\n",
      "2018-07-12T18:34:30.567768: step 1606, loss 0.542003, acc 0.796875\n",
      "2018-07-12T18:34:32.276688: step 1607, loss 0.445377, acc 0.78125\n",
      "2018-07-12T18:34:33.909284: step 1608, loss 0.427876, acc 0.78125\n",
      "2018-07-12T18:34:35.530127: step 1609, loss 0.276618, acc 0.875\n",
      "2018-07-12T18:34:37.118626: step 1610, loss 0.354753, acc 0.828125\n",
      "2018-07-12T18:34:38.757022: step 1611, loss 0.602662, acc 0.734375\n",
      "2018-07-12T18:34:40.369424: step 1612, loss 0.299172, acc 0.859375\n",
      "2018-07-12T18:34:42.014124: step 1613, loss 0.548406, acc 0.71875\n",
      "2018-07-12T18:34:43.678212: step 1614, loss 0.38081, acc 0.796875\n",
      "2018-07-12T18:34:45.306420: step 1615, loss 0.449652, acc 0.84375\n",
      "2018-07-12T18:34:46.916250: step 1616, loss 0.436152, acc 0.734375\n",
      "2018-07-12T18:34:48.601044: step 1617, loss 0.407697, acc 0.84375\n",
      "2018-07-12T18:34:50.233440: step 1618, loss 0.426219, acc 0.859375\n",
      "2018-07-12T18:34:51.885763: step 1619, loss 0.280803, acc 0.890625\n",
      "2018-07-12T18:34:53.516120: step 1620, loss 0.405236, acc 0.78125\n",
      "2018-07-12T18:34:55.189611: step 1621, loss 0.390403, acc 0.796875\n",
      "2018-07-12T18:34:56.873557: step 1622, loss 0.376759, acc 0.8125\n",
      "2018-07-12T18:34:58.552896: step 1623, loss 0.446718, acc 0.765625\n",
      "2018-07-12T18:35:00.173883: step 1624, loss 0.468054, acc 0.8125\n",
      "2018-07-12T18:35:01.820929: step 1625, loss 0.474005, acc 0.796875\n",
      "2018-07-12T18:35:03.432433: step 1626, loss 0.466955, acc 0.828125\n",
      "2018-07-12T18:35:05.030237: step 1627, loss 0.351417, acc 0.859375\n",
      "2018-07-12T18:35:06.619218: step 1628, loss 0.387455, acc 0.84375\n",
      "2018-07-12T18:35:08.233983: step 1629, loss 0.625829, acc 0.65625\n",
      "2018-07-12T18:35:09.810939: step 1630, loss 0.373191, acc 0.8125\n",
      "2018-07-12T18:35:11.460464: step 1631, loss 0.485115, acc 0.78125\n",
      "2018-07-12T18:35:13.116371: step 1632, loss 0.309792, acc 0.890625\n",
      "2018-07-12T18:35:14.769039: step 1633, loss 0.569589, acc 0.703125\n",
      "2018-07-12T18:35:16.437012: step 1634, loss 0.554248, acc 0.75\n",
      "2018-07-12T18:35:18.119461: step 1635, loss 0.429925, acc 0.78125\n",
      "2018-07-12T18:35:19.755623: step 1636, loss 0.604558, acc 0.765625\n",
      "2018-07-12T18:35:21.385787: step 1637, loss 0.541877, acc 0.765625\n",
      "2018-07-12T18:35:23.025950: step 1638, loss 0.412881, acc 0.78125\n",
      "2018-07-12T18:35:24.726053: step 1639, loss 0.407376, acc 0.8125\n",
      "2018-07-12T18:35:26.336333: step 1640, loss 0.512955, acc 0.75\n",
      "2018-07-12T18:35:27.938331: step 1641, loss 0.62906, acc 0.71875\n",
      "2018-07-12T18:35:29.535454: step 1642, loss 0.310535, acc 0.875\n",
      "2018-07-12T18:35:31.140416: step 1643, loss 0.441733, acc 0.765625\n",
      "2018-07-12T18:35:32.787533: step 1644, loss 0.344204, acc 0.8125\n",
      "2018-07-12T18:35:34.427762: step 1645, loss 0.431032, acc 0.796875\n",
      "2018-07-12T18:35:36.050546: step 1646, loss 0.434953, acc 0.828125\n",
      "2018-07-12T18:35:37.672574: step 1647, loss 0.582565, acc 0.765625\n",
      "2018-07-12T18:35:39.325392: step 1648, loss 0.37871, acc 0.828125\n",
      "2018-07-12T18:35:40.973174: step 1649, loss 0.549332, acc 0.75\n",
      "2018-07-12T18:35:42.565986: step 1650, loss 0.444325, acc 0.78125\n",
      "2018-07-12T18:35:44.147575: step 1651, loss 0.517089, acc 0.71875\n",
      "2018-07-12T18:35:45.762554: step 1652, loss 0.430789, acc 0.796875\n",
      "2018-07-12T18:35:47.363842: step 1653, loss 0.392188, acc 0.84375\n",
      "2018-07-12T18:35:48.965272: step 1654, loss 0.423587, acc 0.8125\n",
      "2018-07-12T18:35:50.619364: step 1655, loss 0.394359, acc 0.8125\n",
      "2018-07-12T18:35:52.247135: step 1656, loss 0.305573, acc 0.875\n",
      "2018-07-12T18:35:53.869115: step 1657, loss 0.604653, acc 0.703125\n",
      "2018-07-12T18:35:55.517799: step 1658, loss 0.502164, acc 0.8125\n",
      "2018-07-12T18:35:57.112677: step 1659, loss 0.28526, acc 0.890625\n",
      "2018-07-12T18:35:58.713785: step 1660, loss 0.478232, acc 0.765625\n",
      "2018-07-12T18:36:00.360224: step 1661, loss 0.406473, acc 0.8125\n",
      "2018-07-12T18:36:02.029315: step 1662, loss 0.46296, acc 0.734375\n",
      "2018-07-12T18:36:03.657055: step 1663, loss 0.354309, acc 0.890625\n",
      "2018-07-12T18:36:05.293129: step 1664, loss 0.337552, acc 0.890625\n",
      "2018-07-12T18:36:06.883991: step 1665, loss 0.369835, acc 0.78125\n",
      "2018-07-12T18:36:08.508724: step 1666, loss 0.398933, acc 0.8125\n",
      "2018-07-12T18:36:10.162130: step 1667, loss 0.434615, acc 0.796875\n",
      "2018-07-12T18:36:11.783400: step 1668, loss 0.407517, acc 0.8125\n",
      "2018-07-12T18:36:13.409354: step 1669, loss 0.254882, acc 0.90625\n",
      "2018-07-12T18:36:15.012558: step 1670, loss 0.357788, acc 0.8125\n",
      "2018-07-12T18:36:16.618539: step 1671, loss 0.415699, acc 0.828125\n",
      "2018-07-12T18:36:18.245273: step 1672, loss 0.410233, acc 0.796875\n",
      "2018-07-12T18:36:19.895853: step 1673, loss 0.245798, acc 0.921875\n",
      "2018-07-12T18:36:21.531826: step 1674, loss 0.316933, acc 0.828125\n",
      "2018-07-12T18:36:23.183466: step 1675, loss 0.459195, acc 0.8125\n",
      "2018-07-12T18:36:24.777633: step 1676, loss 0.521522, acc 0.734375\n",
      "2018-07-12T18:36:26.413460: step 1677, loss 0.413134, acc 0.8125\n",
      "2018-07-12T18:36:28.085479: step 1678, loss 0.361188, acc 0.8125\n",
      "2018-07-12T18:36:29.670416: step 1679, loss 0.373212, acc 0.828125\n",
      "2018-07-12T18:36:31.265520: step 1680, loss 0.502436, acc 0.765625\n",
      "2018-07-12T18:36:32.835434: step 1681, loss 0.400853, acc 0.796875\n",
      "2018-07-12T18:36:34.483947: step 1682, loss 0.373277, acc 0.828125\n",
      "2018-07-12T18:36:36.157057: step 1683, loss 0.595841, acc 0.765625\n",
      "2018-07-12T18:36:37.859574: step 1684, loss 0.45593, acc 0.8125\n",
      "2018-07-12T18:36:39.521950: step 1685, loss 0.503802, acc 0.78125\n",
      "2018-07-12T18:36:41.174433: step 1686, loss 0.541371, acc 0.6875\n",
      "2018-07-12T18:36:42.821099: step 1687, loss 0.5568, acc 0.734375\n",
      "2018-07-12T18:36:44.492249: step 1688, loss 0.382934, acc 0.84375\n",
      "2018-07-12T18:36:46.136515: step 1689, loss 0.493816, acc 0.765625\n",
      "2018-07-12T18:36:47.777704: step 1690, loss 0.384063, acc 0.796875\n",
      "2018-07-12T18:36:49.403086: step 1691, loss 0.330971, acc 0.859375\n",
      "2018-07-12T18:36:51.090022: step 1692, loss 0.339221, acc 0.859375\n",
      "2018-07-12T18:36:52.757751: step 1693, loss 0.336916, acc 0.828125\n",
      "2018-07-12T18:36:54.362892: step 1694, loss 0.397693, acc 0.8125\n",
      "2018-07-12T18:36:55.930780: step 1695, loss 0.38945, acc 0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:36:57.530170: step 1696, loss 0.416882, acc 0.796875\n",
      "2018-07-12T18:36:59.159551: step 1697, loss 0.309341, acc 0.84375\n",
      "2018-07-12T18:37:00.784658: step 1698, loss 0.416266, acc 0.8125\n",
      "2018-07-12T18:37:02.432539: step 1699, loss 0.415476, acc 0.75\n",
      "2018-07-12T18:37:04.128296: step 1700, loss 0.468827, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:37:22.063415: step 1700, loss 0.358816, acc 0.8468\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1700\n",
      "\n",
      "2018-07-12T18:37:24.716288: step 1701, loss 0.51818, acc 0.75\n",
      "2018-07-12T18:37:26.344894: step 1702, loss 0.543421, acc 0.75\n",
      "2018-07-12T18:37:27.941373: step 1703, loss 0.439262, acc 0.796875\n",
      "2018-07-12T18:37:29.518831: step 1704, loss 0.471169, acc 0.796875\n",
      "2018-07-12T18:37:31.112196: step 1705, loss 0.330516, acc 0.890625\n",
      "2018-07-12T18:37:32.712462: step 1706, loss 0.427705, acc 0.78125\n",
      "2018-07-12T18:37:34.312061: step 1707, loss 0.461187, acc 0.8125\n",
      "2018-07-12T18:37:35.938856: step 1708, loss 0.417175, acc 0.78125\n",
      "2018-07-12T18:37:37.606791: step 1709, loss 0.439994, acc 0.796875\n",
      "2018-07-12T18:37:39.233015: step 1710, loss 0.371677, acc 0.796875\n",
      "2018-07-12T18:37:40.901629: step 1711, loss 0.454894, acc 0.828125\n",
      "2018-07-12T18:37:42.544176: step 1712, loss 0.543998, acc 0.765625\n",
      "2018-07-12T18:37:44.148902: step 1713, loss 0.487827, acc 0.8125\n",
      "2018-07-12T18:37:45.789857: step 1714, loss 0.330884, acc 0.875\n",
      "2018-07-12T18:37:47.485387: step 1715, loss 0.510776, acc 0.78125\n",
      "2018-07-12T18:37:49.090946: step 1716, loss 0.428752, acc 0.796875\n",
      "2018-07-12T18:37:50.684452: step 1717, loss 0.43974, acc 0.8125\n",
      "2018-07-12T18:37:52.293708: step 1718, loss 0.455223, acc 0.796875\n",
      "2018-07-12T18:37:53.929520: step 1719, loss 0.430628, acc 0.828125\n",
      "2018-07-12T18:37:55.560675: step 1720, loss 0.44796, acc 0.765625\n",
      "2018-07-12T18:37:57.207134: step 1721, loss 0.295347, acc 0.875\n",
      "2018-07-12T18:37:58.795149: step 1722, loss 0.395542, acc 0.796875\n",
      "2018-07-12T18:38:00.458895: step 1723, loss 0.3737, acc 0.78125\n",
      "2018-07-12T18:38:02.036358: step 1724, loss 0.312011, acc 0.859375\n",
      "2018-07-12T18:38:03.683907: step 1725, loss 0.607512, acc 0.65625\n",
      "2018-07-12T18:38:05.276616: step 1726, loss 0.473871, acc 0.796875\n",
      "2018-07-12T18:38:06.903344: step 1727, loss 0.405748, acc 0.828125\n",
      "2018-07-12T18:38:08.612133: step 1728, loss 0.283044, acc 0.890625\n",
      "2018-07-12T18:38:10.259302: step 1729, loss 0.440728, acc 0.828125\n",
      "2018-07-12T18:38:11.851461: step 1730, loss 0.671861, acc 0.734375\n",
      "2018-07-12T18:38:13.443605: step 1731, loss 0.29541, acc 0.859375\n",
      "2018-07-12T18:38:15.061242: step 1732, loss 0.402186, acc 0.828125\n",
      "2018-07-12T18:38:16.697289: step 1733, loss 0.401692, acc 0.796875\n",
      "2018-07-12T18:38:18.333898: step 1734, loss 0.581865, acc 0.65625\n",
      "2018-07-12T18:38:19.953355: step 1735, loss 0.401514, acc 0.828125\n",
      "2018-07-12T18:38:21.591773: step 1736, loss 0.394024, acc 0.84375\n",
      "2018-07-12T18:38:23.197560: step 1737, loss 0.323417, acc 0.921875\n",
      "2018-07-12T18:38:24.740427: step 1738, loss 0.362069, acc 0.84375\n",
      "2018-07-12T18:38:26.486208: step 1739, loss 0.305795, acc 0.875\n",
      "2018-07-12T18:38:28.095356: step 1740, loss 0.419235, acc 0.8125\n",
      "2018-07-12T18:38:29.699421: step 1741, loss 0.459999, acc 0.71875\n",
      "2018-07-12T18:38:31.288889: step 1742, loss 0.543422, acc 0.8125\n",
      "2018-07-12T18:38:32.959437: step 1743, loss 0.455965, acc 0.765625\n",
      "2018-07-12T18:38:34.615773: step 1744, loss 0.359081, acc 0.84375\n",
      "2018-07-12T18:38:36.273856: step 1745, loss 0.245687, acc 0.90625\n",
      "2018-07-12T18:38:37.904144: step 1746, loss 0.488019, acc 0.78125\n",
      "2018-07-12T18:38:39.533060: step 1747, loss 0.407385, acc 0.796875\n",
      "2018-07-12T18:38:41.165466: step 1748, loss 0.412802, acc 0.796875\n",
      "2018-07-12T18:38:42.919126: step 1749, loss 0.39442, acc 0.859375\n",
      "2018-07-12T18:38:44.609178: step 1750, loss 0.3316, acc 0.859375\n",
      "2018-07-12T18:38:46.234488: step 1751, loss 0.387663, acc 0.828125\n",
      "2018-07-12T18:38:47.841111: step 1752, loss 0.415404, acc 0.765625\n",
      "2018-07-12T18:38:49.482200: step 1753, loss 0.388916, acc 0.765625\n",
      "2018-07-12T18:38:51.096291: step 1754, loss 0.354051, acc 0.875\n",
      "2018-07-12T18:38:52.726751: step 1755, loss 0.592309, acc 0.734375\n",
      "2018-07-12T18:38:54.378921: step 1756, loss 0.360402, acc 0.8125\n",
      "2018-07-12T18:38:56.004342: step 1757, loss 0.31056, acc 0.890625\n",
      "2018-07-12T18:38:57.655415: step 1758, loss 0.497421, acc 0.75\n",
      "2018-07-12T18:38:59.252894: step 1759, loss 0.484072, acc 0.84375\n",
      "2018-07-12T18:39:00.264530: step 1760, loss 0.319983, acc 0.861111\n",
      "2018-07-12T18:39:01.864602: step 1761, loss 0.466919, acc 0.796875\n",
      "2018-07-12T18:39:03.483149: step 1762, loss 0.432744, acc 0.78125\n",
      "2018-07-12T18:39:05.136574: step 1763, loss 0.548417, acc 0.6875\n",
      "2018-07-12T18:39:06.760537: step 1764, loss 0.353938, acc 0.8125\n",
      "2018-07-12T18:39:08.363533: step 1765, loss 0.370842, acc 0.8125\n",
      "2018-07-12T18:39:09.948866: step 1766, loss 0.46172, acc 0.78125\n",
      "2018-07-12T18:39:11.624803: step 1767, loss 0.387177, acc 0.84375\n",
      "2018-07-12T18:39:13.247016: step 1768, loss 0.439867, acc 0.78125\n",
      "2018-07-12T18:39:14.939986: step 1769, loss 0.394343, acc 0.796875\n",
      "2018-07-12T18:39:16.574985: step 1770, loss 0.382503, acc 0.828125\n",
      "2018-07-12T18:39:18.157089: step 1771, loss 0.424919, acc 0.8125\n",
      "2018-07-12T18:39:19.808352: step 1772, loss 0.319708, acc 0.859375\n",
      "2018-07-12T18:39:21.395292: step 1773, loss 0.33576, acc 0.84375\n",
      "2018-07-12T18:39:23.022492: step 1774, loss 0.308621, acc 0.890625\n",
      "2018-07-12T18:39:24.679044: step 1775, loss 0.381858, acc 0.8125\n",
      "2018-07-12T18:39:26.263414: step 1776, loss 0.362643, acc 0.84375\n",
      "2018-07-12T18:39:27.933079: step 1777, loss 0.353107, acc 0.8125\n",
      "2018-07-12T18:39:29.519344: step 1778, loss 0.30727, acc 0.921875\n",
      "2018-07-12T18:39:31.106824: step 1779, loss 0.247099, acc 0.921875\n",
      "2018-07-12T18:39:32.729601: step 1780, loss 0.413807, acc 0.796875\n",
      "2018-07-12T18:39:34.309079: step 1781, loss 0.487874, acc 0.796875\n",
      "2018-07-12T18:39:35.902322: step 1782, loss 0.379016, acc 0.859375\n",
      "2018-07-12T18:39:37.573466: step 1783, loss 0.363632, acc 0.859375\n",
      "2018-07-12T18:39:39.196523: step 1784, loss 0.29004, acc 0.921875\n",
      "2018-07-12T18:39:40.823672: step 1785, loss 0.349795, acc 0.859375\n",
      "2018-07-12T18:39:42.481575: step 1786, loss 0.370955, acc 0.8125\n",
      "2018-07-12T18:39:44.054762: step 1787, loss 0.38875, acc 0.84375\n",
      "2018-07-12T18:39:45.660201: step 1788, loss 0.366706, acc 0.828125\n",
      "2018-07-12T18:39:47.284054: step 1789, loss 0.365011, acc 0.859375\n",
      "2018-07-12T18:39:48.881939: step 1790, loss 0.316167, acc 0.84375\n",
      "2018-07-12T18:39:50.533831: step 1791, loss 0.408767, acc 0.796875\n",
      "2018-07-12T18:39:52.190949: step 1792, loss 0.355516, acc 0.859375\n",
      "2018-07-12T18:39:53.862094: step 1793, loss 0.449767, acc 0.796875\n",
      "2018-07-12T18:39:55.509054: step 1794, loss 0.307074, acc 0.859375\n",
      "2018-07-12T18:39:57.208383: step 1795, loss 0.389465, acc 0.8125\n",
      "2018-07-12T18:39:58.862207: step 1796, loss 0.409202, acc 0.859375\n",
      "2018-07-12T18:40:00.544763: step 1797, loss 0.416574, acc 0.78125\n",
      "2018-07-12T18:40:02.208889: step 1798, loss 0.509137, acc 0.828125\n",
      "2018-07-12T18:40:03.844381: step 1799, loss 0.339301, acc 0.828125\n",
      "2018-07-12T18:40:05.528494: step 1800, loss 0.288042, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:40:23.434178: step 1800, loss 0.351009, acc 0.85\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1800\n",
      "\n",
      "2018-07-12T18:40:26.125121: step 1801, loss 0.381595, acc 0.828125\n",
      "2018-07-12T18:40:27.785595: step 1802, loss 0.423063, acc 0.828125\n",
      "2018-07-12T18:40:29.391396: step 1803, loss 0.447716, acc 0.796875\n",
      "2018-07-12T18:40:31.106362: step 1804, loss 0.325599, acc 0.828125\n",
      "2018-07-12T18:40:32.756485: step 1805, loss 0.312458, acc 0.890625\n",
      "2018-07-12T18:40:34.404819: step 1806, loss 0.483079, acc 0.84375\n",
      "2018-07-12T18:40:36.027242: step 1807, loss 0.384309, acc 0.8125\n",
      "2018-07-12T18:40:37.637904: step 1808, loss 0.268729, acc 0.875\n",
      "2018-07-12T18:40:39.305319: step 1809, loss 0.351212, acc 0.8125\n",
      "2018-07-12T18:40:40.946713: step 1810, loss 0.452767, acc 0.8125\n",
      "2018-07-12T18:40:42.560746: step 1811, loss 0.48272, acc 0.8125\n",
      "2018-07-12T18:40:44.187037: step 1812, loss 0.278818, acc 0.8125\n",
      "2018-07-12T18:40:45.818959: step 1813, loss 0.41124, acc 0.78125\n",
      "2018-07-12T18:40:47.470061: step 1814, loss 0.55541, acc 0.65625\n",
      "2018-07-12T18:40:49.079607: step 1815, loss 0.295961, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:40:50.763475: step 1816, loss 0.328223, acc 0.84375\n",
      "2018-07-12T18:40:52.414798: step 1817, loss 0.40254, acc 0.859375\n",
      "2018-07-12T18:40:54.044279: step 1818, loss 0.395119, acc 0.875\n",
      "2018-07-12T18:40:55.736830: step 1819, loss 0.326162, acc 0.875\n",
      "2018-07-12T18:40:57.388926: step 1820, loss 0.325636, acc 0.890625\n",
      "2018-07-12T18:40:59.061057: step 1821, loss 0.32853, acc 0.859375\n",
      "2018-07-12T18:41:00.682655: step 1822, loss 0.285495, acc 0.90625\n",
      "2018-07-12T18:41:02.265839: step 1823, loss 0.458167, acc 0.8125\n",
      "2018-07-12T18:41:03.882442: step 1824, loss 0.243768, acc 0.921875\n",
      "2018-07-12T18:41:05.537879: step 1825, loss 0.238628, acc 0.875\n",
      "2018-07-12T18:41:07.142294: step 1826, loss 0.400147, acc 0.828125\n",
      "2018-07-12T18:41:08.785605: step 1827, loss 0.363834, acc 0.84375\n",
      "2018-07-12T18:41:10.425899: step 1828, loss 0.275649, acc 0.921875\n",
      "2018-07-12T18:41:12.078500: step 1829, loss 0.440949, acc 0.84375\n",
      "2018-07-12T18:41:13.740448: step 1830, loss 0.500766, acc 0.8125\n",
      "2018-07-12T18:41:15.381977: step 1831, loss 0.311387, acc 0.890625\n",
      "2018-07-12T18:41:17.064041: step 1832, loss 0.419427, acc 0.78125\n",
      "2018-07-12T18:41:18.755530: step 1833, loss 0.353111, acc 0.859375\n",
      "2018-07-12T18:41:20.344642: step 1834, loss 0.39535, acc 0.859375\n",
      "2018-07-12T18:41:21.965256: step 1835, loss 0.446077, acc 0.8125\n",
      "2018-07-12T18:41:23.673477: step 1836, loss 0.350967, acc 0.84375\n",
      "2018-07-12T18:41:25.306049: step 1837, loss 0.308906, acc 0.890625\n",
      "2018-07-12T18:41:26.940829: step 1838, loss 0.336226, acc 0.828125\n",
      "2018-07-12T18:41:28.576932: step 1839, loss 0.411694, acc 0.78125\n",
      "2018-07-12T18:41:30.201111: step 1840, loss 0.293289, acc 0.875\n",
      "2018-07-12T18:41:31.907352: step 1841, loss 0.471508, acc 0.828125\n",
      "2018-07-12T18:41:33.509671: step 1842, loss 0.403435, acc 0.796875\n",
      "2018-07-12T18:41:35.224400: step 1843, loss 0.544695, acc 0.78125\n",
      "2018-07-12T18:41:36.871882: step 1844, loss 0.365996, acc 0.8125\n",
      "2018-07-12T18:41:38.487428: step 1845, loss 0.402841, acc 0.84375\n",
      "2018-07-12T18:41:40.098406: step 1846, loss 0.261977, acc 0.875\n",
      "2018-07-12T18:41:41.701850: step 1847, loss 0.313068, acc 0.875\n",
      "2018-07-12T18:41:43.401219: step 1848, loss 0.300935, acc 0.859375\n",
      "2018-07-12T18:41:45.042700: step 1849, loss 0.338765, acc 0.828125\n",
      "2018-07-12T18:41:46.669367: step 1850, loss 0.369938, acc 0.828125\n",
      "2018-07-12T18:41:48.275595: step 1851, loss 0.345006, acc 0.84375\n",
      "2018-07-12T18:41:49.942734: step 1852, loss 0.39129, acc 0.828125\n",
      "2018-07-12T18:41:51.555951: step 1853, loss 0.302746, acc 0.875\n",
      "2018-07-12T18:41:53.153444: step 1854, loss 0.350978, acc 0.859375\n",
      "2018-07-12T18:41:54.725215: step 1855, loss 0.383435, acc 0.859375\n",
      "2018-07-12T18:41:56.373275: step 1856, loss 0.458747, acc 0.734375\n",
      "2018-07-12T18:41:58.006073: step 1857, loss 0.439349, acc 0.796875\n",
      "2018-07-12T18:41:59.624164: step 1858, loss 0.259955, acc 0.90625\n",
      "2018-07-12T18:42:01.268684: step 1859, loss 0.417261, acc 0.796875\n",
      "2018-07-12T18:42:02.921969: step 1860, loss 0.708148, acc 0.71875\n",
      "2018-07-12T18:42:04.553068: step 1861, loss 0.398634, acc 0.765625\n",
      "2018-07-12T18:42:06.178747: step 1862, loss 0.501575, acc 0.8125\n",
      "2018-07-12T18:42:07.885866: step 1863, loss 0.393222, acc 0.8125\n",
      "2018-07-12T18:42:09.567568: step 1864, loss 0.493325, acc 0.78125\n",
      "2018-07-12T18:42:11.212643: step 1865, loss 0.288456, acc 0.828125\n",
      "2018-07-12T18:42:12.837655: step 1866, loss 0.34029, acc 0.8125\n",
      "2018-07-12T18:42:14.487315: step 1867, loss 0.330306, acc 0.84375\n",
      "2018-07-12T18:42:16.151980: step 1868, loss 0.319075, acc 0.84375\n",
      "2018-07-12T18:42:17.823052: step 1869, loss 0.230615, acc 0.890625\n",
      "2018-07-12T18:42:19.457800: step 1870, loss 0.36965, acc 0.8125\n",
      "2018-07-12T18:42:21.090236: step 1871, loss 0.33788, acc 0.84375\n",
      "2018-07-12T18:42:22.712467: step 1872, loss 0.54268, acc 0.75\n",
      "2018-07-12T18:42:24.327022: step 1873, loss 0.301315, acc 0.859375\n",
      "2018-07-12T18:42:25.945066: step 1874, loss 0.278038, acc 0.859375\n",
      "2018-07-12T18:42:27.595601: step 1875, loss 0.357289, acc 0.859375\n",
      "2018-07-12T18:42:29.218744: step 1876, loss 0.335727, acc 0.875\n",
      "2018-07-12T18:42:30.908104: step 1877, loss 0.358412, acc 0.890625\n",
      "2018-07-12T18:42:32.491961: step 1878, loss 0.441146, acc 0.765625\n",
      "2018-07-12T18:42:34.101272: step 1879, loss 0.540558, acc 0.75\n",
      "2018-07-12T18:42:35.679296: step 1880, loss 0.48297, acc 0.75\n",
      "2018-07-12T18:42:37.302359: step 1881, loss 0.257981, acc 0.890625\n",
      "2018-07-12T18:42:38.875931: step 1882, loss 0.540481, acc 0.78125\n",
      "2018-07-12T18:42:40.498653: step 1883, loss 0.335697, acc 0.84375\n",
      "2018-07-12T18:42:42.152044: step 1884, loss 0.418987, acc 0.8125\n",
      "2018-07-12T18:42:43.780984: step 1885, loss 0.382752, acc 0.75\n",
      "2018-07-12T18:42:45.364312: step 1886, loss 0.413624, acc 0.765625\n",
      "2018-07-12T18:42:47.049274: step 1887, loss 0.402783, acc 0.84375\n",
      "2018-07-12T18:42:48.698190: step 1888, loss 0.316153, acc 0.859375\n",
      "2018-07-12T18:42:50.377277: step 1889, loss 0.498756, acc 0.78125\n",
      "2018-07-12T18:42:52.023133: step 1890, loss 0.402193, acc 0.828125\n",
      "2018-07-12T18:42:53.645219: step 1891, loss 0.39922, acc 0.84375\n",
      "2018-07-12T18:42:55.253076: step 1892, loss 0.339128, acc 0.8125\n",
      "2018-07-12T18:42:56.894376: step 1893, loss 0.50344, acc 0.734375\n",
      "2018-07-12T18:42:58.606654: step 1894, loss 0.271687, acc 0.859375\n",
      "2018-07-12T18:43:00.225340: step 1895, loss 0.450645, acc 0.796875\n",
      "2018-07-12T18:43:01.891936: step 1896, loss 0.505604, acc 0.84375\n",
      "2018-07-12T18:43:03.474385: step 1897, loss 0.299806, acc 0.828125\n",
      "2018-07-12T18:43:05.171729: step 1898, loss 0.410519, acc 0.796875\n",
      "2018-07-12T18:43:06.775649: step 1899, loss 0.366576, acc 0.796875\n",
      "2018-07-12T18:43:08.442707: step 1900, loss 0.478304, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:43:26.240696: step 1900, loss 0.352659, acc 0.848\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-1900\n",
      "\n",
      "2018-07-12T18:43:28.740910: step 1901, loss 0.412729, acc 0.8125\n",
      "2018-07-12T18:43:30.375350: step 1902, loss 0.346835, acc 0.859375\n",
      "2018-07-12T18:43:31.965524: step 1903, loss 0.463602, acc 0.828125\n",
      "2018-07-12T18:43:33.587727: step 1904, loss 0.339338, acc 0.875\n",
      "2018-07-12T18:43:35.212583: step 1905, loss 0.450548, acc 0.78125\n",
      "2018-07-12T18:43:36.836481: step 1906, loss 0.350002, acc 0.875\n",
      "2018-07-12T18:43:38.462604: step 1907, loss 0.327407, acc 0.875\n",
      "2018-07-12T18:43:40.053105: step 1908, loss 0.408248, acc 0.796875\n",
      "2018-07-12T18:43:41.629980: step 1909, loss 0.334771, acc 0.859375\n",
      "2018-07-12T18:43:43.276481: step 1910, loss 0.283601, acc 0.875\n",
      "2018-07-12T18:43:44.881808: step 1911, loss 0.418452, acc 0.8125\n",
      "2018-07-12T18:43:46.475920: step 1912, loss 0.3647, acc 0.796875\n",
      "2018-07-12T18:43:48.136740: step 1913, loss 0.37029, acc 0.859375\n",
      "2018-07-12T18:43:49.720092: step 1914, loss 0.358954, acc 0.84375\n",
      "2018-07-12T18:43:51.343304: step 1915, loss 0.390441, acc 0.796875\n",
      "2018-07-12T18:43:52.994404: step 1916, loss 0.436137, acc 0.796875\n",
      "2018-07-12T18:43:54.600000: step 1917, loss 0.449296, acc 0.8125\n",
      "2018-07-12T18:43:56.228754: step 1918, loss 0.427335, acc 0.8125\n",
      "2018-07-12T18:43:57.920055: step 1919, loss 0.540731, acc 0.765625\n",
      "2018-07-12T18:43:59.534745: step 1920, loss 0.348724, acc 0.859375\n",
      "2018-07-12T18:44:01.221951: step 1921, loss 0.333006, acc 0.84375\n",
      "2018-07-12T18:44:02.829567: step 1922, loss 0.465778, acc 0.8125\n",
      "2018-07-12T18:44:04.457303: step 1923, loss 0.243453, acc 0.890625\n",
      "2018-07-12T18:44:06.052134: step 1924, loss 0.344656, acc 0.828125\n",
      "2018-07-12T18:44:07.649095: step 1925, loss 0.372276, acc 0.859375\n",
      "2018-07-12T18:44:09.296298: step 1926, loss 0.417548, acc 0.828125\n",
      "2018-07-12T18:44:10.976293: step 1927, loss 0.362455, acc 0.828125\n",
      "2018-07-12T18:44:12.596794: step 1928, loss 0.488878, acc 0.78125\n",
      "2018-07-12T18:44:14.240677: step 1929, loss 0.204369, acc 0.921875\n",
      "2018-07-12T18:44:15.808447: step 1930, loss 0.257312, acc 0.90625\n",
      "2018-07-12T18:44:17.441478: step 1931, loss 0.363601, acc 0.828125\n",
      "2018-07-12T18:44:19.115094: step 1932, loss 0.325176, acc 0.84375\n",
      "2018-07-12T18:44:20.730782: step 1933, loss 0.250429, acc 0.890625\n",
      "2018-07-12T18:44:22.398114: step 1934, loss 0.328321, acc 0.828125\n",
      "2018-07-12T18:44:24.041960: step 1935, loss 0.264354, acc 0.890625\n",
      "2018-07-12T18:44:25.676375: step 1936, loss 0.260872, acc 0.90625\n",
      "2018-07-12T18:44:27.343183: step 1937, loss 0.381664, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:44:28.987087: step 1938, loss 0.386709, acc 0.828125\n",
      "2018-07-12T18:44:30.608454: step 1939, loss 0.526295, acc 0.796875\n",
      "2018-07-12T18:44:32.221947: step 1940, loss 0.334643, acc 0.796875\n",
      "2018-07-12T18:44:33.874956: step 1941, loss 0.374915, acc 0.859375\n",
      "2018-07-12T18:44:35.562579: step 1942, loss 0.255134, acc 0.890625\n",
      "2018-07-12T18:44:37.195685: step 1943, loss 0.57051, acc 0.765625\n",
      "2018-07-12T18:44:38.842455: step 1944, loss 0.28469, acc 0.828125\n",
      "2018-07-12T18:44:40.422739: step 1945, loss 0.462423, acc 0.765625\n",
      "2018-07-12T18:44:42.021939: step 1946, loss 0.229004, acc 0.9375\n",
      "2018-07-12T18:44:43.681729: step 1947, loss 0.296038, acc 0.859375\n",
      "2018-07-12T18:44:45.328793: step 1948, loss 0.534071, acc 0.8125\n",
      "2018-07-12T18:44:46.971049: step 1949, loss 0.343438, acc 0.828125\n",
      "2018-07-12T18:44:48.587450: step 1950, loss 0.499436, acc 0.75\n",
      "2018-07-12T18:44:50.189481: step 1951, loss 0.336318, acc 0.8125\n",
      "2018-07-12T18:44:51.846719: step 1952, loss 0.329297, acc 0.84375\n",
      "2018-07-12T18:44:53.543448: step 1953, loss 0.388919, acc 0.84375\n",
      "2018-07-12T18:44:55.243962: step 1954, loss 0.480556, acc 0.828125\n",
      "2018-07-12T18:44:56.840237: step 1955, loss 0.380327, acc 0.84375\n",
      "2018-07-12T18:44:58.517557: step 1956, loss 0.469368, acc 0.828125\n",
      "2018-07-12T18:45:00.191208: step 1957, loss 0.432204, acc 0.796875\n",
      "2018-07-12T18:45:01.795932: step 1958, loss 0.331524, acc 0.875\n",
      "2018-07-12T18:45:03.414057: step 1959, loss 0.412942, acc 0.796875\n",
      "2018-07-12T18:45:05.032847: step 1960, loss 0.537823, acc 0.75\n",
      "2018-07-12T18:45:06.704716: step 1961, loss 0.453678, acc 0.796875\n",
      "2018-07-12T18:45:08.287608: step 1962, loss 0.469273, acc 0.78125\n",
      "2018-07-12T18:45:09.918588: step 1963, loss 0.335741, acc 0.859375\n",
      "2018-07-12T18:45:11.543203: step 1964, loss 0.472077, acc 0.765625\n",
      "2018-07-12T18:45:13.179009: step 1965, loss 0.392992, acc 0.765625\n",
      "2018-07-12T18:45:14.797308: step 1966, loss 0.397347, acc 0.78125\n",
      "2018-07-12T18:45:16.474697: step 1967, loss 0.422634, acc 0.765625\n",
      "2018-07-12T18:45:18.138997: step 1968, loss 0.238233, acc 0.921875\n",
      "2018-07-12T18:45:19.792244: step 1969, loss 0.343187, acc 0.875\n",
      "2018-07-12T18:45:21.389893: step 1970, loss 0.356411, acc 0.84375\n",
      "2018-07-12T18:45:23.038053: step 1971, loss 0.508134, acc 0.78125\n",
      "2018-07-12T18:45:24.703936: step 1972, loss 0.39009, acc 0.84375\n",
      "2018-07-12T18:45:26.407637: step 1973, loss 0.398445, acc 0.84375\n",
      "2018-07-12T18:45:28.046785: step 1974, loss 0.345682, acc 0.84375\n",
      "2018-07-12T18:45:29.640105: step 1975, loss 0.416064, acc 0.796875\n",
      "2018-07-12T18:45:31.242816: step 1976, loss 0.494017, acc 0.75\n",
      "2018-07-12T18:45:32.855545: step 1977, loss 0.325282, acc 0.84375\n",
      "2018-07-12T18:45:34.472071: step 1978, loss 0.558107, acc 0.71875\n",
      "2018-07-12T18:45:36.092430: step 1979, loss 0.342539, acc 0.859375\n",
      "2018-07-12T18:45:37.695265: step 1980, loss 0.395484, acc 0.84375\n",
      "2018-07-12T18:45:39.331125: step 1981, loss 0.306697, acc 0.859375\n",
      "2018-07-12T18:45:40.986976: step 1982, loss 0.469716, acc 0.734375\n",
      "2018-07-12T18:45:42.621595: step 1983, loss 0.387617, acc 0.84375\n",
      "2018-07-12T18:45:44.226560: step 1984, loss 0.393819, acc 0.765625\n",
      "2018-07-12T18:45:45.808657: step 1985, loss 0.297284, acc 0.875\n",
      "2018-07-12T18:45:47.443101: step 1986, loss 0.338506, acc 0.875\n",
      "2018-07-12T18:45:49.106115: step 1987, loss 0.292743, acc 0.875\n",
      "2018-07-12T18:45:50.780627: step 1988, loss 0.292111, acc 0.859375\n",
      "2018-07-12T18:45:52.394540: step 1989, loss 0.269691, acc 0.875\n",
      "2018-07-12T18:45:53.974263: step 1990, loss 0.309961, acc 0.84375\n",
      "2018-07-12T18:45:55.586455: step 1991, loss 0.380158, acc 0.8125\n",
      "2018-07-12T18:45:57.181401: step 1992, loss 0.453282, acc 0.765625\n",
      "2018-07-12T18:45:58.783500: step 1993, loss 0.364819, acc 0.84375\n",
      "2018-07-12T18:46:00.375484: step 1994, loss 0.404076, acc 0.796875\n",
      "2018-07-12T18:46:01.957092: step 1995, loss 0.36491, acc 0.859375\n",
      "2018-07-12T18:46:03.534109: step 1996, loss 0.405888, acc 0.765625\n",
      "2018-07-12T18:46:05.147277: step 1997, loss 0.355018, acc 0.8125\n",
      "2018-07-12T18:46:06.787065: step 1998, loss 0.306265, acc 0.859375\n",
      "2018-07-12T18:46:08.435561: step 1999, loss 0.305589, acc 0.875\n",
      "2018-07-12T18:46:10.114436: step 2000, loss 0.2896, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:46:27.895396: step 2000, loss 0.344798, acc 0.8544\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2000\n",
      "\n",
      "2018-07-12T18:46:30.524018: step 2001, loss 0.268973, acc 0.890625\n",
      "2018-07-12T18:46:32.150582: step 2002, loss 0.442033, acc 0.78125\n",
      "2018-07-12T18:46:33.747060: step 2003, loss 0.299928, acc 0.859375\n",
      "2018-07-12T18:46:35.405888: step 2004, loss 0.478352, acc 0.78125\n",
      "2018-07-12T18:46:37.053786: step 2005, loss 0.399946, acc 0.78125\n",
      "2018-07-12T18:46:38.691701: step 2006, loss 0.287079, acc 0.875\n",
      "2018-07-12T18:46:40.344830: step 2007, loss 0.411111, acc 0.78125\n",
      "2018-07-12T18:46:41.970245: step 2008, loss 0.322641, acc 0.828125\n",
      "2018-07-12T18:46:43.607437: step 2009, loss 0.342345, acc 0.84375\n",
      "2018-07-12T18:46:45.340939: step 2010, loss 0.392888, acc 0.8125\n",
      "2018-07-12T18:46:47.039815: step 2011, loss 0.365351, acc 0.828125\n",
      "2018-07-12T18:46:48.655775: step 2012, loss 0.382607, acc 0.875\n",
      "2018-07-12T18:46:50.270708: step 2013, loss 0.299676, acc 0.859375\n",
      "2018-07-12T18:46:51.900105: step 2014, loss 0.515559, acc 0.78125\n",
      "2018-07-12T18:46:53.511440: step 2015, loss 0.31102, acc 0.859375\n",
      "2018-07-12T18:46:55.099894: step 2016, loss 0.393516, acc 0.84375\n",
      "2018-07-12T18:46:56.750962: step 2017, loss 0.338766, acc 0.84375\n",
      "2018-07-12T18:46:58.354248: step 2018, loss 0.266733, acc 0.921875\n",
      "2018-07-12T18:47:00.035118: step 2019, loss 0.379875, acc 0.828125\n",
      "2018-07-12T18:47:01.658381: step 2020, loss 0.395244, acc 0.859375\n",
      "2018-07-12T18:47:03.361293: step 2021, loss 0.453436, acc 0.765625\n",
      "2018-07-12T18:47:05.005415: step 2022, loss 0.249328, acc 0.875\n",
      "2018-07-12T18:47:06.651551: step 2023, loss 0.364301, acc 0.8125\n",
      "2018-07-12T18:47:08.269191: step 2024, loss 0.300703, acc 0.796875\n",
      "2018-07-12T18:47:09.906793: step 2025, loss 0.347459, acc 0.84375\n",
      "2018-07-12T18:47:11.565350: step 2026, loss 0.338161, acc 0.828125\n",
      "2018-07-12T18:47:13.173344: step 2027, loss 0.420131, acc 0.78125\n",
      "2018-07-12T18:47:14.818019: step 2028, loss 0.362547, acc 0.84375\n",
      "2018-07-12T18:47:16.536968: step 2029, loss 0.337842, acc 0.8125\n",
      "2018-07-12T18:47:18.172168: step 2030, loss 0.422866, acc 0.796875\n",
      "2018-07-12T18:47:19.807547: step 2031, loss 0.291401, acc 0.859375\n",
      "2018-07-12T18:47:21.450332: step 2032, loss 0.264433, acc 0.859375\n",
      "2018-07-12T18:47:23.130222: step 2033, loss 0.269092, acc 0.875\n",
      "2018-07-12T18:47:24.771166: step 2034, loss 0.268199, acc 0.875\n",
      "2018-07-12T18:47:26.400095: step 2035, loss 0.525785, acc 0.765625\n",
      "2018-07-12T18:47:28.039182: step 2036, loss 0.277578, acc 0.890625\n",
      "2018-07-12T18:47:29.636317: step 2037, loss 0.356818, acc 0.84375\n",
      "2018-07-12T18:47:31.278847: step 2038, loss 0.362661, acc 0.828125\n",
      "2018-07-12T18:47:32.897495: step 2039, loss 0.427594, acc 0.8125\n",
      "2018-07-12T18:47:34.501938: step 2040, loss 0.358435, acc 0.8125\n",
      "2018-07-12T18:47:36.138644: step 2041, loss 0.252547, acc 0.90625\n",
      "2018-07-12T18:47:37.733384: step 2042, loss 0.501571, acc 0.8125\n",
      "2018-07-12T18:47:39.420155: step 2043, loss 0.290125, acc 0.828125\n",
      "2018-07-12T18:47:41.079747: step 2044, loss 0.279945, acc 0.875\n",
      "2018-07-12T18:47:42.764503: step 2045, loss 0.33169, acc 0.859375\n",
      "2018-07-12T18:47:44.405198: step 2046, loss 0.366089, acc 0.859375\n",
      "2018-07-12T18:47:46.066892: step 2047, loss 0.287287, acc 0.90625\n",
      "2018-07-12T18:47:47.698613: step 2048, loss 0.440344, acc 0.765625\n",
      "2018-07-12T18:47:49.292014: step 2049, loss 0.407677, acc 0.796875\n",
      "2018-07-12T18:47:50.917179: step 2050, loss 0.347524, acc 0.8125\n",
      "2018-07-12T18:47:52.672415: step 2051, loss 0.385539, acc 0.828125\n",
      "2018-07-12T18:47:54.310715: step 2052, loss 0.294151, acc 0.90625\n",
      "2018-07-12T18:47:55.903418: step 2053, loss 0.31254, acc 0.890625\n",
      "2018-07-12T18:47:57.470377: step 2054, loss 0.488967, acc 0.765625\n",
      "2018-07-12T18:47:59.072472: step 2055, loss 0.428524, acc 0.8125\n",
      "2018-07-12T18:48:00.712215: step 2056, loss 0.32765, acc 0.859375\n",
      "2018-07-12T18:48:02.341754: step 2057, loss 0.461837, acc 0.828125\n",
      "2018-07-12T18:48:03.966363: step 2058, loss 0.306695, acc 0.828125\n",
      "2018-07-12T18:48:05.619409: step 2059, loss 0.257062, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:48:07.293352: step 2060, loss 0.342852, acc 0.828125\n",
      "2018-07-12T18:48:08.987289: step 2061, loss 0.487167, acc 0.796875\n",
      "2018-07-12T18:48:10.698124: step 2062, loss 0.508306, acc 0.8125\n",
      "2018-07-12T18:48:12.322244: step 2063, loss 0.439251, acc 0.78125\n",
      "2018-07-12T18:48:13.940325: step 2064, loss 0.366834, acc 0.859375\n",
      "2018-07-12T18:48:15.533183: step 2065, loss 0.381548, acc 0.8125\n",
      "2018-07-12T18:48:17.126211: step 2066, loss 0.469743, acc 0.75\n",
      "2018-07-12T18:48:18.712729: step 2067, loss 0.384392, acc 0.75\n",
      "2018-07-12T18:48:20.326405: step 2068, loss 0.28655, acc 0.859375\n",
      "2018-07-12T18:48:21.899871: step 2069, loss 0.306799, acc 0.84375\n",
      "2018-07-12T18:48:23.561406: step 2070, loss 0.275833, acc 0.90625\n",
      "2018-07-12T18:48:25.249195: step 2071, loss 0.246374, acc 0.875\n",
      "2018-07-12T18:48:26.910612: step 2072, loss 0.352449, acc 0.8125\n",
      "2018-07-12T18:48:28.503013: step 2073, loss 0.490012, acc 0.765625\n",
      "2018-07-12T18:48:30.129220: step 2074, loss 0.248544, acc 0.890625\n",
      "2018-07-12T18:48:31.747339: step 2075, loss 0.307912, acc 0.84375\n",
      "2018-07-12T18:48:33.398308: step 2076, loss 0.484113, acc 0.734375\n",
      "2018-07-12T18:48:35.024218: step 2077, loss 0.219008, acc 0.921875\n",
      "2018-07-12T18:48:36.706610: step 2078, loss 0.337544, acc 0.828125\n",
      "2018-07-12T18:48:38.302820: step 2079, loss 0.356919, acc 0.890625\n",
      "2018-07-12T18:48:39.965183: step 2080, loss 0.426649, acc 0.8125\n",
      "2018-07-12T18:48:41.555942: step 2081, loss 0.355642, acc 0.8125\n",
      "2018-07-12T18:48:43.217964: step 2082, loss 0.345539, acc 0.875\n",
      "2018-07-12T18:48:44.895007: step 2083, loss 0.348843, acc 0.765625\n",
      "2018-07-12T18:48:46.504660: step 2084, loss 0.408628, acc 0.875\n",
      "2018-07-12T18:48:48.112436: step 2085, loss 0.337076, acc 0.828125\n",
      "2018-07-12T18:48:49.834920: step 2086, loss 0.395719, acc 0.90625\n",
      "2018-07-12T18:48:51.452803: step 2087, loss 0.375233, acc 0.828125\n",
      "2018-07-12T18:48:53.114864: step 2088, loss 0.391049, acc 0.796875\n",
      "2018-07-12T18:48:54.736046: step 2089, loss 0.274909, acc 0.859375\n",
      "2018-07-12T18:48:56.423388: step 2090, loss 0.335008, acc 0.828125\n",
      "2018-07-12T18:48:58.095739: step 2091, loss 0.283396, acc 0.890625\n",
      "2018-07-12T18:48:59.717945: step 2092, loss 0.24698, acc 0.890625\n",
      "2018-07-12T18:49:01.313705: step 2093, loss 0.385778, acc 0.828125\n",
      "2018-07-12T18:49:02.870679: step 2094, loss 0.448764, acc 0.8125\n",
      "2018-07-12T18:49:04.498486: step 2095, loss 0.301425, acc 0.875\n",
      "2018-07-12T18:49:06.064587: step 2096, loss 0.287442, acc 0.890625\n",
      "2018-07-12T18:49:07.699483: step 2097, loss 0.289274, acc 0.859375\n",
      "2018-07-12T18:49:09.267175: step 2098, loss 0.28456, acc 0.84375\n",
      "2018-07-12T18:49:10.880927: step 2099, loss 0.356079, acc 0.796875\n",
      "2018-07-12T18:49:12.548786: step 2100, loss 0.453802, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:49:30.376398: step 2100, loss 0.315743, acc 0.8648\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2100\n",
      "\n",
      "2018-07-12T18:49:33.068087: step 2101, loss 0.295017, acc 0.90625\n",
      "2018-07-12T18:49:34.695664: step 2102, loss 0.395534, acc 0.828125\n",
      "2018-07-12T18:49:36.305774: step 2103, loss 0.365817, acc 0.828125\n",
      "2018-07-12T18:49:37.941486: step 2104, loss 0.317037, acc 0.875\n",
      "2018-07-12T18:49:39.618092: step 2105, loss 0.405611, acc 0.84375\n",
      "2018-07-12T18:49:41.218111: step 2106, loss 0.433684, acc 0.75\n",
      "2018-07-12T18:49:42.911754: step 2107, loss 0.321967, acc 0.84375\n",
      "2018-07-12T18:49:44.541454: step 2108, loss 0.412751, acc 0.78125\n",
      "2018-07-12T18:49:46.192048: step 2109, loss 0.395456, acc 0.8125\n",
      "2018-07-12T18:49:47.902609: step 2110, loss 0.393657, acc 0.84375\n",
      "2018-07-12T18:49:49.656677: step 2111, loss 0.225117, acc 0.90625\n",
      "2018-07-12T18:49:50.696047: step 2112, loss 0.260535, acc 0.916667\n",
      "2018-07-12T18:49:52.292776: step 2113, loss 0.244365, acc 0.90625\n",
      "2018-07-12T18:49:53.886369: step 2114, loss 0.429375, acc 0.75\n",
      "2018-07-12T18:49:55.517071: step 2115, loss 0.405943, acc 0.765625\n",
      "2018-07-12T18:49:57.111601: step 2116, loss 0.308608, acc 0.828125\n",
      "2018-07-12T18:49:58.752797: step 2117, loss 0.3621, acc 0.859375\n",
      "2018-07-12T18:50:00.371361: step 2118, loss 0.307148, acc 0.875\n",
      "2018-07-12T18:50:02.022184: step 2119, loss 0.304622, acc 0.859375\n",
      "2018-07-12T18:50:03.647155: step 2120, loss 0.364271, acc 0.875\n",
      "2018-07-12T18:50:05.338330: step 2121, loss 0.358691, acc 0.828125\n",
      "2018-07-12T18:50:06.971886: step 2122, loss 0.329743, acc 0.84375\n",
      "2018-07-12T18:50:08.563544: step 2123, loss 0.374199, acc 0.828125\n",
      "2018-07-12T18:50:10.194755: step 2124, loss 0.472905, acc 0.828125\n",
      "2018-07-12T18:50:11.832045: step 2125, loss 0.368708, acc 0.828125\n",
      "2018-07-12T18:50:13.443049: step 2126, loss 0.348879, acc 0.84375\n",
      "2018-07-12T18:50:15.010429: step 2127, loss 0.231512, acc 0.90625\n",
      "2018-07-12T18:50:16.599144: step 2128, loss 0.356864, acc 0.796875\n",
      "2018-07-12T18:50:18.233457: step 2129, loss 0.41077, acc 0.84375\n",
      "2018-07-12T18:50:19.854125: step 2130, loss 0.352826, acc 0.84375\n",
      "2018-07-12T18:50:21.528344: step 2131, loss 0.258502, acc 0.90625\n",
      "2018-07-12T18:50:23.159110: step 2132, loss 0.339207, acc 0.859375\n",
      "2018-07-12T18:50:24.800759: step 2133, loss 0.249369, acc 0.890625\n",
      "2018-07-12T18:50:26.406664: step 2134, loss 0.381287, acc 0.796875\n",
      "2018-07-12T18:50:28.013174: step 2135, loss 0.255104, acc 0.90625\n",
      "2018-07-12T18:50:29.633588: step 2136, loss 0.269396, acc 0.890625\n",
      "2018-07-12T18:50:31.358505: step 2137, loss 0.389621, acc 0.828125\n",
      "2018-07-12T18:50:32.953020: step 2138, loss 0.232424, acc 0.90625\n",
      "2018-07-12T18:50:34.593302: step 2139, loss 0.274468, acc 0.84375\n",
      "2018-07-12T18:50:36.179136: step 2140, loss 0.467496, acc 0.828125\n",
      "2018-07-12T18:50:37.768490: step 2141, loss 0.272297, acc 0.859375\n",
      "2018-07-12T18:50:39.411204: step 2142, loss 0.33865, acc 0.84375\n",
      "2018-07-12T18:50:41.019810: step 2143, loss 0.258313, acc 0.84375\n",
      "2018-07-12T18:50:42.605957: step 2144, loss 0.336708, acc 0.84375\n",
      "2018-07-12T18:50:44.283176: step 2145, loss 0.308061, acc 0.828125\n",
      "2018-07-12T18:50:45.868088: step 2146, loss 0.389419, acc 0.78125\n",
      "2018-07-12T18:50:47.519317: step 2147, loss 0.31192, acc 0.890625\n",
      "2018-07-12T18:50:49.151206: step 2148, loss 0.249374, acc 0.953125\n",
      "2018-07-12T18:50:50.791990: step 2149, loss 0.378192, acc 0.84375\n",
      "2018-07-12T18:50:52.403522: step 2150, loss 0.261064, acc 0.859375\n",
      "2018-07-12T18:50:54.033079: step 2151, loss 0.346888, acc 0.875\n",
      "2018-07-12T18:50:55.687796: step 2152, loss 0.30986, acc 0.875\n",
      "2018-07-12T18:50:57.298469: step 2153, loss 0.306731, acc 0.84375\n",
      "2018-07-12T18:50:58.921284: step 2154, loss 0.27405, acc 0.796875\n",
      "2018-07-12T18:51:00.465251: step 2155, loss 0.396519, acc 0.828125\n",
      "2018-07-12T18:51:02.050172: step 2156, loss 0.357269, acc 0.875\n",
      "2018-07-12T18:51:03.682419: step 2157, loss 0.300916, acc 0.84375\n",
      "2018-07-12T18:51:05.288325: step 2158, loss 0.407415, acc 0.8125\n",
      "2018-07-12T18:51:06.948765: step 2159, loss 0.294871, acc 0.859375\n",
      "2018-07-12T18:51:08.579301: step 2160, loss 0.306155, acc 0.890625\n",
      "2018-07-12T18:51:10.235960: step 2161, loss 0.294484, acc 0.859375\n",
      "2018-07-12T18:51:11.871574: step 2162, loss 0.387953, acc 0.8125\n",
      "2018-07-12T18:51:13.493741: step 2163, loss 0.320381, acc 0.84375\n",
      "2018-07-12T18:51:15.113609: step 2164, loss 0.417469, acc 0.78125\n",
      "2018-07-12T18:51:16.833442: step 2165, loss 0.326796, acc 0.796875\n",
      "2018-07-12T18:51:18.430041: step 2166, loss 0.230508, acc 0.859375\n",
      "2018-07-12T18:51:20.036630: step 2167, loss 0.291997, acc 0.890625\n",
      "2018-07-12T18:51:21.711399: step 2168, loss 0.222028, acc 0.90625\n",
      "2018-07-12T18:51:23.338996: step 2169, loss 0.285796, acc 0.84375\n",
      "2018-07-12T18:51:24.981253: step 2170, loss 0.42443, acc 0.796875\n",
      "2018-07-12T18:51:26.614099: step 2171, loss 0.519941, acc 0.796875\n",
      "2018-07-12T18:51:28.235742: step 2172, loss 0.326927, acc 0.859375\n",
      "2018-07-12T18:51:29.840592: step 2173, loss 0.399218, acc 0.78125\n",
      "2018-07-12T18:51:31.466024: step 2174, loss 0.443416, acc 0.78125\n",
      "2018-07-12T18:51:33.100609: step 2175, loss 0.255709, acc 0.921875\n",
      "2018-07-12T18:51:34.700651: step 2176, loss 0.210761, acc 0.921875\n",
      "2018-07-12T18:51:36.341421: step 2177, loss 0.443875, acc 0.796875\n",
      "2018-07-12T18:51:37.968866: step 2178, loss 0.312763, acc 0.859375\n",
      "2018-07-12T18:51:39.645874: step 2179, loss 0.317338, acc 0.828125\n",
      "2018-07-12T18:51:41.280149: step 2180, loss 0.246242, acc 0.90625\n",
      "2018-07-12T18:51:42.890696: step 2181, loss 0.21047, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:51:44.549514: step 2182, loss 0.219582, acc 0.921875\n",
      "2018-07-12T18:51:46.131819: step 2183, loss 0.470299, acc 0.71875\n",
      "2018-07-12T18:51:47.831338: step 2184, loss 0.331062, acc 0.78125\n",
      "2018-07-12T18:51:49.498619: step 2185, loss 0.354613, acc 0.875\n",
      "2018-07-12T18:51:51.158316: step 2186, loss 0.264835, acc 0.90625\n",
      "2018-07-12T18:51:52.840577: step 2187, loss 0.268924, acc 0.90625\n",
      "2018-07-12T18:51:54.531026: step 2188, loss 0.409797, acc 0.78125\n",
      "2018-07-12T18:51:56.155589: step 2189, loss 0.35656, acc 0.84375\n",
      "2018-07-12T18:51:57.821742: step 2190, loss 0.290987, acc 0.859375\n",
      "2018-07-12T18:51:59.392932: step 2191, loss 0.368304, acc 0.890625\n",
      "2018-07-12T18:52:01.069397: step 2192, loss 0.338124, acc 0.859375\n",
      "2018-07-12T18:52:02.766766: step 2193, loss 0.316846, acc 0.828125\n",
      "2018-07-12T18:52:04.388906: step 2194, loss 0.370185, acc 0.8125\n",
      "2018-07-12T18:52:05.982204: step 2195, loss 0.308627, acc 0.84375\n",
      "2018-07-12T18:52:07.572130: step 2196, loss 0.379033, acc 0.8125\n",
      "2018-07-12T18:52:09.194215: step 2197, loss 0.332923, acc 0.8125\n",
      "2018-07-12T18:52:10.899550: step 2198, loss 0.206855, acc 0.9375\n",
      "2018-07-12T18:52:12.547580: step 2199, loss 0.387312, acc 0.84375\n",
      "2018-07-12T18:52:14.213186: step 2200, loss 0.338003, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:52:32.337015: step 2200, loss 0.328389, acc 0.8556\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2200\n",
      "\n",
      "2018-07-12T18:52:34.944669: step 2201, loss 0.269936, acc 0.890625\n",
      "2018-07-12T18:52:36.584740: step 2202, loss 0.335573, acc 0.859375\n",
      "2018-07-12T18:52:38.167607: step 2203, loss 0.275933, acc 0.921875\n",
      "2018-07-12T18:52:39.799524: step 2204, loss 0.247766, acc 0.90625\n",
      "2018-07-12T18:52:41.483184: step 2205, loss 0.243578, acc 0.90625\n",
      "2018-07-12T18:52:43.073577: step 2206, loss 0.365784, acc 0.84375\n",
      "2018-07-12T18:52:44.753688: step 2207, loss 0.358021, acc 0.875\n",
      "2018-07-12T18:52:46.401075: step 2208, loss 0.323197, acc 0.84375\n",
      "2018-07-12T18:52:48.047492: step 2209, loss 0.324257, acc 0.84375\n",
      "2018-07-12T18:52:49.632628: step 2210, loss 0.191187, acc 0.9375\n",
      "2018-07-12T18:52:51.288532: step 2211, loss 0.301074, acc 0.875\n",
      "2018-07-12T18:52:52.913988: step 2212, loss 0.285557, acc 0.859375\n",
      "2018-07-12T18:52:54.543994: step 2213, loss 0.532699, acc 0.796875\n",
      "2018-07-12T18:52:56.204589: step 2214, loss 0.222062, acc 0.9375\n",
      "2018-07-12T18:52:57.862936: step 2215, loss 0.250254, acc 0.90625\n",
      "2018-07-12T18:52:59.473781: step 2216, loss 0.258966, acc 0.890625\n",
      "2018-07-12T18:53:01.094044: step 2217, loss 0.222483, acc 0.90625\n",
      "2018-07-12T18:53:02.763673: step 2218, loss 0.280189, acc 0.859375\n",
      "2018-07-12T18:53:04.386677: step 2219, loss 0.262684, acc 0.875\n",
      "2018-07-12T18:53:06.032936: step 2220, loss 0.266528, acc 0.875\n",
      "2018-07-12T18:53:07.679895: step 2221, loss 0.262257, acc 0.890625\n",
      "2018-07-12T18:53:09.302089: step 2222, loss 0.341284, acc 0.78125\n",
      "2018-07-12T18:53:10.890244: step 2223, loss 0.33332, acc 0.84375\n",
      "2018-07-12T18:53:12.520328: step 2224, loss 0.244883, acc 0.890625\n",
      "2018-07-12T18:53:14.203819: step 2225, loss 0.265626, acc 0.875\n",
      "2018-07-12T18:53:15.855792: step 2226, loss 0.241212, acc 0.875\n",
      "2018-07-12T18:53:17.537312: step 2227, loss 0.359645, acc 0.84375\n",
      "2018-07-12T18:53:19.198343: step 2228, loss 0.373574, acc 0.828125\n",
      "2018-07-12T18:53:20.779705: step 2229, loss 0.328501, acc 0.90625\n",
      "2018-07-12T18:53:22.407070: step 2230, loss 0.320036, acc 0.890625\n",
      "2018-07-12T18:53:24.016477: step 2231, loss 0.286636, acc 0.890625\n",
      "2018-07-12T18:53:25.625754: step 2232, loss 0.470416, acc 0.734375\n",
      "2018-07-12T18:53:27.228014: step 2233, loss 0.432414, acc 0.796875\n",
      "2018-07-12T18:53:28.860943: step 2234, loss 0.213532, acc 0.90625\n",
      "2018-07-12T18:53:30.513513: step 2235, loss 0.247492, acc 0.953125\n",
      "2018-07-12T18:53:32.170407: step 2236, loss 0.292696, acc 0.84375\n",
      "2018-07-12T18:53:33.802516: step 2237, loss 0.353869, acc 0.90625\n",
      "2018-07-12T18:53:35.410643: step 2238, loss 0.376725, acc 0.84375\n",
      "2018-07-12T18:53:37.020775: step 2239, loss 0.332349, acc 0.8125\n",
      "2018-07-12T18:53:38.618351: step 2240, loss 0.434753, acc 0.796875\n",
      "2018-07-12T18:53:40.321050: step 2241, loss 0.227688, acc 0.890625\n",
      "2018-07-12T18:53:42.053197: step 2242, loss 0.242886, acc 0.890625\n",
      "2018-07-12T18:53:43.678681: step 2243, loss 0.416593, acc 0.8125\n",
      "2018-07-12T18:53:45.317635: step 2244, loss 0.253674, acc 0.90625\n",
      "2018-07-12T18:53:46.921592: step 2245, loss 0.329022, acc 0.84375\n",
      "2018-07-12T18:53:48.583296: step 2246, loss 0.271442, acc 0.90625\n",
      "2018-07-12T18:53:50.171919: step 2247, loss 0.400724, acc 0.828125\n",
      "2018-07-12T18:53:51.844081: step 2248, loss 0.37055, acc 0.84375\n",
      "2018-07-12T18:53:53.515276: step 2249, loss 0.426484, acc 0.828125\n",
      "2018-07-12T18:53:55.096537: step 2250, loss 0.325464, acc 0.8125\n",
      "2018-07-12T18:53:56.686562: step 2251, loss 0.366518, acc 0.84375\n",
      "2018-07-12T18:53:58.259975: step 2252, loss 0.356606, acc 0.84375\n",
      "2018-07-12T18:53:59.888386: step 2253, loss 0.359087, acc 0.828125\n",
      "2018-07-12T18:54:01.498814: step 2254, loss 0.247524, acc 0.90625\n",
      "2018-07-12T18:54:03.146098: step 2255, loss 0.266584, acc 0.90625\n",
      "2018-07-12T18:54:04.815490: step 2256, loss 0.191898, acc 0.921875\n",
      "2018-07-12T18:54:06.458712: step 2257, loss 0.38385, acc 0.875\n",
      "2018-07-12T18:54:08.145701: step 2258, loss 0.213214, acc 0.90625\n",
      "2018-07-12T18:54:09.750532: step 2259, loss 0.32817, acc 0.859375\n",
      "2018-07-12T18:54:11.354888: step 2260, loss 0.40453, acc 0.8125\n",
      "2018-07-12T18:54:13.006305: step 2261, loss 0.276969, acc 0.9375\n",
      "2018-07-12T18:54:14.639665: step 2262, loss 0.448297, acc 0.78125\n",
      "2018-07-12T18:54:16.244039: step 2263, loss 0.472147, acc 0.828125\n",
      "2018-07-12T18:54:17.876803: step 2264, loss 0.303697, acc 0.90625\n",
      "2018-07-12T18:54:19.596754: step 2265, loss 0.299142, acc 0.921875\n",
      "2018-07-12T18:54:21.280079: step 2266, loss 0.340973, acc 0.859375\n",
      "2018-07-12T18:54:23.004918: step 2267, loss 0.46983, acc 0.84375\n",
      "2018-07-12T18:54:24.670423: step 2268, loss 0.223525, acc 0.9375\n",
      "2018-07-12T18:54:26.221428: step 2269, loss 0.352645, acc 0.90625\n",
      "2018-07-12T18:54:27.798504: step 2270, loss 0.380214, acc 0.84375\n",
      "2018-07-12T18:54:29.457788: step 2271, loss 0.361405, acc 0.8125\n",
      "2018-07-12T18:54:31.143585: step 2272, loss 0.230161, acc 0.890625\n",
      "2018-07-12T18:54:32.819047: step 2273, loss 0.301058, acc 0.84375\n",
      "2018-07-12T18:54:34.425256: step 2274, loss 0.28799, acc 0.890625\n",
      "2018-07-12T18:54:35.979409: step 2275, loss 0.421223, acc 0.875\n",
      "2018-07-12T18:54:37.613182: step 2276, loss 0.309569, acc 0.90625\n",
      "2018-07-12T18:54:39.281138: step 2277, loss 0.377448, acc 0.8125\n",
      "2018-07-12T18:54:40.891693: step 2278, loss 0.307494, acc 0.875\n",
      "2018-07-12T18:54:42.480896: step 2279, loss 0.408358, acc 0.84375\n",
      "2018-07-12T18:54:44.082372: step 2280, loss 0.288329, acc 0.859375\n",
      "2018-07-12T18:54:45.682725: step 2281, loss 0.238951, acc 0.890625\n",
      "2018-07-12T18:54:47.299267: step 2282, loss 0.291533, acc 0.859375\n",
      "2018-07-12T18:54:48.869516: step 2283, loss 0.340378, acc 0.84375\n",
      "2018-07-12T18:54:50.468636: step 2284, loss 0.311229, acc 0.875\n",
      "2018-07-12T18:54:52.062989: step 2285, loss 0.22555, acc 0.890625\n",
      "2018-07-12T18:54:53.719760: step 2286, loss 0.267245, acc 0.9375\n",
      "2018-07-12T18:54:55.322943: step 2287, loss 0.38928, acc 0.796875\n",
      "2018-07-12T18:54:56.912363: step 2288, loss 0.347004, acc 0.828125\n",
      "2018-07-12T18:54:58.614103: step 2289, loss 0.205286, acc 0.96875\n",
      "2018-07-12T18:55:00.313302: step 2290, loss 0.223241, acc 0.921875\n",
      "2018-07-12T18:55:01.979279: step 2291, loss 0.431136, acc 0.8125\n",
      "2018-07-12T18:55:03.661197: step 2292, loss 0.282518, acc 0.890625\n",
      "2018-07-12T18:55:05.339478: step 2293, loss 0.342267, acc 0.859375\n",
      "2018-07-12T18:55:07.023420: step 2294, loss 0.336132, acc 0.84375\n",
      "2018-07-12T18:55:08.611645: step 2295, loss 0.234117, acc 0.875\n",
      "2018-07-12T18:55:10.242448: step 2296, loss 0.293951, acc 0.890625\n",
      "2018-07-12T18:55:11.856519: step 2297, loss 0.274598, acc 0.875\n",
      "2018-07-12T18:55:13.483756: step 2298, loss 0.291977, acc 0.859375\n",
      "2018-07-12T18:55:15.076432: step 2299, loss 0.242125, acc 0.875\n",
      "2018-07-12T18:55:16.649331: step 2300, loss 0.364607, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:55:34.814108: step 2300, loss 0.301151, acc 0.8728\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2300\n",
      "\n",
      "2018-07-12T18:55:37.327189: step 2301, loss 0.255989, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:55:38.969744: step 2302, loss 0.338978, acc 0.859375\n",
      "2018-07-12T18:55:40.604488: step 2303, loss 0.328056, acc 0.84375\n",
      "2018-07-12T18:55:42.297289: step 2304, loss 0.317694, acc 0.8125\n",
      "2018-07-12T18:55:43.992420: step 2305, loss 0.211537, acc 0.921875\n",
      "2018-07-12T18:55:45.611426: step 2306, loss 0.185974, acc 0.921875\n",
      "2018-07-12T18:55:47.253268: step 2307, loss 0.23107, acc 0.921875\n",
      "2018-07-12T18:55:48.888027: step 2308, loss 0.343204, acc 0.859375\n",
      "2018-07-12T18:55:50.580894: step 2309, loss 0.353321, acc 0.84375\n",
      "2018-07-12T18:55:52.182760: step 2310, loss 0.466275, acc 0.8125\n",
      "2018-07-12T18:55:53.837847: step 2311, loss 0.296066, acc 0.859375\n",
      "2018-07-12T18:55:55.458624: step 2312, loss 0.30361, acc 0.84375\n",
      "2018-07-12T18:55:57.067542: step 2313, loss 0.385581, acc 0.84375\n",
      "2018-07-12T18:55:58.695433: step 2314, loss 0.249584, acc 0.875\n",
      "2018-07-12T18:56:00.311661: step 2315, loss 0.317089, acc 0.796875\n",
      "2018-07-12T18:56:02.049186: step 2316, loss 0.253051, acc 0.890625\n",
      "2018-07-12T18:56:03.683377: step 2317, loss 0.245619, acc 0.90625\n",
      "2018-07-12T18:56:05.318754: step 2318, loss 0.41977, acc 0.828125\n",
      "2018-07-12T18:56:06.930189: step 2319, loss 0.29602, acc 0.90625\n",
      "2018-07-12T18:56:08.614343: step 2320, loss 0.330808, acc 0.875\n",
      "2018-07-12T18:56:10.257412: step 2321, loss 0.378979, acc 0.78125\n",
      "2018-07-12T18:56:11.907268: step 2322, loss 0.338477, acc 0.8125\n",
      "2018-07-12T18:56:13.511013: step 2323, loss 0.258686, acc 0.875\n",
      "2018-07-12T18:56:15.193819: step 2324, loss 0.226185, acc 0.90625\n",
      "2018-07-12T18:56:16.845721: step 2325, loss 0.309599, acc 0.875\n",
      "2018-07-12T18:56:18.489150: step 2326, loss 0.289493, acc 0.875\n",
      "2018-07-12T18:56:20.043922: step 2327, loss 0.392639, acc 0.796875\n",
      "2018-07-12T18:56:21.679937: step 2328, loss 0.213967, acc 0.90625\n",
      "2018-07-12T18:56:23.331175: step 2329, loss 0.289986, acc 0.90625\n",
      "2018-07-12T18:56:24.974549: step 2330, loss 0.270822, acc 0.921875\n",
      "2018-07-12T18:56:26.563182: step 2331, loss 0.45365, acc 0.84375\n",
      "2018-07-12T18:56:28.112469: step 2332, loss 0.31183, acc 0.859375\n",
      "2018-07-12T18:56:29.750506: step 2333, loss 0.304949, acc 0.890625\n",
      "2018-07-12T18:56:31.395132: step 2334, loss 0.323462, acc 0.828125\n",
      "2018-07-12T18:56:33.008033: step 2335, loss 0.179031, acc 0.921875\n",
      "2018-07-12T18:56:34.674495: step 2336, loss 0.386213, acc 0.8125\n",
      "2018-07-12T18:56:36.320249: step 2337, loss 0.212832, acc 0.9375\n",
      "2018-07-12T18:56:37.957544: step 2338, loss 0.184417, acc 0.921875\n",
      "2018-07-12T18:56:39.596888: step 2339, loss 0.356571, acc 0.875\n",
      "2018-07-12T18:56:41.299432: step 2340, loss 0.326145, acc 0.890625\n",
      "2018-07-12T18:56:42.916721: step 2341, loss 0.299369, acc 0.890625\n",
      "2018-07-12T18:56:44.530828: step 2342, loss 0.256856, acc 0.875\n",
      "2018-07-12T18:56:46.121090: step 2343, loss 0.400195, acc 0.828125\n",
      "2018-07-12T18:56:47.872545: step 2344, loss 0.29838, acc 0.875\n",
      "2018-07-12T18:56:49.537244: step 2345, loss 0.320986, acc 0.859375\n",
      "2018-07-12T18:56:51.180819: step 2346, loss 0.340096, acc 0.890625\n",
      "2018-07-12T18:56:52.766563: step 2347, loss 0.364306, acc 0.890625\n",
      "2018-07-12T18:56:54.365445: step 2348, loss 0.354145, acc 0.875\n",
      "2018-07-12T18:56:55.986089: step 2349, loss 0.187466, acc 0.9375\n",
      "2018-07-12T18:56:57.614130: step 2350, loss 0.244854, acc 0.890625\n",
      "2018-07-12T18:56:59.266912: step 2351, loss 0.323486, acc 0.875\n",
      "2018-07-12T18:57:00.816233: step 2352, loss 0.307122, acc 0.890625\n",
      "2018-07-12T18:57:02.408577: step 2353, loss 0.394765, acc 0.765625\n",
      "2018-07-12T18:57:04.118555: step 2354, loss 0.350105, acc 0.859375\n",
      "2018-07-12T18:57:05.754417: step 2355, loss 0.284796, acc 0.90625\n",
      "2018-07-12T18:57:07.391810: step 2356, loss 0.309609, acc 0.828125\n",
      "2018-07-12T18:57:09.001208: step 2357, loss 0.178308, acc 0.9375\n",
      "2018-07-12T18:57:10.604911: step 2358, loss 0.373156, acc 0.8125\n",
      "2018-07-12T18:57:12.270537: step 2359, loss 0.359621, acc 0.84375\n",
      "2018-07-12T18:57:13.888984: step 2360, loss 0.2956, acc 0.84375\n",
      "2018-07-12T18:57:15.575863: step 2361, loss 0.349413, acc 0.859375\n",
      "2018-07-12T18:57:17.217916: step 2362, loss 0.23833, acc 0.859375\n",
      "2018-07-12T18:57:18.872632: step 2363, loss 0.217971, acc 0.921875\n",
      "2018-07-12T18:57:20.444416: step 2364, loss 0.390911, acc 0.84375\n",
      "2018-07-12T18:57:22.151485: step 2365, loss 0.31932, acc 0.890625\n",
      "2018-07-12T18:57:23.780762: step 2366, loss 0.370437, acc 0.8125\n",
      "2018-07-12T18:57:25.385938: step 2367, loss 0.351993, acc 0.796875\n",
      "2018-07-12T18:57:27.091825: step 2368, loss 0.258319, acc 0.828125\n",
      "2018-07-12T18:57:28.713800: step 2369, loss 0.323656, acc 0.859375\n",
      "2018-07-12T18:57:30.362735: step 2370, loss 0.246868, acc 0.890625\n",
      "2018-07-12T18:57:31.942854: step 2371, loss 0.384558, acc 0.859375\n",
      "2018-07-12T18:57:33.556973: step 2372, loss 0.369208, acc 0.84375\n",
      "2018-07-12T18:57:35.143556: step 2373, loss 0.219559, acc 0.890625\n",
      "2018-07-12T18:57:36.797766: step 2374, loss 0.19112, acc 0.921875\n",
      "2018-07-12T18:57:38.370220: step 2375, loss 0.504934, acc 0.828125\n",
      "2018-07-12T18:57:39.952748: step 2376, loss 0.272462, acc 0.890625\n",
      "2018-07-12T18:57:41.587401: step 2377, loss 0.294027, acc 0.859375\n",
      "2018-07-12T18:57:43.235858: step 2378, loss 0.379699, acc 0.828125\n",
      "2018-07-12T18:57:44.937682: step 2379, loss 0.218248, acc 0.890625\n",
      "2018-07-12T18:57:46.642957: step 2380, loss 0.422125, acc 0.8125\n",
      "2018-07-12T18:57:48.221590: step 2381, loss 0.333794, acc 0.90625\n",
      "2018-07-12T18:57:49.830037: step 2382, loss 0.357623, acc 0.84375\n",
      "2018-07-12T18:57:51.423880: step 2383, loss 0.519191, acc 0.765625\n",
      "2018-07-12T18:57:53.019810: step 2384, loss 0.373439, acc 0.828125\n",
      "2018-07-12T18:57:54.626363: step 2385, loss 0.220521, acc 0.921875\n",
      "2018-07-12T18:57:56.177300: step 2386, loss 0.435172, acc 0.796875\n",
      "2018-07-12T18:57:57.802089: step 2387, loss 0.325834, acc 0.859375\n",
      "2018-07-12T18:57:59.441587: step 2388, loss 0.265333, acc 0.875\n",
      "2018-07-12T18:58:01.107141: step 2389, loss 0.281407, acc 0.90625\n",
      "2018-07-12T18:58:02.694822: step 2390, loss 0.186582, acc 0.921875\n",
      "2018-07-12T18:58:04.321299: step 2391, loss 0.284319, acc 0.796875\n",
      "2018-07-12T18:58:05.903503: step 2392, loss 0.323278, acc 0.859375\n",
      "2018-07-12T18:58:07.551855: step 2393, loss 0.333975, acc 0.875\n",
      "2018-07-12T18:58:09.157663: step 2394, loss 0.240433, acc 0.90625\n",
      "2018-07-12T18:58:10.898085: step 2395, loss 0.633795, acc 0.75\n",
      "2018-07-12T18:58:12.537414: step 2396, loss 0.328394, acc 0.8125\n",
      "2018-07-12T18:58:14.103968: step 2397, loss 0.35405, acc 0.8125\n",
      "2018-07-12T18:58:15.712627: step 2398, loss 0.277268, acc 0.90625\n",
      "2018-07-12T18:58:17.357944: step 2399, loss 0.541222, acc 0.75\n",
      "2018-07-12T18:58:18.996120: step 2400, loss 0.23878, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T18:58:36.988265: step 2400, loss 0.294194, acc 0.8736\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2400\n",
      "\n",
      "2018-07-12T18:58:39.568353: step 2401, loss 0.280744, acc 0.890625\n",
      "2018-07-12T18:58:41.212004: step 2402, loss 0.212717, acc 0.875\n",
      "2018-07-12T18:58:42.897537: step 2403, loss 0.285707, acc 0.875\n",
      "2018-07-12T18:58:44.508212: step 2404, loss 0.511327, acc 0.765625\n",
      "2018-07-12T18:58:46.196376: step 2405, loss 0.271046, acc 0.859375\n",
      "2018-07-12T18:58:47.836733: step 2406, loss 0.325875, acc 0.84375\n",
      "2018-07-12T18:58:49.464167: step 2407, loss 0.348231, acc 0.84375\n",
      "2018-07-12T18:58:51.042420: step 2408, loss 0.295886, acc 0.859375\n",
      "2018-07-12T18:58:52.644399: step 2409, loss 0.390632, acc 0.828125\n",
      "2018-07-12T18:58:54.246589: step 2410, loss 0.460643, acc 0.84375\n",
      "2018-07-12T18:58:55.845206: step 2411, loss 0.389598, acc 0.8125\n",
      "2018-07-12T18:58:57.542623: step 2412, loss 0.192289, acc 0.9375\n",
      "2018-07-12T18:58:59.174640: step 2413, loss 0.243879, acc 0.875\n",
      "2018-07-12T18:59:00.776002: step 2414, loss 0.387021, acc 0.8125\n",
      "2018-07-12T18:59:02.345897: step 2415, loss 0.292419, acc 0.859375\n",
      "2018-07-12T18:59:03.972847: step 2416, loss 0.265276, acc 0.875\n",
      "2018-07-12T18:59:05.660453: step 2417, loss 0.392186, acc 0.84375\n",
      "2018-07-12T18:59:07.300574: step 2418, loss 0.251307, acc 0.890625\n",
      "2018-07-12T18:59:08.823965: step 2419, loss 0.269796, acc 0.890625\n",
      "2018-07-12T18:59:10.422862: step 2420, loss 0.270174, acc 0.890625\n",
      "2018-07-12T18:59:12.069695: step 2421, loss 0.28227, acc 0.859375\n",
      "2018-07-12T18:59:13.732267: step 2422, loss 0.342528, acc 0.859375\n",
      "2018-07-12T18:59:15.388693: step 2423, loss 0.298348, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T18:59:17.000158: step 2424, loss 0.443159, acc 0.796875\n",
      "2018-07-12T18:59:18.591246: step 2425, loss 0.334894, acc 0.875\n",
      "2018-07-12T18:59:20.244553: step 2426, loss 0.335345, acc 0.828125\n",
      "2018-07-12T18:59:21.982865: step 2427, loss 0.149275, acc 0.9375\n",
      "2018-07-12T18:59:23.751261: step 2428, loss 0.299804, acc 0.875\n",
      "2018-07-12T18:59:25.363573: step 2429, loss 0.252349, acc 0.875\n",
      "2018-07-12T18:59:27.015072: step 2430, loss 0.187901, acc 0.96875\n",
      "2018-07-12T18:59:28.688308: step 2431, loss 0.307427, acc 0.84375\n",
      "2018-07-12T18:59:30.348307: step 2432, loss 0.424112, acc 0.84375\n",
      "2018-07-12T18:59:31.958298: step 2433, loss 0.283484, acc 0.921875\n",
      "2018-07-12T18:59:33.588284: step 2434, loss 0.394125, acc 0.8125\n",
      "2018-07-12T18:59:35.261682: step 2435, loss 0.257076, acc 0.875\n",
      "2018-07-12T18:59:36.888212: step 2436, loss 0.505072, acc 0.828125\n",
      "2018-07-12T18:59:38.519025: step 2437, loss 0.298699, acc 0.921875\n",
      "2018-07-12T18:59:40.161943: step 2438, loss 0.341092, acc 0.859375\n",
      "2018-07-12T18:59:41.806285: step 2439, loss 0.175701, acc 0.96875\n",
      "2018-07-12T18:59:43.505490: step 2440, loss 0.312133, acc 0.84375\n",
      "2018-07-12T18:59:45.156143: step 2441, loss 0.298139, acc 0.859375\n",
      "2018-07-12T18:59:46.759304: step 2442, loss 0.296696, acc 0.859375\n",
      "2018-07-12T18:59:48.387517: step 2443, loss 0.256004, acc 0.921875\n",
      "2018-07-12T18:59:50.124796: step 2444, loss 0.245829, acc 0.9375\n",
      "2018-07-12T18:59:51.832502: step 2445, loss 0.15325, acc 0.96875\n",
      "2018-07-12T18:59:53.414493: step 2446, loss 0.281413, acc 0.890625\n",
      "2018-07-12T18:59:55.079976: step 2447, loss 0.234356, acc 0.90625\n",
      "2018-07-12T18:59:56.766835: step 2448, loss 0.372532, acc 0.875\n",
      "2018-07-12T18:59:58.350538: step 2449, loss 0.384709, acc 0.828125\n",
      "2018-07-12T18:59:59.940723: step 2450, loss 0.156128, acc 0.921875\n",
      "2018-07-12T19:00:01.530001: step 2451, loss 0.343242, acc 0.875\n",
      "2018-07-12T19:00:03.187693: step 2452, loss 0.191276, acc 0.921875\n",
      "2018-07-12T19:00:04.805077: step 2453, loss 0.37599, acc 0.84375\n",
      "2018-07-12T19:00:06.435194: step 2454, loss 0.352674, acc 0.84375\n",
      "2018-07-12T19:00:08.009487: step 2455, loss 0.281037, acc 0.875\n",
      "2018-07-12T19:00:09.639484: step 2456, loss 0.519139, acc 0.78125\n",
      "2018-07-12T19:00:11.280508: step 2457, loss 0.178411, acc 0.921875\n",
      "2018-07-12T19:00:12.949994: step 2458, loss 0.481412, acc 0.796875\n",
      "2018-07-12T19:00:14.557856: step 2459, loss 0.272943, acc 0.890625\n",
      "2018-07-12T19:00:16.221949: step 2460, loss 0.369478, acc 0.84375\n",
      "2018-07-12T19:00:17.930305: step 2461, loss 0.441237, acc 0.84375\n",
      "2018-07-12T19:00:19.577739: step 2462, loss 0.21995, acc 0.921875\n",
      "2018-07-12T19:00:21.172208: step 2463, loss 0.375063, acc 0.796875\n",
      "2018-07-12T19:00:22.259483: step 2464, loss 0.445907, acc 0.833333\n",
      "2018-07-12T19:00:23.874571: step 2465, loss 0.262854, acc 0.921875\n",
      "2018-07-12T19:00:25.455812: step 2466, loss 0.264122, acc 0.90625\n",
      "2018-07-12T19:00:27.056178: step 2467, loss 0.277847, acc 0.90625\n",
      "2018-07-12T19:00:28.685980: step 2468, loss 0.194669, acc 0.9375\n",
      "2018-07-12T19:00:30.327950: step 2469, loss 0.404276, acc 0.8125\n",
      "2018-07-12T19:00:31.888362: step 2470, loss 0.181084, acc 0.921875\n",
      "2018-07-12T19:00:33.503215: step 2471, loss 0.244722, acc 0.90625\n",
      "2018-07-12T19:00:35.163690: step 2472, loss 0.302886, acc 0.90625\n",
      "2018-07-12T19:00:36.841981: step 2473, loss 0.172789, acc 0.9375\n",
      "2018-07-12T19:00:38.479270: step 2474, loss 0.216039, acc 0.90625\n",
      "2018-07-12T19:00:40.172723: step 2475, loss 0.271505, acc 0.84375\n",
      "2018-07-12T19:00:41.859787: step 2476, loss 0.283839, acc 0.890625\n",
      "2018-07-12T19:00:43.559165: step 2477, loss 0.257507, acc 0.84375\n",
      "2018-07-12T19:00:45.183873: step 2478, loss 0.32046, acc 0.859375\n",
      "2018-07-12T19:00:46.842087: step 2479, loss 0.299378, acc 0.9375\n",
      "2018-07-12T19:00:48.440982: step 2480, loss 0.253446, acc 0.90625\n",
      "2018-07-12T19:00:50.096868: step 2481, loss 0.289327, acc 0.875\n",
      "2018-07-12T19:00:51.762740: step 2482, loss 0.215286, acc 0.9375\n",
      "2018-07-12T19:00:53.360444: step 2483, loss 0.261464, acc 0.859375\n",
      "2018-07-12T19:00:54.932323: step 2484, loss 0.285172, acc 0.875\n",
      "2018-07-12T19:00:56.559853: step 2485, loss 0.190608, acc 0.890625\n",
      "2018-07-12T19:00:58.221259: step 2486, loss 0.318723, acc 0.859375\n",
      "2018-07-12T19:00:59.810763: step 2487, loss 0.324268, acc 0.859375\n",
      "2018-07-12T19:01:01.461750: step 2488, loss 0.270797, acc 0.828125\n",
      "2018-07-12T19:01:03.114599: step 2489, loss 0.415191, acc 0.828125\n",
      "2018-07-12T19:01:04.732119: step 2490, loss 0.264936, acc 0.890625\n",
      "2018-07-12T19:01:06.334652: step 2491, loss 0.23696, acc 0.9375\n",
      "2018-07-12T19:01:07.911349: step 2492, loss 0.299696, acc 0.875\n",
      "2018-07-12T19:01:09.556801: step 2493, loss 0.276175, acc 0.859375\n",
      "2018-07-12T19:01:11.229419: step 2494, loss 0.299575, acc 0.859375\n",
      "2018-07-12T19:01:12.812816: step 2495, loss 0.294012, acc 0.921875\n",
      "2018-07-12T19:01:14.440982: step 2496, loss 0.230601, acc 0.859375\n",
      "2018-07-12T19:01:16.066886: step 2497, loss 0.23341, acc 0.859375\n",
      "2018-07-12T19:01:17.729373: step 2498, loss 0.296296, acc 0.84375\n",
      "2018-07-12T19:01:19.345275: step 2499, loss 0.201722, acc 0.875\n",
      "2018-07-12T19:01:20.944345: step 2500, loss 0.219329, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:01:39.061616: step 2500, loss 0.290347, acc 0.8744\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2500\n",
      "\n",
      "2018-07-12T19:01:41.649386: step 2501, loss 0.39519, acc 0.796875\n",
      "2018-07-12T19:01:43.241144: step 2502, loss 0.229305, acc 0.890625\n",
      "2018-07-12T19:01:44.864512: step 2503, loss 0.177935, acc 0.953125\n",
      "2018-07-12T19:01:46.505433: step 2504, loss 0.378679, acc 0.796875\n",
      "2018-07-12T19:01:48.123273: step 2505, loss 0.432502, acc 0.828125\n",
      "2018-07-12T19:01:49.715769: step 2506, loss 0.350296, acc 0.828125\n",
      "2018-07-12T19:01:51.311341: step 2507, loss 0.254783, acc 0.90625\n",
      "2018-07-12T19:01:52.945773: step 2508, loss 0.268499, acc 0.875\n",
      "2018-07-12T19:01:54.549807: step 2509, loss 0.177954, acc 0.9375\n",
      "2018-07-12T19:01:56.138079: step 2510, loss 0.195039, acc 0.9375\n",
      "2018-07-12T19:01:57.741210: step 2511, loss 0.309807, acc 0.828125\n",
      "2018-07-12T19:01:59.460569: step 2512, loss 0.257729, acc 0.921875\n",
      "2018-07-12T19:02:01.112792: step 2513, loss 0.360359, acc 0.875\n",
      "2018-07-12T19:02:02.754069: step 2514, loss 0.295826, acc 0.84375\n",
      "2018-07-12T19:02:04.374026: step 2515, loss 0.394302, acc 0.875\n",
      "2018-07-12T19:02:06.010414: step 2516, loss 0.325911, acc 0.875\n",
      "2018-07-12T19:02:07.595341: step 2517, loss 0.245762, acc 0.921875\n",
      "2018-07-12T19:02:09.188455: step 2518, loss 0.38011, acc 0.875\n",
      "2018-07-12T19:02:10.747385: step 2519, loss 0.240994, acc 0.859375\n",
      "2018-07-12T19:02:12.350657: step 2520, loss 0.231987, acc 0.890625\n",
      "2018-07-12T19:02:13.964799: step 2521, loss 0.19263, acc 0.9375\n",
      "2018-07-12T19:02:15.606023: step 2522, loss 0.167929, acc 0.953125\n",
      "2018-07-12T19:02:17.200367: step 2523, loss 0.312077, acc 0.859375\n",
      "2018-07-12T19:02:18.737917: step 2524, loss 0.399777, acc 0.828125\n",
      "2018-07-12T19:02:20.437872: step 2525, loss 0.315478, acc 0.875\n",
      "2018-07-12T19:02:22.038898: step 2526, loss 0.181925, acc 0.9375\n",
      "2018-07-12T19:02:23.726905: step 2527, loss 0.374237, acc 0.828125\n",
      "2018-07-12T19:02:25.389469: step 2528, loss 0.230248, acc 0.875\n",
      "2018-07-12T19:02:27.057827: step 2529, loss 0.242389, acc 0.875\n",
      "2018-07-12T19:02:28.707487: step 2530, loss 0.338087, acc 0.84375\n",
      "2018-07-12T19:02:30.356226: step 2531, loss 0.256949, acc 0.875\n",
      "2018-07-12T19:02:31.978288: step 2532, loss 0.377281, acc 0.859375\n",
      "2018-07-12T19:02:33.536940: step 2533, loss 0.330485, acc 0.828125\n",
      "2018-07-12T19:02:35.250494: step 2534, loss 0.260545, acc 0.859375\n",
      "2018-07-12T19:02:36.874756: step 2535, loss 0.42219, acc 0.8125\n",
      "2018-07-12T19:02:38.547270: step 2536, loss 0.275534, acc 0.875\n",
      "2018-07-12T19:02:40.186349: step 2537, loss 0.247433, acc 0.875\n",
      "2018-07-12T19:02:41.754293: step 2538, loss 0.223769, acc 0.953125\n",
      "2018-07-12T19:02:43.296934: step 2539, loss 0.161156, acc 0.9375\n",
      "2018-07-12T19:02:44.944653: step 2540, loss 0.259445, acc 0.90625\n",
      "2018-07-12T19:02:46.627764: step 2541, loss 0.236893, acc 0.890625\n",
      "2018-07-12T19:02:48.276530: step 2542, loss 0.331858, acc 0.875\n",
      "2018-07-12T19:02:49.902511: step 2543, loss 0.158202, acc 0.9375\n",
      "2018-07-12T19:02:51.519023: step 2544, loss 0.378098, acc 0.8125\n",
      "2018-07-12T19:02:53.167494: step 2545, loss 0.316909, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:02:54.821681: step 2546, loss 0.264694, acc 0.890625\n",
      "2018-07-12T19:02:56.410736: step 2547, loss 0.216782, acc 0.90625\n",
      "2018-07-12T19:02:58.064563: step 2548, loss 0.386099, acc 0.875\n",
      "2018-07-12T19:02:59.677476: step 2549, loss 0.341095, acc 0.90625\n",
      "2018-07-12T19:03:01.250156: step 2550, loss 0.365154, acc 0.84375\n",
      "2018-07-12T19:03:02.979724: step 2551, loss 0.194582, acc 0.921875\n",
      "2018-07-12T19:03:04.561276: step 2552, loss 0.137251, acc 0.953125\n",
      "2018-07-12T19:03:06.199233: step 2553, loss 0.158963, acc 0.9375\n",
      "2018-07-12T19:03:07.803782: step 2554, loss 0.14118, acc 0.9375\n",
      "2018-07-12T19:03:09.439674: step 2555, loss 0.276963, acc 0.875\n",
      "2018-07-12T19:03:11.151889: step 2556, loss 0.214936, acc 0.90625\n",
      "2018-07-12T19:03:12.791289: step 2557, loss 0.282264, acc 0.859375\n",
      "2018-07-12T19:03:14.393096: step 2558, loss 0.234316, acc 0.921875\n",
      "2018-07-12T19:03:16.040941: step 2559, loss 0.184305, acc 0.921875\n",
      "2018-07-12T19:03:17.698880: step 2560, loss 0.333858, acc 0.875\n",
      "2018-07-12T19:03:19.378629: step 2561, loss 0.294787, acc 0.828125\n",
      "2018-07-12T19:03:21.029231: step 2562, loss 0.232859, acc 0.875\n",
      "2018-07-12T19:03:22.641356: step 2563, loss 0.288543, acc 0.875\n",
      "2018-07-12T19:03:24.286910: step 2564, loss 0.297669, acc 0.875\n",
      "2018-07-12T19:03:25.965427: step 2565, loss 0.218913, acc 0.890625\n",
      "2018-07-12T19:03:27.670692: step 2566, loss 0.192819, acc 0.9375\n",
      "2018-07-12T19:03:29.347310: step 2567, loss 0.203664, acc 0.90625\n",
      "2018-07-12T19:03:31.018679: step 2568, loss 0.184479, acc 0.921875\n",
      "2018-07-12T19:03:32.659318: step 2569, loss 0.191556, acc 0.90625\n",
      "2018-07-12T19:03:34.308538: step 2570, loss 0.278461, acc 0.875\n",
      "2018-07-12T19:03:35.979420: step 2571, loss 0.232201, acc 0.90625\n",
      "2018-07-12T19:03:37.653665: step 2572, loss 0.360749, acc 0.890625\n",
      "2018-07-12T19:03:39.325166: step 2573, loss 0.215762, acc 0.90625\n",
      "2018-07-12T19:03:40.964018: step 2574, loss 0.135406, acc 0.953125\n",
      "2018-07-12T19:03:42.687918: step 2575, loss 0.180618, acc 0.953125\n",
      "2018-07-12T19:03:44.331608: step 2576, loss 0.123736, acc 0.921875\n",
      "2018-07-12T19:03:45.983971: step 2577, loss 0.235132, acc 0.921875\n",
      "2018-07-12T19:03:47.632628: step 2578, loss 0.219155, acc 0.890625\n",
      "2018-07-12T19:03:49.311979: step 2579, loss 0.400117, acc 0.84375\n",
      "2018-07-12T19:03:50.991099: step 2580, loss 0.277416, acc 0.875\n",
      "2018-07-12T19:03:52.634276: step 2581, loss 0.23181, acc 0.90625\n",
      "2018-07-12T19:03:54.255059: step 2582, loss 0.244892, acc 0.890625\n",
      "2018-07-12T19:03:55.911198: step 2583, loss 0.209499, acc 0.921875\n",
      "2018-07-12T19:03:57.510321: step 2584, loss 0.241524, acc 0.890625\n",
      "2018-07-12T19:03:59.162842: step 2585, loss 0.435356, acc 0.890625\n",
      "2018-07-12T19:04:00.811257: step 2586, loss 0.175225, acc 0.96875\n",
      "2018-07-12T19:04:02.437605: step 2587, loss 0.205268, acc 0.90625\n",
      "2018-07-12T19:04:04.112311: step 2588, loss 0.221123, acc 0.921875\n",
      "2018-07-12T19:04:05.701491: step 2589, loss 0.188821, acc 0.9375\n",
      "2018-07-12T19:04:07.370504: step 2590, loss 0.210556, acc 0.90625\n",
      "2018-07-12T19:04:08.986288: step 2591, loss 0.18922, acc 0.90625\n",
      "2018-07-12T19:04:10.703922: step 2592, loss 0.471568, acc 0.78125\n",
      "2018-07-12T19:04:12.321538: step 2593, loss 0.280597, acc 0.90625\n",
      "2018-07-12T19:04:13.928048: step 2594, loss 0.293403, acc 0.90625\n",
      "2018-07-12T19:04:15.528710: step 2595, loss 0.304215, acc 0.890625\n",
      "2018-07-12T19:04:17.111924: step 2596, loss 0.140813, acc 0.9375\n",
      "2018-07-12T19:04:18.669885: step 2597, loss 0.200562, acc 0.921875\n",
      "2018-07-12T19:04:20.254041: step 2598, loss 0.43937, acc 0.828125\n",
      "2018-07-12T19:04:21.836488: step 2599, loss 0.18591, acc 0.921875\n",
      "2018-07-12T19:04:23.510592: step 2600, loss 0.24303, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:04:41.319608: step 2600, loss 0.303508, acc 0.8708\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2600\n",
      "\n",
      "2018-07-12T19:04:43.928651: step 2601, loss 0.338244, acc 0.875\n",
      "2018-07-12T19:04:45.577743: step 2602, loss 0.130778, acc 0.96875\n",
      "2018-07-12T19:04:47.173165: step 2603, loss 0.397048, acc 0.859375\n",
      "2018-07-12T19:04:48.795907: step 2604, loss 0.2723, acc 0.890625\n",
      "2018-07-12T19:04:50.458030: step 2605, loss 0.19359, acc 0.90625\n",
      "2018-07-12T19:04:52.059791: step 2606, loss 0.284403, acc 0.890625\n",
      "2018-07-12T19:04:53.709976: step 2607, loss 0.187338, acc 0.9375\n",
      "2018-07-12T19:04:55.303393: step 2608, loss 0.229713, acc 0.90625\n",
      "2018-07-12T19:04:56.958274: step 2609, loss 0.248831, acc 0.890625\n",
      "2018-07-12T19:04:58.572306: step 2610, loss 0.249558, acc 0.859375\n",
      "2018-07-12T19:05:00.234639: step 2611, loss 0.291054, acc 0.859375\n",
      "2018-07-12T19:05:01.812445: step 2612, loss 0.358426, acc 0.828125\n",
      "2018-07-12T19:05:03.427692: step 2613, loss 0.142992, acc 0.953125\n",
      "2018-07-12T19:05:05.059360: step 2614, loss 0.448601, acc 0.8125\n",
      "2018-07-12T19:05:06.706304: step 2615, loss 0.247755, acc 0.921875\n",
      "2018-07-12T19:05:08.401482: step 2616, loss 0.346491, acc 0.84375\n",
      "2018-07-12T19:05:10.042208: step 2617, loss 0.318414, acc 0.859375\n",
      "2018-07-12T19:05:11.673089: step 2618, loss 0.325827, acc 0.890625\n",
      "2018-07-12T19:05:13.281490: step 2619, loss 0.256285, acc 0.875\n",
      "2018-07-12T19:05:14.881511: step 2620, loss 0.204479, acc 0.890625\n",
      "2018-07-12T19:05:16.530409: step 2621, loss 0.283143, acc 0.8125\n",
      "2018-07-12T19:05:18.173567: step 2622, loss 0.374533, acc 0.890625\n",
      "2018-07-12T19:05:19.790190: step 2623, loss 0.281397, acc 0.890625\n",
      "2018-07-12T19:05:21.483917: step 2624, loss 0.239202, acc 0.859375\n",
      "2018-07-12T19:05:23.102909: step 2625, loss 0.24573, acc 0.90625\n",
      "2018-07-12T19:05:24.759843: step 2626, loss 0.293983, acc 0.890625\n",
      "2018-07-12T19:05:26.397598: step 2627, loss 0.190834, acc 0.921875\n",
      "2018-07-12T19:05:28.017750: step 2628, loss 0.206584, acc 0.90625\n",
      "2018-07-12T19:05:29.660577: step 2629, loss 0.28613, acc 0.859375\n",
      "2018-07-12T19:05:31.315900: step 2630, loss 0.248907, acc 0.890625\n",
      "2018-07-12T19:05:32.952682: step 2631, loss 0.185096, acc 0.953125\n",
      "2018-07-12T19:05:34.556815: step 2632, loss 0.246148, acc 0.9375\n",
      "2018-07-12T19:05:36.211997: step 2633, loss 0.314652, acc 0.890625\n",
      "2018-07-12T19:05:37.825682: step 2634, loss 0.262338, acc 0.875\n",
      "2018-07-12T19:05:39.437366: step 2635, loss 0.251243, acc 0.84375\n",
      "2018-07-12T19:05:41.042681: step 2636, loss 0.257209, acc 0.890625\n",
      "2018-07-12T19:05:42.643784: step 2637, loss 0.240344, acc 0.9375\n",
      "2018-07-12T19:05:44.228157: step 2638, loss 0.196135, acc 0.875\n",
      "2018-07-12T19:05:45.795908: step 2639, loss 0.188282, acc 0.921875\n",
      "2018-07-12T19:05:47.415294: step 2640, loss 0.261471, acc 0.90625\n",
      "2018-07-12T19:05:49.040105: step 2641, loss 0.16973, acc 0.9375\n",
      "2018-07-12T19:05:50.670994: step 2642, loss 0.18921, acc 0.9375\n",
      "2018-07-12T19:05:52.322235: step 2643, loss 0.221584, acc 0.921875\n",
      "2018-07-12T19:05:53.920191: step 2644, loss 0.305052, acc 0.890625\n",
      "2018-07-12T19:05:55.525363: step 2645, loss 0.263342, acc 0.859375\n",
      "2018-07-12T19:05:57.161369: step 2646, loss 0.281016, acc 0.875\n",
      "2018-07-12T19:05:58.756750: step 2647, loss 0.32499, acc 0.890625\n",
      "2018-07-12T19:06:00.410627: step 2648, loss 0.273142, acc 0.90625\n",
      "2018-07-12T19:06:02.000819: step 2649, loss 0.245716, acc 0.875\n",
      "2018-07-12T19:06:03.626444: step 2650, loss 0.164424, acc 0.90625\n",
      "2018-07-12T19:06:05.229322: step 2651, loss 0.211928, acc 0.9375\n",
      "2018-07-12T19:06:06.815641: step 2652, loss 0.147053, acc 0.953125\n",
      "2018-07-12T19:06:08.473115: step 2653, loss 0.266738, acc 0.875\n",
      "2018-07-12T19:06:10.030129: step 2654, loss 0.363549, acc 0.875\n",
      "2018-07-12T19:06:11.615170: step 2655, loss 0.197254, acc 0.90625\n",
      "2018-07-12T19:06:13.293702: step 2656, loss 0.111497, acc 0.984375\n",
      "2018-07-12T19:06:14.892144: step 2657, loss 0.304012, acc 0.890625\n",
      "2018-07-12T19:06:16.451198: step 2658, loss 0.266048, acc 0.84375\n",
      "2018-07-12T19:06:18.095081: step 2659, loss 0.332517, acc 0.828125\n",
      "2018-07-12T19:06:19.697204: step 2660, loss 0.396079, acc 0.875\n",
      "2018-07-12T19:06:21.360084: step 2661, loss 0.155363, acc 0.9375\n",
      "2018-07-12T19:06:22.951448: step 2662, loss 0.205402, acc 0.890625\n",
      "2018-07-12T19:06:24.604188: step 2663, loss 0.382846, acc 0.859375\n",
      "2018-07-12T19:06:26.242935: step 2664, loss 0.319179, acc 0.859375\n",
      "2018-07-12T19:06:27.914472: step 2665, loss 0.171941, acc 0.9375\n",
      "2018-07-12T19:06:29.509065: step 2666, loss 0.273143, acc 0.859375\n",
      "2018-07-12T19:06:31.144129: step 2667, loss 0.228514, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:06:32.798621: step 2668, loss 0.225659, acc 0.921875\n",
      "2018-07-12T19:06:34.464116: step 2669, loss 0.255692, acc 0.890625\n",
      "2018-07-12T19:06:36.070920: step 2670, loss 0.24454, acc 0.90625\n",
      "2018-07-12T19:06:37.730268: step 2671, loss 0.187825, acc 0.890625\n",
      "2018-07-12T19:06:39.381022: step 2672, loss 0.321117, acc 0.875\n",
      "2018-07-12T19:06:41.035068: step 2673, loss 0.30771, acc 0.875\n",
      "2018-07-12T19:06:42.611935: step 2674, loss 0.231373, acc 0.921875\n",
      "2018-07-12T19:06:44.238265: step 2675, loss 0.274473, acc 0.921875\n",
      "2018-07-12T19:06:45.874401: step 2676, loss 0.325181, acc 0.875\n",
      "2018-07-12T19:06:47.547506: step 2677, loss 0.136583, acc 0.921875\n",
      "2018-07-12T19:06:49.107262: step 2678, loss 0.198249, acc 0.90625\n",
      "2018-07-12T19:06:50.744592: step 2679, loss 0.391777, acc 0.828125\n",
      "2018-07-12T19:06:52.403484: step 2680, loss 0.281163, acc 0.875\n",
      "2018-07-12T19:06:54.030811: step 2681, loss 0.328242, acc 0.859375\n",
      "2018-07-12T19:06:55.638198: step 2682, loss 0.251094, acc 0.90625\n",
      "2018-07-12T19:06:57.342022: step 2683, loss 0.291677, acc 0.890625\n",
      "2018-07-12T19:06:58.952222: step 2684, loss 0.212028, acc 0.9375\n",
      "2018-07-12T19:07:00.614960: step 2685, loss 0.294848, acc 0.84375\n",
      "2018-07-12T19:07:02.190477: step 2686, loss 0.347865, acc 0.78125\n",
      "2018-07-12T19:07:03.767061: step 2687, loss 0.275415, acc 0.828125\n",
      "2018-07-12T19:07:05.490292: step 2688, loss 0.28838, acc 0.875\n",
      "2018-07-12T19:07:07.086721: step 2689, loss 0.212651, acc 0.875\n",
      "2018-07-12T19:07:08.730308: step 2690, loss 0.336812, acc 0.890625\n",
      "2018-07-12T19:07:10.389486: step 2691, loss 0.32522, acc 0.890625\n",
      "2018-07-12T19:07:12.037948: step 2692, loss 0.294606, acc 0.890625\n",
      "2018-07-12T19:07:13.670790: step 2693, loss 0.245097, acc 0.90625\n",
      "2018-07-12T19:07:15.276354: step 2694, loss 0.246461, acc 0.890625\n",
      "2018-07-12T19:07:16.930188: step 2695, loss 0.55815, acc 0.75\n",
      "2018-07-12T19:07:18.502110: step 2696, loss 0.223422, acc 0.890625\n",
      "2018-07-12T19:07:20.113594: step 2697, loss 0.230033, acc 0.90625\n",
      "2018-07-12T19:07:21.696016: step 2698, loss 0.116195, acc 0.953125\n",
      "2018-07-12T19:07:23.305673: step 2699, loss 0.2726, acc 0.859375\n",
      "2018-07-12T19:07:25.067390: step 2700, loss 0.238912, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:07:43.016640: step 2700, loss 0.306991, acc 0.8708\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2700\n",
      "\n",
      "2018-07-12T19:07:45.466603: step 2701, loss 0.223568, acc 0.921875\n",
      "2018-07-12T19:07:47.148437: step 2702, loss 0.251562, acc 0.890625\n",
      "2018-07-12T19:07:48.801561: step 2703, loss 0.318672, acc 0.859375\n",
      "2018-07-12T19:07:50.466292: step 2704, loss 0.21242, acc 0.9375\n",
      "2018-07-12T19:07:52.129925: step 2705, loss 0.268276, acc 0.859375\n",
      "2018-07-12T19:07:53.723850: step 2706, loss 0.214955, acc 0.890625\n",
      "2018-07-12T19:07:55.297613: step 2707, loss 0.265045, acc 0.90625\n",
      "2018-07-12T19:07:56.915869: step 2708, loss 0.281116, acc 0.875\n",
      "2018-07-12T19:07:58.523304: step 2709, loss 0.248464, acc 0.84375\n",
      "2018-07-12T19:08:00.130105: step 2710, loss 0.203076, acc 0.9375\n",
      "2018-07-12T19:08:01.790925: step 2711, loss 0.308746, acc 0.875\n",
      "2018-07-12T19:08:03.424353: step 2712, loss 0.327493, acc 0.875\n",
      "2018-07-12T19:08:05.107497: step 2713, loss 0.302621, acc 0.875\n",
      "2018-07-12T19:08:06.727043: step 2714, loss 0.292226, acc 0.890625\n",
      "2018-07-12T19:08:08.370236: step 2715, loss 0.267121, acc 0.84375\n",
      "2018-07-12T19:08:09.947281: step 2716, loss 0.319915, acc 0.859375\n",
      "2018-07-12T19:08:11.564635: step 2717, loss 0.300066, acc 0.890625\n",
      "2018-07-12T19:08:13.146885: step 2718, loss 0.342206, acc 0.90625\n",
      "2018-07-12T19:08:14.734735: step 2719, loss 0.310487, acc 0.875\n",
      "2018-07-12T19:08:16.351112: step 2720, loss 0.196757, acc 0.890625\n",
      "2018-07-12T19:08:17.949806: step 2721, loss 0.188852, acc 0.90625\n",
      "2018-07-12T19:08:19.611640: step 2722, loss 0.394291, acc 0.84375\n",
      "2018-07-12T19:08:21.234538: step 2723, loss 0.193272, acc 0.9375\n",
      "2018-07-12T19:08:22.807575: step 2724, loss 0.120218, acc 0.984375\n",
      "2018-07-12T19:08:24.455571: step 2725, loss 0.200702, acc 0.921875\n",
      "2018-07-12T19:08:26.068260: step 2726, loss 0.208077, acc 0.9375\n",
      "2018-07-12T19:08:27.693022: step 2727, loss 0.274522, acc 0.875\n",
      "2018-07-12T19:08:29.374126: step 2728, loss 0.197705, acc 0.890625\n",
      "2018-07-12T19:08:31.015022: step 2729, loss 0.286517, acc 0.90625\n",
      "2018-07-12T19:08:32.625144: step 2730, loss 0.22478, acc 0.875\n",
      "2018-07-12T19:08:34.208102: step 2731, loss 0.228371, acc 0.9375\n",
      "2018-07-12T19:08:35.826373: step 2732, loss 0.277363, acc 0.921875\n",
      "2018-07-12T19:08:37.570991: step 2733, loss 0.262056, acc 0.890625\n",
      "2018-07-12T19:08:39.174723: step 2734, loss 0.282428, acc 0.890625\n",
      "2018-07-12T19:08:40.803276: step 2735, loss 0.189205, acc 0.9375\n",
      "2018-07-12T19:08:42.401782: step 2736, loss 0.215331, acc 0.9375\n",
      "2018-07-12T19:08:44.075454: step 2737, loss 0.270663, acc 0.890625\n",
      "2018-07-12T19:08:45.738754: step 2738, loss 0.288271, acc 0.875\n",
      "2018-07-12T19:08:47.353290: step 2739, loss 0.159537, acc 0.9375\n",
      "2018-07-12T19:08:48.993084: step 2740, loss 0.113884, acc 0.96875\n",
      "2018-07-12T19:08:50.686530: step 2741, loss 0.157812, acc 0.921875\n",
      "2018-07-12T19:08:52.294510: step 2742, loss 0.442874, acc 0.8125\n",
      "2018-07-12T19:08:53.967868: step 2743, loss 0.149453, acc 0.921875\n",
      "2018-07-12T19:08:55.637023: step 2744, loss 0.235938, acc 0.90625\n",
      "2018-07-12T19:08:57.255217: step 2745, loss 0.295685, acc 0.90625\n",
      "2018-07-12T19:08:58.929176: step 2746, loss 0.147648, acc 0.984375\n",
      "2018-07-12T19:09:00.628066: step 2747, loss 0.230918, acc 0.890625\n",
      "2018-07-12T19:09:02.292237: step 2748, loss 0.175356, acc 0.921875\n",
      "2018-07-12T19:09:03.926231: step 2749, loss 0.192431, acc 0.9375\n",
      "2018-07-12T19:09:05.594108: step 2750, loss 0.413465, acc 0.828125\n",
      "2018-07-12T19:09:07.246098: step 2751, loss 0.237447, acc 0.90625\n",
      "2018-07-12T19:09:08.886941: step 2752, loss 0.166038, acc 0.9375\n",
      "2018-07-12T19:09:10.528813: step 2753, loss 0.22096, acc 0.90625\n",
      "2018-07-12T19:09:12.194203: step 2754, loss 0.29088, acc 0.875\n",
      "2018-07-12T19:09:13.812331: step 2755, loss 0.335128, acc 0.890625\n",
      "2018-07-12T19:09:15.510081: step 2756, loss 0.215704, acc 0.9375\n",
      "2018-07-12T19:09:17.079605: step 2757, loss 0.2608, acc 0.90625\n",
      "2018-07-12T19:09:18.711868: step 2758, loss 0.32241, acc 0.859375\n",
      "2018-07-12T19:09:20.415318: step 2759, loss 0.259583, acc 0.921875\n",
      "2018-07-12T19:09:22.067079: step 2760, loss 0.190607, acc 0.921875\n",
      "2018-07-12T19:09:23.691955: step 2761, loss 0.226805, acc 0.921875\n",
      "2018-07-12T19:09:25.242226: step 2762, loss 0.181409, acc 0.953125\n",
      "2018-07-12T19:09:26.896539: step 2763, loss 0.172075, acc 0.921875\n",
      "2018-07-12T19:09:28.530749: step 2764, loss 0.303817, acc 0.875\n",
      "2018-07-12T19:09:30.202936: step 2765, loss 0.142017, acc 0.953125\n",
      "2018-07-12T19:09:31.935082: step 2766, loss 0.201946, acc 0.921875\n",
      "2018-07-12T19:09:33.619090: step 2767, loss 0.228134, acc 0.875\n",
      "2018-07-12T19:09:35.288379: step 2768, loss 0.34165, acc 0.90625\n",
      "2018-07-12T19:09:36.942707: step 2769, loss 0.243977, acc 0.90625\n",
      "2018-07-12T19:09:38.551646: step 2770, loss 0.135051, acc 0.9375\n",
      "2018-07-12T19:09:40.142813: step 2771, loss 0.202057, acc 0.890625\n",
      "2018-07-12T19:09:41.767745: step 2772, loss 0.240549, acc 0.90625\n",
      "2018-07-12T19:09:43.379017: step 2773, loss 0.331623, acc 0.890625\n",
      "2018-07-12T19:09:45.007769: step 2774, loss 0.412488, acc 0.84375\n",
      "2018-07-12T19:09:46.633007: step 2775, loss 0.302357, acc 0.859375\n",
      "2018-07-12T19:09:48.218692: step 2776, loss 0.227498, acc 0.921875\n",
      "2018-07-12T19:09:49.859850: step 2777, loss 0.299817, acc 0.875\n",
      "2018-07-12T19:09:51.527342: step 2778, loss 0.245714, acc 0.828125\n",
      "2018-07-12T19:09:53.152919: step 2779, loss 0.194027, acc 0.90625\n",
      "2018-07-12T19:09:54.729062: step 2780, loss 0.337002, acc 0.875\n",
      "2018-07-12T19:09:56.296702: step 2781, loss 0.315784, acc 0.890625\n",
      "2018-07-12T19:09:57.923140: step 2782, loss 0.479297, acc 0.796875\n",
      "2018-07-12T19:09:59.544324: step 2783, loss 0.253674, acc 0.875\n",
      "2018-07-12T19:10:01.199787: step 2784, loss 0.228245, acc 0.890625\n",
      "2018-07-12T19:10:02.789520: step 2785, loss 0.272892, acc 0.890625\n",
      "2018-07-12T19:10:04.407823: step 2786, loss 0.327011, acc 0.859375\n",
      "2018-07-12T19:10:06.041500: step 2787, loss 0.267369, acc 0.890625\n",
      "2018-07-12T19:10:07.721277: step 2788, loss 0.296534, acc 0.921875\n",
      "2018-07-12T19:10:09.345197: step 2789, loss 0.17987, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:10:11.071473: step 2790, loss 0.20205, acc 0.90625\n",
      "2018-07-12T19:10:12.744862: step 2791, loss 0.149952, acc 0.921875\n",
      "2018-07-12T19:10:14.353047: step 2792, loss 0.329384, acc 0.875\n",
      "2018-07-12T19:10:15.959285: step 2793, loss 0.354002, acc 0.828125\n",
      "2018-07-12T19:10:17.540073: step 2794, loss 0.375117, acc 0.828125\n",
      "2018-07-12T19:10:19.196395: step 2795, loss 0.338504, acc 0.84375\n",
      "2018-07-12T19:10:20.812823: step 2796, loss 0.304958, acc 0.875\n",
      "2018-07-12T19:10:22.456872: step 2797, loss 0.430557, acc 0.8125\n",
      "2018-07-12T19:10:24.070693: step 2798, loss 0.159601, acc 0.953125\n",
      "2018-07-12T19:10:25.670170: step 2799, loss 0.136125, acc 0.953125\n",
      "2018-07-12T19:10:27.328854: step 2800, loss 0.317223, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:10:45.498653: step 2800, loss 0.280039, acc 0.8788\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2800\n",
      "\n",
      "2018-07-12T19:10:48.223531: step 2801, loss 0.16726, acc 0.9375\n",
      "2018-07-12T19:10:49.864339: step 2802, loss 0.306072, acc 0.859375\n",
      "2018-07-12T19:10:51.489226: step 2803, loss 0.348953, acc 0.8125\n",
      "2018-07-12T19:10:53.202416: step 2804, loss 0.243716, acc 0.859375\n",
      "2018-07-12T19:10:54.820442: step 2805, loss 0.226162, acc 0.890625\n",
      "2018-07-12T19:10:56.448522: step 2806, loss 0.326793, acc 0.859375\n",
      "2018-07-12T19:10:58.080580: step 2807, loss 0.150394, acc 0.96875\n",
      "2018-07-12T19:10:59.786862: step 2808, loss 0.187274, acc 0.96875\n",
      "2018-07-12T19:11:01.434208: step 2809, loss 0.296546, acc 0.890625\n",
      "2018-07-12T19:11:03.028920: step 2810, loss 0.30596, acc 0.859375\n",
      "2018-07-12T19:11:04.616549: step 2811, loss 0.205096, acc 0.9375\n",
      "2018-07-12T19:11:06.278799: step 2812, loss 0.202246, acc 0.921875\n",
      "2018-07-12T19:11:07.945573: step 2813, loss 0.197701, acc 0.90625\n",
      "2018-07-12T19:11:09.650964: step 2814, loss 0.184382, acc 0.921875\n",
      "2018-07-12T19:11:11.282402: step 2815, loss 0.177139, acc 0.96875\n",
      "2018-07-12T19:11:12.303518: step 2816, loss 0.278412, acc 0.888889\n",
      "2018-07-12T19:11:13.941515: step 2817, loss 0.193483, acc 0.953125\n",
      "2018-07-12T19:11:15.608713: step 2818, loss 0.126065, acc 0.953125\n",
      "2018-07-12T19:11:17.246636: step 2819, loss 0.17306, acc 0.9375\n",
      "2018-07-12T19:11:18.826039: step 2820, loss 0.156012, acc 0.921875\n",
      "2018-07-12T19:11:20.452792: step 2821, loss 0.349623, acc 0.859375\n",
      "2018-07-12T19:11:22.056066: step 2822, loss 0.388236, acc 0.84375\n",
      "2018-07-12T19:11:23.680234: step 2823, loss 0.254418, acc 0.84375\n",
      "2018-07-12T19:11:25.295548: step 2824, loss 0.209565, acc 0.875\n",
      "2018-07-12T19:11:26.975897: step 2825, loss 0.230383, acc 0.875\n",
      "2018-07-12T19:11:28.631587: step 2826, loss 0.182533, acc 0.921875\n",
      "2018-07-12T19:11:30.220103: step 2827, loss 0.124008, acc 0.984375\n",
      "2018-07-12T19:11:31.841664: step 2828, loss 0.246971, acc 0.875\n",
      "2018-07-12T19:11:33.464547: step 2829, loss 0.0827807, acc 0.984375\n",
      "2018-07-12T19:11:35.053612: step 2830, loss 0.202503, acc 0.953125\n",
      "2018-07-12T19:11:36.667624: step 2831, loss 0.12053, acc 0.953125\n",
      "2018-07-12T19:11:38.284738: step 2832, loss 0.225279, acc 0.890625\n",
      "2018-07-12T19:11:39.898933: step 2833, loss 0.374009, acc 0.875\n",
      "2018-07-12T19:11:41.503837: step 2834, loss 0.212539, acc 0.9375\n",
      "2018-07-12T19:11:43.252146: step 2835, loss 0.219165, acc 0.890625\n",
      "2018-07-12T19:11:44.867331: step 2836, loss 0.25391, acc 0.90625\n",
      "2018-07-12T19:11:46.517330: step 2837, loss 0.228669, acc 0.9375\n",
      "2018-07-12T19:11:48.168179: step 2838, loss 0.23412, acc 0.921875\n",
      "2018-07-12T19:11:49.792037: step 2839, loss 0.261579, acc 0.90625\n",
      "2018-07-12T19:11:51.412586: step 2840, loss 0.291143, acc 0.890625\n",
      "2018-07-12T19:11:53.010863: step 2841, loss 0.13785, acc 0.96875\n",
      "2018-07-12T19:11:54.608706: step 2842, loss 0.17876, acc 0.90625\n",
      "2018-07-12T19:11:56.215406: step 2843, loss 0.27269, acc 0.859375\n",
      "2018-07-12T19:11:57.843399: step 2844, loss 0.337969, acc 0.875\n",
      "2018-07-12T19:11:59.581590: step 2845, loss 0.252023, acc 0.890625\n",
      "2018-07-12T19:12:01.206140: step 2846, loss 0.38235, acc 0.859375\n",
      "2018-07-12T19:12:02.825744: step 2847, loss 0.231794, acc 0.890625\n",
      "2018-07-12T19:12:04.405538: step 2848, loss 0.247146, acc 0.890625\n",
      "2018-07-12T19:12:06.035278: step 2849, loss 0.194501, acc 0.90625\n",
      "2018-07-12T19:12:07.649031: step 2850, loss 0.268666, acc 0.875\n",
      "2018-07-12T19:12:09.299048: step 2851, loss 0.249223, acc 0.875\n",
      "2018-07-12T19:12:11.070755: step 2852, loss 0.139119, acc 0.984375\n",
      "2018-07-12T19:12:12.762274: step 2853, loss 0.166174, acc 0.921875\n",
      "2018-07-12T19:12:14.409729: step 2854, loss 0.164048, acc 0.921875\n",
      "2018-07-12T19:12:15.990714: step 2855, loss 0.307473, acc 0.859375\n",
      "2018-07-12T19:12:17.626716: step 2856, loss 0.140995, acc 0.921875\n",
      "2018-07-12T19:12:19.264181: step 2857, loss 0.194876, acc 0.875\n",
      "2018-07-12T19:12:20.969191: step 2858, loss 0.196125, acc 0.890625\n",
      "2018-07-12T19:12:22.587413: step 2859, loss 0.231434, acc 0.9375\n",
      "2018-07-12T19:12:24.229429: step 2860, loss 0.213317, acc 0.9375\n",
      "2018-07-12T19:12:25.830800: step 2861, loss 0.22748, acc 0.875\n",
      "2018-07-12T19:12:27.517079: step 2862, loss 0.381781, acc 0.84375\n",
      "2018-07-12T19:12:29.137989: step 2863, loss 0.373218, acc 0.875\n",
      "2018-07-12T19:12:30.817250: step 2864, loss 0.257313, acc 0.90625\n",
      "2018-07-12T19:12:32.361208: step 2865, loss 0.225166, acc 0.921875\n",
      "2018-07-12T19:12:33.999501: step 2866, loss 0.151463, acc 0.953125\n",
      "2018-07-12T19:12:35.583339: step 2867, loss 0.0817234, acc 0.984375\n",
      "2018-07-12T19:12:37.217629: step 2868, loss 0.255686, acc 0.875\n",
      "2018-07-12T19:12:38.819146: step 2869, loss 0.257622, acc 0.921875\n",
      "2018-07-12T19:12:40.395442: step 2870, loss 0.227888, acc 0.890625\n",
      "2018-07-12T19:12:41.986644: step 2871, loss 0.116635, acc 0.953125\n",
      "2018-07-12T19:12:43.605881: step 2872, loss 0.294404, acc 0.859375\n",
      "2018-07-12T19:12:45.194831: step 2873, loss 0.128816, acc 0.953125\n",
      "2018-07-12T19:12:46.813267: step 2874, loss 0.175436, acc 0.953125\n",
      "2018-07-12T19:12:48.393359: step 2875, loss 0.331453, acc 0.90625\n",
      "2018-07-12T19:12:49.974763: step 2876, loss 0.121788, acc 0.921875\n",
      "2018-07-12T19:12:51.603790: step 2877, loss 0.290172, acc 0.890625\n",
      "2018-07-12T19:12:53.234030: step 2878, loss 0.212863, acc 0.90625\n",
      "2018-07-12T19:12:54.910080: step 2879, loss 0.153206, acc 0.953125\n",
      "2018-07-12T19:12:56.531781: step 2880, loss 0.157309, acc 0.96875\n",
      "2018-07-12T19:12:58.101906: step 2881, loss 0.282549, acc 0.875\n",
      "2018-07-12T19:12:59.779383: step 2882, loss 0.251253, acc 0.90625\n",
      "2018-07-12T19:13:01.369343: step 2883, loss 0.174005, acc 0.921875\n",
      "2018-07-12T19:13:03.030828: step 2884, loss 0.355214, acc 0.859375\n",
      "2018-07-12T19:13:04.618728: step 2885, loss 0.142847, acc 0.953125\n",
      "2018-07-12T19:13:06.289139: step 2886, loss 0.147119, acc 0.953125\n",
      "2018-07-12T19:13:07.860355: step 2887, loss 0.197694, acc 0.9375\n",
      "2018-07-12T19:13:09.508246: step 2888, loss 0.127056, acc 0.9375\n",
      "2018-07-12T19:13:11.130586: step 2889, loss 0.183551, acc 0.953125\n",
      "2018-07-12T19:13:12.778393: step 2890, loss 0.189139, acc 0.953125\n",
      "2018-07-12T19:13:14.411756: step 2891, loss 0.172848, acc 0.9375\n",
      "2018-07-12T19:13:16.079002: step 2892, loss 0.280068, acc 0.8125\n",
      "2018-07-12T19:13:17.703828: step 2893, loss 0.0909126, acc 0.953125\n",
      "2018-07-12T19:13:19.336053: step 2894, loss 0.0973079, acc 0.96875\n",
      "2018-07-12T19:13:20.945210: step 2895, loss 0.281521, acc 0.875\n",
      "2018-07-12T19:13:22.557599: step 2896, loss 0.184063, acc 0.921875\n",
      "2018-07-12T19:13:24.154788: step 2897, loss 0.27416, acc 0.875\n",
      "2018-07-12T19:13:25.794629: step 2898, loss 0.274861, acc 0.859375\n",
      "2018-07-12T19:13:27.443826: step 2899, loss 0.31953, acc 0.90625\n",
      "2018-07-12T19:13:29.068007: step 2900, loss 0.182092, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:13:46.709684: step 2900, loss 0.277485, acc 0.8812\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-2900\n",
      "\n",
      "2018-07-12T19:13:49.339473: step 2901, loss 0.235779, acc 0.9375\n",
      "2018-07-12T19:13:51.024235: step 2902, loss 0.119839, acc 0.9375\n",
      "2018-07-12T19:13:52.693920: step 2903, loss 0.208925, acc 0.90625\n",
      "2018-07-12T19:13:54.315669: step 2904, loss 0.195089, acc 0.90625\n",
      "2018-07-12T19:13:55.920430: step 2905, loss 0.280002, acc 0.859375\n",
      "2018-07-12T19:13:57.562499: step 2906, loss 0.243786, acc 0.890625\n",
      "2018-07-12T19:13:59.154837: step 2907, loss 0.232694, acc 0.90625\n",
      "2018-07-12T19:14:00.783188: step 2908, loss 0.09982, acc 0.984375\n",
      "2018-07-12T19:14:02.327477: step 2909, loss 0.276647, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:14:03.957231: step 2910, loss 0.197096, acc 0.921875\n",
      "2018-07-12T19:14:05.601016: step 2911, loss 0.244426, acc 0.875\n",
      "2018-07-12T19:14:07.219547: step 2912, loss 0.354023, acc 0.859375\n",
      "2018-07-12T19:14:08.806770: step 2913, loss 0.216494, acc 0.890625\n",
      "2018-07-12T19:14:10.491809: step 2914, loss 0.149728, acc 0.921875\n",
      "2018-07-12T19:14:12.147096: step 2915, loss 0.17247, acc 0.921875\n",
      "2018-07-12T19:14:13.801520: step 2916, loss 0.119202, acc 0.96875\n",
      "2018-07-12T19:14:15.372462: step 2917, loss 0.108953, acc 0.984375\n",
      "2018-07-12T19:14:17.063218: step 2918, loss 0.204044, acc 0.890625\n",
      "2018-07-12T19:14:18.657849: step 2919, loss 0.181391, acc 0.921875\n",
      "2018-07-12T19:14:20.292258: step 2920, loss 0.212874, acc 0.9375\n",
      "2018-07-12T19:14:22.001619: step 2921, loss 0.220222, acc 0.921875\n",
      "2018-07-12T19:14:23.629551: step 2922, loss 0.224521, acc 0.890625\n",
      "2018-07-12T19:14:25.207456: step 2923, loss 0.139324, acc 0.9375\n",
      "2018-07-12T19:14:26.860031: step 2924, loss 0.169813, acc 0.9375\n",
      "2018-07-12T19:14:28.503755: step 2925, loss 0.234826, acc 0.921875\n",
      "2018-07-12T19:14:30.115504: step 2926, loss 0.143278, acc 0.9375\n",
      "2018-07-12T19:14:31.722104: step 2927, loss 0.268568, acc 0.890625\n",
      "2018-07-12T19:14:33.370717: step 2928, loss 0.134311, acc 0.96875\n",
      "2018-07-12T19:14:34.997154: step 2929, loss 0.186853, acc 0.90625\n",
      "2018-07-12T19:14:36.625350: step 2930, loss 0.208016, acc 0.90625\n",
      "2018-07-12T19:14:38.236805: step 2931, loss 0.347613, acc 0.890625\n",
      "2018-07-12T19:14:39.843289: step 2932, loss 0.20859, acc 0.9375\n",
      "2018-07-12T19:14:41.482285: step 2933, loss 0.128587, acc 0.953125\n",
      "2018-07-12T19:14:43.171191: step 2934, loss 0.31799, acc 0.921875\n",
      "2018-07-12T19:14:44.813869: step 2935, loss 0.253907, acc 0.875\n",
      "2018-07-12T19:14:46.392931: step 2936, loss 0.172173, acc 0.9375\n",
      "2018-07-12T19:14:48.017743: step 2937, loss 0.118392, acc 0.96875\n",
      "2018-07-12T19:14:49.645573: step 2938, loss 0.257082, acc 0.921875\n",
      "2018-07-12T19:14:51.245947: step 2939, loss 0.149414, acc 0.953125\n",
      "2018-07-12T19:14:52.925691: step 2940, loss 0.142655, acc 0.9375\n",
      "2018-07-12T19:14:54.527442: step 2941, loss 0.188973, acc 0.875\n",
      "2018-07-12T19:14:56.152235: step 2942, loss 0.244138, acc 0.859375\n",
      "2018-07-12T19:14:57.748887: step 2943, loss 0.206233, acc 0.921875\n",
      "2018-07-12T19:14:59.363691: step 2944, loss 0.169594, acc 0.9375\n",
      "2018-07-12T19:15:00.940052: step 2945, loss 0.263718, acc 0.921875\n",
      "2018-07-12T19:15:02.661997: step 2946, loss 0.133577, acc 0.9375\n",
      "2018-07-12T19:15:04.264436: step 2947, loss 0.144871, acc 0.953125\n",
      "2018-07-12T19:15:05.899361: step 2948, loss 0.302975, acc 0.90625\n",
      "2018-07-12T19:15:07.541823: step 2949, loss 0.229444, acc 0.921875\n",
      "2018-07-12T19:15:09.162837: step 2950, loss 0.172948, acc 0.9375\n",
      "2018-07-12T19:15:10.794573: step 2951, loss 0.263685, acc 0.921875\n",
      "2018-07-12T19:15:12.380456: step 2952, loss 0.223486, acc 0.890625\n",
      "2018-07-12T19:15:14.038543: step 2953, loss 0.119226, acc 0.984375\n",
      "2018-07-12T19:15:15.663920: step 2954, loss 0.189732, acc 0.9375\n",
      "2018-07-12T19:15:17.386750: step 2955, loss 0.17004, acc 0.90625\n",
      "2018-07-12T19:15:19.069122: step 2956, loss 0.200044, acc 0.90625\n",
      "2018-07-12T19:15:20.737419: step 2957, loss 0.22677, acc 0.921875\n",
      "2018-07-12T19:15:22.368865: step 2958, loss 0.492216, acc 0.796875\n",
      "2018-07-12T19:15:23.980396: step 2959, loss 0.166662, acc 0.90625\n",
      "2018-07-12T19:15:25.631884: step 2960, loss 0.158453, acc 0.9375\n",
      "2018-07-12T19:15:27.236069: step 2961, loss 0.187889, acc 0.90625\n",
      "2018-07-12T19:15:28.818579: step 2962, loss 0.167588, acc 0.875\n",
      "2018-07-12T19:15:30.460377: step 2963, loss 0.155445, acc 0.9375\n",
      "2018-07-12T19:15:32.145628: step 2964, loss 0.330906, acc 0.859375\n",
      "2018-07-12T19:15:33.811860: step 2965, loss 0.203216, acc 0.921875\n",
      "2018-07-12T19:15:35.414920: step 2966, loss 0.159996, acc 0.9375\n",
      "2018-07-12T19:15:37.023453: step 2967, loss 0.342979, acc 0.859375\n",
      "2018-07-12T19:15:38.719243: step 2968, loss 0.160825, acc 0.921875\n",
      "2018-07-12T19:15:40.365478: step 2969, loss 0.238087, acc 0.9375\n",
      "2018-07-12T19:15:42.011639: step 2970, loss 0.139207, acc 0.953125\n",
      "2018-07-12T19:15:43.641662: step 2971, loss 0.153149, acc 0.9375\n",
      "2018-07-12T19:15:45.325170: step 2972, loss 0.155596, acc 0.90625\n",
      "2018-07-12T19:15:47.006278: step 2973, loss 0.191826, acc 0.9375\n",
      "2018-07-12T19:15:48.632490: step 2974, loss 0.110873, acc 0.96875\n",
      "2018-07-12T19:15:50.249704: step 2975, loss 0.232124, acc 0.90625\n",
      "2018-07-12T19:15:51.944021: step 2976, loss 0.214301, acc 0.953125\n",
      "2018-07-12T19:15:53.586759: step 2977, loss 0.186805, acc 0.921875\n",
      "2018-07-12T19:15:55.207759: step 2978, loss 0.195094, acc 0.90625\n",
      "2018-07-12T19:15:56.839751: step 2979, loss 0.289977, acc 0.890625\n",
      "2018-07-12T19:15:58.542107: step 2980, loss 0.168042, acc 0.90625\n",
      "2018-07-12T19:16:00.225728: step 2981, loss 0.338426, acc 0.875\n",
      "2018-07-12T19:16:01.809403: step 2982, loss 0.165501, acc 0.921875\n",
      "2018-07-12T19:16:03.469696: step 2983, loss 0.242261, acc 0.921875\n",
      "2018-07-12T19:16:05.085359: step 2984, loss 0.206914, acc 0.953125\n",
      "2018-07-12T19:16:06.721809: step 2985, loss 0.210195, acc 0.9375\n",
      "2018-07-12T19:16:08.378773: step 2986, loss 0.176573, acc 0.9375\n",
      "2018-07-12T19:16:10.000011: step 2987, loss 0.170212, acc 0.90625\n",
      "2018-07-12T19:16:11.633333: step 2988, loss 0.23547, acc 0.921875\n",
      "2018-07-12T19:16:13.252616: step 2989, loss 0.128872, acc 0.984375\n",
      "2018-07-12T19:16:14.848438: step 2990, loss 0.27058, acc 0.859375\n",
      "2018-07-12T19:16:16.486206: step 2991, loss 0.230485, acc 0.875\n",
      "2018-07-12T19:16:18.136419: step 2992, loss 0.139435, acc 0.953125\n",
      "2018-07-12T19:16:19.771738: step 2993, loss 0.183219, acc 0.9375\n",
      "2018-07-12T19:16:21.377059: step 2994, loss 0.259701, acc 0.890625\n",
      "2018-07-12T19:16:23.050662: step 2995, loss 0.187976, acc 0.921875\n",
      "2018-07-12T19:16:24.737414: step 2996, loss 0.176922, acc 0.9375\n",
      "2018-07-12T19:16:26.422497: step 2997, loss 0.211851, acc 0.921875\n",
      "2018-07-12T19:16:27.989680: step 2998, loss 0.402903, acc 0.8125\n",
      "2018-07-12T19:16:29.613274: step 2999, loss 0.219762, acc 0.921875\n",
      "2018-07-12T19:16:31.284659: step 3000, loss 0.143817, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:16:48.856882: step 3000, loss 0.280799, acc 0.8784\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3000\n",
      "\n",
      "2018-07-12T19:16:51.437343: step 3001, loss 0.214192, acc 0.890625\n",
      "2018-07-12T19:16:52.997240: step 3002, loss 0.144512, acc 0.953125\n",
      "2018-07-12T19:16:54.691299: step 3003, loss 0.220538, acc 0.921875\n",
      "2018-07-12T19:16:56.298050: step 3004, loss 0.222061, acc 0.953125\n",
      "2018-07-12T19:16:57.907057: step 3005, loss 0.10902, acc 0.9375\n",
      "2018-07-12T19:16:59.520666: step 3006, loss 0.299029, acc 0.875\n",
      "2018-07-12T19:17:01.191319: step 3007, loss 0.100818, acc 0.96875\n",
      "2018-07-12T19:17:02.846403: step 3008, loss 0.141253, acc 0.953125\n",
      "2018-07-12T19:17:04.542706: step 3009, loss 0.16024, acc 0.90625\n",
      "2018-07-12T19:17:06.188046: step 3010, loss 0.181415, acc 0.921875\n",
      "2018-07-12T19:17:07.819076: step 3011, loss 0.213074, acc 0.9375\n",
      "2018-07-12T19:17:09.386509: step 3012, loss 0.171627, acc 0.96875\n",
      "2018-07-12T19:17:11.027775: step 3013, loss 0.157964, acc 0.921875\n",
      "2018-07-12T19:17:12.661211: step 3014, loss 0.192136, acc 0.90625\n",
      "2018-07-12T19:17:14.323697: step 3015, loss 0.136891, acc 0.9375\n",
      "2018-07-12T19:17:15.995085: step 3016, loss 0.166302, acc 0.921875\n",
      "2018-07-12T19:17:17.616739: step 3017, loss 0.176587, acc 0.9375\n",
      "2018-07-12T19:17:19.246395: step 3018, loss 0.228688, acc 0.890625\n",
      "2018-07-12T19:17:20.826097: step 3019, loss 0.228821, acc 0.90625\n",
      "2018-07-12T19:17:22.430635: step 3020, loss 0.255674, acc 0.890625\n",
      "2018-07-12T19:17:24.064397: step 3021, loss 0.230673, acc 0.890625\n",
      "2018-07-12T19:17:25.652943: step 3022, loss 0.151083, acc 0.9375\n",
      "2018-07-12T19:17:27.290233: step 3023, loss 0.131232, acc 0.96875\n",
      "2018-07-12T19:17:28.896128: step 3024, loss 0.0998076, acc 0.984375\n",
      "2018-07-12T19:17:30.515452: step 3025, loss 0.102611, acc 0.984375\n",
      "2018-07-12T19:17:32.147937: step 3026, loss 0.198136, acc 0.921875\n",
      "2018-07-12T19:17:33.767061: step 3027, loss 0.179597, acc 0.953125\n",
      "2018-07-12T19:17:35.416963: step 3028, loss 0.171335, acc 0.90625\n",
      "2018-07-12T19:17:37.100823: step 3029, loss 0.193393, acc 0.890625\n",
      "2018-07-12T19:17:38.665501: step 3030, loss 0.351598, acc 0.90625\n",
      "2018-07-12T19:17:40.258163: step 3031, loss 0.134257, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:17:41.974048: step 3032, loss 0.247241, acc 0.90625\n",
      "2018-07-12T19:17:43.607281: step 3033, loss 0.127649, acc 0.96875\n",
      "2018-07-12T19:17:45.168364: step 3034, loss 0.191859, acc 0.90625\n",
      "2018-07-12T19:17:46.795957: step 3035, loss 0.157016, acc 0.921875\n",
      "2018-07-12T19:17:48.436729: step 3036, loss 0.171797, acc 0.90625\n",
      "2018-07-12T19:17:50.009046: step 3037, loss 0.106175, acc 0.953125\n",
      "2018-07-12T19:17:51.636377: step 3038, loss 0.188038, acc 0.90625\n",
      "2018-07-12T19:17:53.323510: step 3039, loss 0.140199, acc 0.9375\n",
      "2018-07-12T19:17:54.923748: step 3040, loss 0.207358, acc 0.890625\n",
      "2018-07-12T19:17:56.594730: step 3041, loss 0.376294, acc 0.875\n",
      "2018-07-12T19:17:58.258485: step 3042, loss 0.147791, acc 0.953125\n",
      "2018-07-12T19:17:59.856567: step 3043, loss 0.203313, acc 0.9375\n",
      "2018-07-12T19:18:01.413740: step 3044, loss 0.242352, acc 0.890625\n",
      "2018-07-12T19:18:03.044357: step 3045, loss 0.167581, acc 0.9375\n",
      "2018-07-12T19:18:04.633269: step 3046, loss 0.279988, acc 0.890625\n",
      "2018-07-12T19:18:06.319454: step 3047, loss 0.211156, acc 0.859375\n",
      "2018-07-12T19:18:07.900100: step 3048, loss 0.189513, acc 0.921875\n",
      "2018-07-12T19:18:09.560906: step 3049, loss 0.125493, acc 0.953125\n",
      "2018-07-12T19:18:11.178336: step 3050, loss 0.155522, acc 0.9375\n",
      "2018-07-12T19:18:12.748649: step 3051, loss 0.195823, acc 0.921875\n",
      "2018-07-12T19:18:14.397439: step 3052, loss 0.310043, acc 0.859375\n",
      "2018-07-12T19:18:15.977211: step 3053, loss 0.2357, acc 0.9375\n",
      "2018-07-12T19:18:17.765401: step 3054, loss 0.26246, acc 0.890625\n",
      "2018-07-12T19:18:19.431992: step 3055, loss 0.255652, acc 0.90625\n",
      "2018-07-12T19:18:21.056331: step 3056, loss 0.2117, acc 0.9375\n",
      "2018-07-12T19:18:22.705890: step 3057, loss 0.235135, acc 0.890625\n",
      "2018-07-12T19:18:24.326675: step 3058, loss 0.242703, acc 0.875\n",
      "2018-07-12T19:18:25.944522: step 3059, loss 0.338586, acc 0.90625\n",
      "2018-07-12T19:18:27.579972: step 3060, loss 0.172325, acc 0.921875\n",
      "2018-07-12T19:18:29.144759: step 3061, loss 0.183963, acc 0.9375\n",
      "2018-07-12T19:18:30.831970: step 3062, loss 0.176059, acc 0.9375\n",
      "2018-07-12T19:18:32.437301: step 3063, loss 0.400812, acc 0.84375\n",
      "2018-07-12T19:18:34.077602: step 3064, loss 0.100276, acc 0.96875\n",
      "2018-07-12T19:18:35.670369: step 3065, loss 0.238962, acc 0.921875\n",
      "2018-07-12T19:18:37.328982: step 3066, loss 0.17067, acc 0.890625\n",
      "2018-07-12T19:18:38.994198: step 3067, loss 0.204661, acc 0.890625\n",
      "2018-07-12T19:18:40.645512: step 3068, loss 0.295678, acc 0.828125\n",
      "2018-07-12T19:18:42.231600: step 3069, loss 0.263549, acc 0.90625\n",
      "2018-07-12T19:18:43.920220: step 3070, loss 0.225113, acc 0.90625\n",
      "2018-07-12T19:18:45.609723: step 3071, loss 0.262417, acc 0.921875\n",
      "2018-07-12T19:18:47.253242: step 3072, loss 0.257486, acc 0.921875\n",
      "2018-07-12T19:18:48.877522: step 3073, loss 0.195317, acc 0.953125\n",
      "2018-07-12T19:18:50.472856: step 3074, loss 0.210835, acc 0.875\n",
      "2018-07-12T19:18:52.147482: step 3075, loss 0.128656, acc 0.984375\n",
      "2018-07-12T19:18:53.761725: step 3076, loss 0.132712, acc 0.96875\n",
      "2018-07-12T19:18:55.357845: step 3077, loss 0.0704302, acc 0.96875\n",
      "2018-07-12T19:18:57.050133: step 3078, loss 0.202029, acc 0.90625\n",
      "2018-07-12T19:18:58.723659: step 3079, loss 0.282484, acc 0.90625\n",
      "2018-07-12T19:19:00.355752: step 3080, loss 0.199483, acc 0.890625\n",
      "2018-07-12T19:19:02.016584: step 3081, loss 0.324377, acc 0.890625\n",
      "2018-07-12T19:19:03.703869: step 3082, loss 0.196747, acc 0.90625\n",
      "2018-07-12T19:19:05.300556: step 3083, loss 0.186822, acc 0.90625\n",
      "2018-07-12T19:19:06.945310: step 3084, loss 0.193992, acc 0.90625\n",
      "2018-07-12T19:19:08.544050: step 3085, loss 0.275747, acc 0.890625\n",
      "2018-07-12T19:19:10.265383: step 3086, loss 0.190066, acc 0.90625\n",
      "2018-07-12T19:19:11.891279: step 3087, loss 0.273111, acc 0.875\n",
      "2018-07-12T19:19:13.546283: step 3088, loss 0.110152, acc 0.953125\n",
      "2018-07-12T19:19:15.149854: step 3089, loss 0.191878, acc 0.953125\n",
      "2018-07-12T19:19:16.752914: step 3090, loss 0.126536, acc 0.953125\n",
      "2018-07-12T19:19:18.380406: step 3091, loss 0.168822, acc 0.9375\n",
      "2018-07-12T19:19:19.985480: step 3092, loss 0.220757, acc 0.921875\n",
      "2018-07-12T19:19:21.572733: step 3093, loss 0.100881, acc 0.953125\n",
      "2018-07-12T19:19:23.166903: step 3094, loss 0.157737, acc 0.9375\n",
      "2018-07-12T19:19:24.802615: step 3095, loss 0.122674, acc 0.953125\n",
      "2018-07-12T19:19:26.419718: step 3096, loss 0.140348, acc 0.953125\n",
      "2018-07-12T19:19:28.061324: step 3097, loss 0.270996, acc 0.890625\n",
      "2018-07-12T19:19:29.759680: step 3098, loss 0.22659, acc 0.90625\n",
      "2018-07-12T19:19:31.383447: step 3099, loss 0.116007, acc 0.921875\n",
      "2018-07-12T19:19:33.047127: step 3100, loss 0.265189, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:19:51.105096: step 3100, loss 0.2773, acc 0.8844\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3100\n",
      "\n",
      "2018-07-12T19:19:53.793887: step 3101, loss 0.177436, acc 0.9375\n",
      "2018-07-12T19:19:55.414948: step 3102, loss 0.179494, acc 0.890625\n",
      "2018-07-12T19:19:57.089872: step 3103, loss 0.221444, acc 0.921875\n",
      "2018-07-12T19:19:58.722933: step 3104, loss 0.178656, acc 0.90625\n",
      "2018-07-12T19:20:00.304528: step 3105, loss 0.170795, acc 0.921875\n",
      "2018-07-12T19:20:01.926473: step 3106, loss 0.1268, acc 0.9375\n",
      "2018-07-12T19:20:03.528737: step 3107, loss 0.324903, acc 0.890625\n",
      "2018-07-12T19:20:05.155654: step 3108, loss 0.177464, acc 0.90625\n",
      "2018-07-12T19:20:06.734134: step 3109, loss 0.122078, acc 0.953125\n",
      "2018-07-12T19:20:08.407956: step 3110, loss 0.211728, acc 0.921875\n",
      "2018-07-12T19:20:10.009084: step 3111, loss 0.115872, acc 0.953125\n",
      "2018-07-12T19:20:11.679953: step 3112, loss 0.272366, acc 0.890625\n",
      "2018-07-12T19:20:13.289775: step 3113, loss 0.229945, acc 0.953125\n",
      "2018-07-12T19:20:14.948508: step 3114, loss 0.213937, acc 0.90625\n",
      "2018-07-12T19:20:16.482357: step 3115, loss 0.145124, acc 0.96875\n",
      "2018-07-12T19:20:18.087895: step 3116, loss 0.134697, acc 0.9375\n",
      "2018-07-12T19:20:19.694100: step 3117, loss 0.296018, acc 0.90625\n",
      "2018-07-12T19:20:21.325146: step 3118, loss 0.380828, acc 0.8125\n",
      "2018-07-12T19:20:23.019101: step 3119, loss 0.151311, acc 0.953125\n",
      "2018-07-12T19:20:24.690340: step 3120, loss 0.18446, acc 0.921875\n",
      "2018-07-12T19:20:26.280138: step 3121, loss 0.150765, acc 0.9375\n",
      "2018-07-12T19:20:27.837560: step 3122, loss 0.160098, acc 0.953125\n",
      "2018-07-12T19:20:29.477483: step 3123, loss 0.205044, acc 0.921875\n",
      "2018-07-12T19:20:31.096794: step 3124, loss 0.199994, acc 0.9375\n",
      "2018-07-12T19:20:32.824605: step 3125, loss 0.271178, acc 0.859375\n",
      "2018-07-12T19:20:34.447995: step 3126, loss 0.18428, acc 0.921875\n",
      "2018-07-12T19:20:36.048251: step 3127, loss 0.256152, acc 0.890625\n",
      "2018-07-12T19:20:37.703975: step 3128, loss 0.187891, acc 0.90625\n",
      "2018-07-12T19:20:39.315666: step 3129, loss 0.208145, acc 0.9375\n",
      "2018-07-12T19:20:40.946173: step 3130, loss 0.260898, acc 0.90625\n",
      "2018-07-12T19:20:42.536831: step 3131, loss 0.137523, acc 0.953125\n",
      "2018-07-12T19:20:44.211903: step 3132, loss 0.292231, acc 0.859375\n",
      "2018-07-12T19:20:45.885192: step 3133, loss 0.106216, acc 0.953125\n",
      "2018-07-12T19:20:47.488726: step 3134, loss 0.211722, acc 0.921875\n",
      "2018-07-12T19:20:49.099989: step 3135, loss 0.123766, acc 0.9375\n",
      "2018-07-12T19:20:50.687506: step 3136, loss 0.251431, acc 0.875\n",
      "2018-07-12T19:20:52.305613: step 3137, loss 0.190113, acc 0.9375\n",
      "2018-07-12T19:20:53.999241: step 3138, loss 0.262747, acc 0.9375\n",
      "2018-07-12T19:20:55.658911: step 3139, loss 0.265954, acc 0.875\n",
      "2018-07-12T19:20:57.280658: step 3140, loss 0.176195, acc 0.9375\n",
      "2018-07-12T19:20:58.972223: step 3141, loss 0.311923, acc 0.828125\n",
      "2018-07-12T19:21:00.549813: step 3142, loss 0.234906, acc 0.9375\n",
      "2018-07-12T19:21:02.156903: step 3143, loss 0.296092, acc 0.9375\n",
      "2018-07-12T19:21:03.747925: step 3144, loss 0.12345, acc 0.96875\n",
      "2018-07-12T19:21:05.380080: step 3145, loss 0.336106, acc 0.875\n",
      "2018-07-12T19:21:07.024859: step 3146, loss 0.277991, acc 0.953125\n",
      "2018-07-12T19:21:08.654248: step 3147, loss 0.235535, acc 0.890625\n",
      "2018-07-12T19:21:10.268147: step 3148, loss 0.259492, acc 0.875\n",
      "2018-07-12T19:21:11.888556: step 3149, loss 0.158657, acc 0.953125\n",
      "2018-07-12T19:21:13.477166: step 3150, loss 0.146406, acc 0.9375\n",
      "2018-07-12T19:21:15.021092: step 3151, loss 0.346384, acc 0.859375\n",
      "2018-07-12T19:21:16.645960: step 3152, loss 0.274282, acc 0.890625\n",
      "2018-07-12T19:21:18.247960: step 3153, loss 0.252448, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:21:19.995463: step 3154, loss 0.253257, acc 0.890625\n",
      "2018-07-12T19:21:21.637907: step 3155, loss 0.0809735, acc 0.984375\n",
      "2018-07-12T19:21:23.357492: step 3156, loss 0.150202, acc 0.9375\n",
      "2018-07-12T19:21:25.023008: step 3157, loss 0.29343, acc 0.890625\n",
      "2018-07-12T19:21:26.769488: step 3158, loss 0.178016, acc 0.90625\n",
      "2018-07-12T19:21:28.359314: step 3159, loss 0.184956, acc 0.90625\n",
      "2018-07-12T19:21:30.011999: step 3160, loss 0.106875, acc 0.953125\n",
      "2018-07-12T19:21:31.659645: step 3161, loss 0.0891233, acc 0.984375\n",
      "2018-07-12T19:21:33.274959: step 3162, loss 0.22651, acc 0.9375\n",
      "2018-07-12T19:21:34.949417: step 3163, loss 0.222472, acc 0.890625\n",
      "2018-07-12T19:21:36.570409: step 3164, loss 0.101023, acc 0.953125\n",
      "2018-07-12T19:21:38.213106: step 3165, loss 0.276223, acc 0.875\n",
      "2018-07-12T19:21:39.870630: step 3166, loss 0.136405, acc 0.953125\n",
      "2018-07-12T19:21:41.479270: step 3167, loss 0.138774, acc 0.953125\n",
      "2018-07-12T19:21:42.479887: step 3168, loss 0.261949, acc 0.833333\n",
      "2018-07-12T19:21:44.120020: step 3169, loss 0.145487, acc 0.984375\n",
      "2018-07-12T19:21:45.717727: step 3170, loss 0.112699, acc 0.9375\n",
      "2018-07-12T19:21:47.310999: step 3171, loss 0.0895504, acc 0.96875\n",
      "2018-07-12T19:21:48.946818: step 3172, loss 0.191723, acc 0.9375\n",
      "2018-07-12T19:21:50.582596: step 3173, loss 0.127, acc 0.921875\n",
      "2018-07-12T19:21:52.224195: step 3174, loss 0.138189, acc 0.9375\n",
      "2018-07-12T19:21:53.882154: step 3175, loss 0.0754804, acc 0.984375\n",
      "2018-07-12T19:21:55.562728: step 3176, loss 0.184431, acc 0.875\n",
      "2018-07-12T19:21:57.209428: step 3177, loss 0.0806974, acc 0.984375\n",
      "2018-07-12T19:21:58.865807: step 3178, loss 0.133806, acc 0.953125\n",
      "2018-07-12T19:22:00.477109: step 3179, loss 0.159314, acc 0.921875\n",
      "2018-07-12T19:22:02.082737: step 3180, loss 0.218698, acc 0.921875\n",
      "2018-07-12T19:22:03.670662: step 3181, loss 0.167773, acc 0.9375\n",
      "2018-07-12T19:22:05.345194: step 3182, loss 0.0864239, acc 0.96875\n",
      "2018-07-12T19:22:06.954854: step 3183, loss 0.185724, acc 0.921875\n",
      "2018-07-12T19:22:08.570856: step 3184, loss 0.135927, acc 0.9375\n",
      "2018-07-12T19:22:10.153958: step 3185, loss 0.146129, acc 0.984375\n",
      "2018-07-12T19:22:11.821397: step 3186, loss 0.220837, acc 0.90625\n",
      "2018-07-12T19:22:13.435941: step 3187, loss 0.0980911, acc 0.984375\n",
      "2018-07-12T19:22:15.103106: step 3188, loss 0.184392, acc 0.953125\n",
      "2018-07-12T19:22:16.701883: step 3189, loss 0.189538, acc 0.90625\n",
      "2018-07-12T19:22:18.259894: step 3190, loss 0.140009, acc 0.921875\n",
      "2018-07-12T19:22:19.897604: step 3191, loss 0.0999134, acc 0.953125\n",
      "2018-07-12T19:22:21.592930: step 3192, loss 0.224522, acc 0.90625\n",
      "2018-07-12T19:22:23.319659: step 3193, loss 0.15125, acc 0.9375\n",
      "2018-07-12T19:22:24.996418: step 3194, loss 0.157846, acc 0.9375\n",
      "2018-07-12T19:22:26.608770: step 3195, loss 0.139448, acc 0.9375\n",
      "2018-07-12T19:22:28.236138: step 3196, loss 0.0971204, acc 0.96875\n",
      "2018-07-12T19:22:29.809100: step 3197, loss 0.328066, acc 0.875\n",
      "2018-07-12T19:22:31.414451: step 3198, loss 0.0923363, acc 0.984375\n",
      "2018-07-12T19:22:33.033897: step 3199, loss 0.195623, acc 0.921875\n",
      "2018-07-12T19:22:34.642493: step 3200, loss 0.185031, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:22:52.574558: step 3200, loss 0.268663, acc 0.886\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3200\n",
      "\n",
      "2018-07-12T19:22:55.154608: step 3201, loss 0.11901, acc 0.953125\n",
      "2018-07-12T19:22:56.814744: step 3202, loss 0.236232, acc 0.90625\n",
      "2018-07-12T19:22:58.444500: step 3203, loss 0.117212, acc 0.984375\n",
      "2018-07-12T19:23:00.067320: step 3204, loss 0.160809, acc 0.90625\n",
      "2018-07-12T19:23:01.673701: step 3205, loss 0.0754492, acc 0.953125\n",
      "2018-07-12T19:23:03.282647: step 3206, loss 0.148518, acc 0.96875\n",
      "2018-07-12T19:23:04.926053: step 3207, loss 0.222528, acc 0.921875\n",
      "2018-07-12T19:23:06.578026: step 3208, loss 0.215659, acc 0.890625\n",
      "2018-07-12T19:23:08.231282: step 3209, loss 0.127658, acc 0.953125\n",
      "2018-07-12T19:23:09.876438: step 3210, loss 0.146621, acc 0.921875\n",
      "2018-07-12T19:23:11.505783: step 3211, loss 0.174456, acc 0.9375\n",
      "2018-07-12T19:23:13.179073: step 3212, loss 0.148223, acc 0.9375\n",
      "2018-07-12T19:23:14.850213: step 3213, loss 0.118713, acc 0.953125\n",
      "2018-07-12T19:23:16.498573: step 3214, loss 0.0617991, acc 0.96875\n",
      "2018-07-12T19:23:18.155249: step 3215, loss 0.184945, acc 0.921875\n",
      "2018-07-12T19:23:19.771975: step 3216, loss 0.187275, acc 0.953125\n",
      "2018-07-12T19:23:21.359885: step 3217, loss 0.188845, acc 0.921875\n",
      "2018-07-12T19:23:22.992347: step 3218, loss 0.109656, acc 0.953125\n",
      "2018-07-12T19:23:24.608359: step 3219, loss 0.0736951, acc 0.96875\n",
      "2018-07-12T19:23:26.173691: step 3220, loss 0.143573, acc 0.9375\n",
      "2018-07-12T19:23:27.861910: step 3221, loss 0.199746, acc 0.921875\n",
      "2018-07-12T19:23:29.486191: step 3222, loss 0.0691148, acc 0.984375\n",
      "2018-07-12T19:23:31.107568: step 3223, loss 0.122477, acc 0.96875\n",
      "2018-07-12T19:23:32.719993: step 3224, loss 0.144689, acc 0.9375\n",
      "2018-07-12T19:23:34.413795: step 3225, loss 0.177717, acc 0.921875\n",
      "2018-07-12T19:23:36.093193: step 3226, loss 0.222274, acc 0.890625\n",
      "2018-07-12T19:23:37.787013: step 3227, loss 0.130326, acc 0.890625\n",
      "2018-07-12T19:23:39.428789: step 3228, loss 0.243122, acc 0.890625\n",
      "2018-07-12T19:23:41.096222: step 3229, loss 0.205276, acc 0.921875\n",
      "2018-07-12T19:23:42.757881: step 3230, loss 0.117423, acc 0.96875\n",
      "2018-07-12T19:23:44.387304: step 3231, loss 0.233793, acc 0.921875\n",
      "2018-07-12T19:23:46.029206: step 3232, loss 0.138384, acc 0.921875\n",
      "2018-07-12T19:23:47.698700: step 3233, loss 0.0982348, acc 0.984375\n",
      "2018-07-12T19:23:49.331231: step 3234, loss 0.113503, acc 0.96875\n",
      "2018-07-12T19:23:50.937366: step 3235, loss 0.0976769, acc 0.9375\n",
      "2018-07-12T19:23:52.623766: step 3236, loss 0.065114, acc 0.984375\n",
      "2018-07-12T19:23:54.285639: step 3237, loss 0.157301, acc 0.9375\n",
      "2018-07-12T19:23:55.868678: step 3238, loss 0.194545, acc 0.890625\n",
      "2018-07-12T19:23:57.562665: step 3239, loss 0.254393, acc 0.859375\n",
      "2018-07-12T19:23:59.360549: step 3240, loss 0.076014, acc 0.96875\n",
      "2018-07-12T19:24:00.966383: step 3241, loss 0.0580324, acc 0.96875\n",
      "2018-07-12T19:24:02.694660: step 3242, loss 0.142723, acc 0.921875\n",
      "2018-07-12T19:24:04.323666: step 3243, loss 0.13132, acc 0.9375\n",
      "2018-07-12T19:24:05.938177: step 3244, loss 0.145526, acc 0.9375\n",
      "2018-07-12T19:24:07.506171: step 3245, loss 0.138735, acc 0.9375\n",
      "2018-07-12T19:24:09.086050: step 3246, loss 0.158403, acc 0.9375\n",
      "2018-07-12T19:24:10.728149: step 3247, loss 0.147924, acc 0.953125\n",
      "2018-07-12T19:24:12.336106: step 3248, loss 0.0682395, acc 0.984375\n",
      "2018-07-12T19:24:14.023695: step 3249, loss 0.0820665, acc 0.96875\n",
      "2018-07-12T19:24:15.600690: step 3250, loss 0.108533, acc 0.953125\n",
      "2018-07-12T19:24:17.176545: step 3251, loss 0.119796, acc 0.953125\n",
      "2018-07-12T19:24:18.851098: step 3252, loss 0.223472, acc 0.890625\n",
      "2018-07-12T19:24:20.412707: step 3253, loss 0.0636676, acc 0.984375\n",
      "2018-07-12T19:24:22.007199: step 3254, loss 0.183315, acc 0.9375\n",
      "2018-07-12T19:24:23.649216: step 3255, loss 0.103914, acc 0.96875\n",
      "2018-07-12T19:24:25.223176: step 3256, loss 0.238847, acc 0.875\n",
      "2018-07-12T19:24:26.791326: step 3257, loss 0.172771, acc 0.921875\n",
      "2018-07-12T19:24:28.416417: step 3258, loss 0.160935, acc 0.9375\n",
      "2018-07-12T19:24:30.044728: step 3259, loss 0.137114, acc 0.953125\n",
      "2018-07-12T19:24:31.712381: step 3260, loss 0.147263, acc 0.953125\n",
      "2018-07-12T19:24:33.334841: step 3261, loss 0.136858, acc 0.921875\n",
      "2018-07-12T19:24:34.943334: step 3262, loss 0.120561, acc 0.953125\n",
      "2018-07-12T19:24:36.612033: step 3263, loss 0.166003, acc 0.9375\n",
      "2018-07-12T19:24:38.266174: step 3264, loss 0.119302, acc 0.953125\n",
      "2018-07-12T19:24:39.950333: step 3265, loss 0.0987036, acc 0.953125\n",
      "2018-07-12T19:24:41.655465: step 3266, loss 0.239495, acc 0.890625\n",
      "2018-07-12T19:24:43.282810: step 3267, loss 0.0736084, acc 0.984375\n",
      "2018-07-12T19:24:44.878119: step 3268, loss 0.0778533, acc 0.984375\n",
      "2018-07-12T19:24:46.535120: step 3269, loss 0.0739248, acc 0.984375\n",
      "2018-07-12T19:24:48.165763: step 3270, loss 0.232659, acc 0.90625\n",
      "2018-07-12T19:24:49.797729: step 3271, loss 0.21499, acc 0.90625\n",
      "2018-07-12T19:24:51.424604: step 3272, loss 0.156395, acc 0.9375\n",
      "2018-07-12T19:24:53.037126: step 3273, loss 0.153626, acc 0.9375\n",
      "2018-07-12T19:24:54.681863: step 3274, loss 0.327256, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:24:56.346870: step 3275, loss 0.118246, acc 0.921875\n",
      "2018-07-12T19:24:57.963750: step 3276, loss 0.12668, acc 0.921875\n",
      "2018-07-12T19:24:59.578690: step 3277, loss 0.37025, acc 0.859375\n",
      "2018-07-12T19:25:01.238066: step 3278, loss 0.24681, acc 0.953125\n",
      "2018-07-12T19:25:02.848565: step 3279, loss 0.18253, acc 0.953125\n",
      "2018-07-12T19:25:04.451976: step 3280, loss 0.24561, acc 0.90625\n",
      "2018-07-12T19:25:06.021377: step 3281, loss 0.0917331, acc 0.96875\n",
      "2018-07-12T19:25:07.631612: step 3282, loss 0.122903, acc 0.96875\n",
      "2018-07-12T19:25:09.230786: step 3283, loss 0.143171, acc 0.953125\n",
      "2018-07-12T19:25:10.771447: step 3284, loss 0.151675, acc 0.921875\n",
      "2018-07-12T19:25:12.443714: step 3285, loss 0.0521269, acc 1\n",
      "2018-07-12T19:25:14.077197: step 3286, loss 0.0979871, acc 0.984375\n",
      "2018-07-12T19:25:15.746051: step 3287, loss 0.107999, acc 0.96875\n",
      "2018-07-12T19:25:17.395053: step 3288, loss 0.121723, acc 0.9375\n",
      "2018-07-12T19:25:19.035403: step 3289, loss 0.248704, acc 0.890625\n",
      "2018-07-12T19:25:20.631031: step 3290, loss 0.151099, acc 0.9375\n",
      "2018-07-12T19:25:22.200411: step 3291, loss 0.109364, acc 0.96875\n",
      "2018-07-12T19:25:23.891095: step 3292, loss 0.15633, acc 0.921875\n",
      "2018-07-12T19:25:25.522075: step 3293, loss 0.11944, acc 0.953125\n",
      "2018-07-12T19:25:27.187895: step 3294, loss 0.148531, acc 0.921875\n",
      "2018-07-12T19:25:28.841215: step 3295, loss 0.182891, acc 0.890625\n",
      "2018-07-12T19:25:30.451570: step 3296, loss 0.271745, acc 0.875\n",
      "2018-07-12T19:25:32.096668: step 3297, loss 0.102054, acc 0.953125\n",
      "2018-07-12T19:25:33.719243: step 3298, loss 0.137863, acc 0.96875\n",
      "2018-07-12T19:25:35.310829: step 3299, loss 0.15123, acc 0.921875\n",
      "2018-07-12T19:25:36.990886: step 3300, loss 0.144908, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:25:54.653330: step 3300, loss 0.271898, acc 0.8852\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3300\n",
      "\n",
      "2018-07-12T19:25:57.138348: step 3301, loss 0.259713, acc 0.921875\n",
      "2018-07-12T19:25:58.757005: step 3302, loss 0.166287, acc 0.921875\n",
      "2018-07-12T19:26:00.445057: step 3303, loss 0.220265, acc 0.9375\n",
      "2018-07-12T19:26:02.193798: step 3304, loss 0.215988, acc 0.90625\n",
      "2018-07-12T19:26:03.862059: step 3305, loss 0.1468, acc 0.953125\n",
      "2018-07-12T19:26:05.442482: step 3306, loss 0.0778083, acc 0.984375\n",
      "2018-07-12T19:26:07.068466: step 3307, loss 0.0886436, acc 0.96875\n",
      "2018-07-12T19:26:08.783215: step 3308, loss 0.122662, acc 0.96875\n",
      "2018-07-12T19:26:10.429880: step 3309, loss 0.21645, acc 0.90625\n",
      "2018-07-12T19:26:12.025383: step 3310, loss 0.0882813, acc 0.953125\n",
      "2018-07-12T19:26:13.609302: step 3311, loss 0.0572004, acc 0.984375\n",
      "2018-07-12T19:26:15.180409: step 3312, loss 0.109526, acc 0.9375\n",
      "2018-07-12T19:26:16.741309: step 3313, loss 0.219833, acc 0.90625\n",
      "2018-07-12T19:26:18.371567: step 3314, loss 0.175068, acc 0.9375\n",
      "2018-07-12T19:26:20.058346: step 3315, loss 0.160619, acc 0.9375\n",
      "2018-07-12T19:26:21.701970: step 3316, loss 0.144096, acc 0.9375\n",
      "2018-07-12T19:26:23.483547: step 3317, loss 0.145494, acc 0.9375\n",
      "2018-07-12T19:26:25.138165: step 3318, loss 0.155816, acc 0.953125\n",
      "2018-07-12T19:26:26.733166: step 3319, loss 0.113044, acc 0.953125\n",
      "2018-07-12T19:26:28.362459: step 3320, loss 0.130631, acc 0.953125\n",
      "2018-07-12T19:26:30.000790: step 3321, loss 0.0994545, acc 0.9375\n",
      "2018-07-12T19:26:31.691026: step 3322, loss 0.207422, acc 0.9375\n",
      "2018-07-12T19:26:33.284995: step 3323, loss 0.0909036, acc 0.96875\n",
      "2018-07-12T19:26:34.907490: step 3324, loss 0.148676, acc 0.953125\n",
      "2018-07-12T19:26:36.568703: step 3325, loss 0.0675191, acc 0.984375\n",
      "2018-07-12T19:26:38.231477: step 3326, loss 0.124487, acc 0.953125\n",
      "2018-07-12T19:26:39.956820: step 3327, loss 0.111747, acc 0.953125\n",
      "2018-07-12T19:26:41.615520: step 3328, loss 0.188894, acc 0.90625\n",
      "2018-07-12T19:26:43.254408: step 3329, loss 0.120048, acc 0.9375\n",
      "2018-07-12T19:26:44.908916: step 3330, loss 0.193351, acc 0.921875\n",
      "2018-07-12T19:26:46.573239: step 3331, loss 0.0881528, acc 0.953125\n",
      "2018-07-12T19:26:48.177762: step 3332, loss 0.191843, acc 0.953125\n",
      "2018-07-12T19:26:49.817745: step 3333, loss 0.183969, acc 0.921875\n",
      "2018-07-12T19:26:51.389187: step 3334, loss 0.127068, acc 0.953125\n",
      "2018-07-12T19:26:53.084551: step 3335, loss 0.183032, acc 0.921875\n",
      "2018-07-12T19:26:54.721457: step 3336, loss 0.169707, acc 0.9375\n",
      "2018-07-12T19:26:56.416587: step 3337, loss 0.199651, acc 0.890625\n",
      "2018-07-12T19:26:58.053619: step 3338, loss 0.146494, acc 0.9375\n",
      "2018-07-12T19:26:59.707100: step 3339, loss 0.14434, acc 0.921875\n",
      "2018-07-12T19:27:01.292709: step 3340, loss 0.155805, acc 0.921875\n",
      "2018-07-12T19:27:02.982823: step 3341, loss 0.233753, acc 0.921875\n",
      "2018-07-12T19:27:04.645836: step 3342, loss 0.0857225, acc 0.984375\n",
      "2018-07-12T19:27:06.258032: step 3343, loss 0.128473, acc 0.953125\n",
      "2018-07-12T19:27:07.862327: step 3344, loss 0.216249, acc 0.9375\n",
      "2018-07-12T19:27:09.565151: step 3345, loss 0.104692, acc 0.953125\n",
      "2018-07-12T19:27:11.216865: step 3346, loss 0.121129, acc 0.953125\n",
      "2018-07-12T19:27:12.879516: step 3347, loss 0.197857, acc 0.953125\n",
      "2018-07-12T19:27:14.474682: step 3348, loss 0.104025, acc 0.984375\n",
      "2018-07-12T19:27:16.103566: step 3349, loss 0.193806, acc 0.953125\n",
      "2018-07-12T19:27:17.824919: step 3350, loss 0.109535, acc 0.953125\n",
      "2018-07-12T19:27:19.415469: step 3351, loss 0.202829, acc 0.921875\n",
      "2018-07-12T19:27:21.001958: step 3352, loss 0.219697, acc 0.875\n",
      "2018-07-12T19:27:22.681648: step 3353, loss 0.0843352, acc 0.953125\n",
      "2018-07-12T19:27:24.454182: step 3354, loss 0.179317, acc 0.875\n",
      "2018-07-12T19:27:26.095981: step 3355, loss 0.232365, acc 0.890625\n",
      "2018-07-12T19:27:27.702935: step 3356, loss 0.110881, acc 0.953125\n",
      "2018-07-12T19:27:29.368858: step 3357, loss 0.130046, acc 0.96875\n",
      "2018-07-12T19:27:30.987762: step 3358, loss 0.16005, acc 0.953125\n",
      "2018-07-12T19:27:32.581912: step 3359, loss 0.153754, acc 0.96875\n",
      "2018-07-12T19:27:34.175357: step 3360, loss 0.0577579, acc 0.984375\n",
      "2018-07-12T19:27:35.793025: step 3361, loss 0.183694, acc 0.921875\n",
      "2018-07-12T19:27:37.439168: step 3362, loss 0.122151, acc 0.9375\n",
      "2018-07-12T19:27:39.051705: step 3363, loss 0.151681, acc 0.9375\n",
      "2018-07-12T19:27:40.656208: step 3364, loss 0.201731, acc 0.890625\n",
      "2018-07-12T19:27:42.323218: step 3365, loss 0.0656251, acc 1\n",
      "2018-07-12T19:27:43.950876: step 3366, loss 0.189054, acc 0.921875\n",
      "2018-07-12T19:27:45.596641: step 3367, loss 0.14769, acc 0.96875\n",
      "2018-07-12T19:27:47.224532: step 3368, loss 0.104257, acc 0.953125\n",
      "2018-07-12T19:27:48.815836: step 3369, loss 0.173097, acc 0.921875\n",
      "2018-07-12T19:27:50.393847: step 3370, loss 0.254005, acc 0.890625\n",
      "2018-07-12T19:27:52.031451: step 3371, loss 0.161743, acc 0.9375\n",
      "2018-07-12T19:27:53.659533: step 3372, loss 0.0953755, acc 0.96875\n",
      "2018-07-12T19:27:55.260701: step 3373, loss 0.13952, acc 0.96875\n",
      "2018-07-12T19:27:56.913851: step 3374, loss 0.0380997, acc 1\n",
      "2018-07-12T19:27:58.530640: step 3375, loss 0.166409, acc 0.921875\n",
      "2018-07-12T19:28:00.147066: step 3376, loss 0.151428, acc 0.9375\n",
      "2018-07-12T19:28:01.726780: step 3377, loss 0.0758882, acc 1\n",
      "2018-07-12T19:28:03.335984: step 3378, loss 0.201664, acc 0.90625\n",
      "2018-07-12T19:28:04.924477: step 3379, loss 0.23963, acc 0.890625\n",
      "2018-07-12T19:28:06.539198: step 3380, loss 0.1674, acc 0.890625\n",
      "2018-07-12T19:28:08.165017: step 3381, loss 0.235776, acc 0.859375\n",
      "2018-07-12T19:28:09.779161: step 3382, loss 0.143907, acc 0.921875\n",
      "2018-07-12T19:28:11.373854: step 3383, loss 0.147181, acc 0.9375\n",
      "2018-07-12T19:28:13.002465: step 3384, loss 0.161404, acc 0.90625\n",
      "2018-07-12T19:28:14.552260: step 3385, loss 0.147142, acc 0.9375\n",
      "2018-07-12T19:28:16.136437: step 3386, loss 0.124492, acc 0.96875\n",
      "2018-07-12T19:28:17.749573: step 3387, loss 0.0983495, acc 0.9375\n",
      "2018-07-12T19:28:19.338743: step 3388, loss 0.15718, acc 0.9375\n",
      "2018-07-12T19:28:20.937916: step 3389, loss 0.246026, acc 0.875\n",
      "2018-07-12T19:28:22.570549: step 3390, loss 0.0897297, acc 0.984375\n",
      "2018-07-12T19:28:24.182336: step 3391, loss 0.178072, acc 0.921875\n",
      "2018-07-12T19:28:25.910717: step 3392, loss 0.176743, acc 0.9375\n",
      "2018-07-12T19:28:27.533461: step 3393, loss 0.176434, acc 0.921875\n",
      "2018-07-12T19:28:29.161371: step 3394, loss 0.211892, acc 0.90625\n",
      "2018-07-12T19:28:30.868278: step 3395, loss 0.244088, acc 0.890625\n",
      "2018-07-12T19:28:32.604319: step 3396, loss 0.206937, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:28:34.321385: step 3397, loss 0.122199, acc 0.9375\n",
      "2018-07-12T19:28:36.013691: step 3398, loss 0.1788, acc 0.9375\n",
      "2018-07-12T19:28:38.006197: step 3399, loss 0.246865, acc 0.921875\n",
      "2018-07-12T19:28:39.783561: step 3400, loss 0.184068, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:28:59.029812: step 3400, loss 0.299358, acc 0.8788\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3400\n",
      "\n",
      "2018-07-12T19:29:01.484022: step 3401, loss 0.165179, acc 0.9375\n",
      "2018-07-12T19:29:03.113235: step 3402, loss 0.285716, acc 0.90625\n",
      "2018-07-12T19:29:04.914419: step 3403, loss 0.127706, acc 0.953125\n",
      "2018-07-12T19:29:06.565870: step 3404, loss 0.0826354, acc 0.984375\n",
      "2018-07-12T19:29:08.308818: step 3405, loss 0.17037, acc 0.921875\n",
      "2018-07-12T19:29:09.895512: step 3406, loss 0.121743, acc 0.96875\n",
      "2018-07-12T19:29:11.474204: step 3407, loss 0.285164, acc 0.890625\n",
      "2018-07-12T19:29:13.093873: step 3408, loss 0.176501, acc 0.9375\n",
      "2018-07-12T19:29:14.721587: step 3409, loss 0.30669, acc 0.90625\n",
      "2018-07-12T19:29:16.429866: step 3410, loss 0.214827, acc 0.90625\n",
      "2018-07-12T19:29:18.104519: step 3411, loss 0.261237, acc 0.921875\n",
      "2018-07-12T19:29:19.799621: step 3412, loss 0.24408, acc 0.9375\n",
      "2018-07-12T19:29:21.484160: step 3413, loss 0.0753341, acc 1\n",
      "2018-07-12T19:29:23.147666: step 3414, loss 0.141016, acc 0.953125\n",
      "2018-07-12T19:29:24.847820: step 3415, loss 0.102309, acc 0.96875\n",
      "2018-07-12T19:29:26.461675: step 3416, loss 0.0929494, acc 0.984375\n",
      "2018-07-12T19:29:28.210493: step 3417, loss 0.139629, acc 0.953125\n",
      "2018-07-12T19:29:29.908346: step 3418, loss 0.0993013, acc 0.953125\n",
      "2018-07-12T19:29:31.699195: step 3419, loss 0.0896765, acc 0.96875\n",
      "2018-07-12T19:29:33.358210: step 3420, loss 0.11631, acc 0.953125\n",
      "2018-07-12T19:29:35.017395: step 3421, loss 0.146663, acc 0.953125\n",
      "2018-07-12T19:29:36.722896: step 3422, loss 0.200718, acc 0.921875\n",
      "2018-07-12T19:29:38.400065: step 3423, loss 0.118066, acc 0.9375\n",
      "2018-07-12T19:29:40.142968: step 3424, loss 0.245221, acc 0.859375\n",
      "2018-07-12T19:29:41.799049: step 3425, loss 0.0904881, acc 0.96875\n",
      "2018-07-12T19:29:43.421762: step 3426, loss 0.135525, acc 0.953125\n",
      "2018-07-12T19:29:45.122600: step 3427, loss 0.156815, acc 0.90625\n",
      "2018-07-12T19:29:46.745204: step 3428, loss 0.123692, acc 0.90625\n",
      "2018-07-12T19:29:48.278641: step 3429, loss 0.139257, acc 0.953125\n",
      "2018-07-12T19:29:49.918628: step 3430, loss 0.0907785, acc 0.96875\n",
      "2018-07-12T19:29:51.620253: step 3431, loss 0.113369, acc 0.96875\n",
      "2018-07-12T19:29:53.250758: step 3432, loss 0.239932, acc 0.90625\n",
      "2018-07-12T19:29:54.934969: step 3433, loss 0.0920372, acc 0.953125\n",
      "2018-07-12T19:29:56.651790: step 3434, loss 0.2404, acc 0.890625\n",
      "2018-07-12T19:29:58.281964: step 3435, loss 0.0699517, acc 0.984375\n",
      "2018-07-12T19:29:59.998120: step 3436, loss 0.10179, acc 0.953125\n",
      "2018-07-12T19:30:01.599789: step 3437, loss 0.0639052, acc 0.96875\n",
      "2018-07-12T19:30:03.220079: step 3438, loss 0.187497, acc 0.921875\n",
      "2018-07-12T19:30:04.842773: step 3439, loss 0.104199, acc 0.9375\n",
      "2018-07-12T19:30:06.443848: step 3440, loss 0.180049, acc 0.953125\n",
      "2018-07-12T19:30:08.062357: step 3441, loss 0.162515, acc 0.921875\n",
      "2018-07-12T19:30:09.668558: step 3442, loss 0.201435, acc 0.875\n",
      "2018-07-12T19:30:11.275924: step 3443, loss 0.0620342, acc 0.984375\n",
      "2018-07-12T19:30:12.879453: step 3444, loss 0.20681, acc 0.90625\n",
      "2018-07-12T19:30:14.547339: step 3445, loss 0.101099, acc 0.96875\n",
      "2018-07-12T19:30:16.199710: step 3446, loss 0.215305, acc 0.921875\n",
      "2018-07-12T19:30:17.843958: step 3447, loss 0.0646342, acc 0.984375\n",
      "2018-07-12T19:30:19.445314: step 3448, loss 0.120729, acc 0.9375\n",
      "2018-07-12T19:30:21.095010: step 3449, loss 0.0673418, acc 0.984375\n",
      "2018-07-12T19:30:22.743548: step 3450, loss 0.0765625, acc 0.953125\n",
      "2018-07-12T19:30:24.420491: step 3451, loss 0.116488, acc 0.96875\n",
      "2018-07-12T19:30:26.058218: step 3452, loss 0.124647, acc 0.9375\n",
      "2018-07-12T19:30:27.663698: step 3453, loss 0.0811101, acc 0.96875\n",
      "2018-07-12T19:30:29.307781: step 3454, loss 0.147399, acc 0.953125\n",
      "2018-07-12T19:30:30.925601: step 3455, loss 0.136442, acc 0.953125\n",
      "2018-07-12T19:30:32.651598: step 3456, loss 0.106314, acc 0.96875\n",
      "2018-07-12T19:30:34.298396: step 3457, loss 0.156754, acc 0.921875\n",
      "2018-07-12T19:30:35.978891: step 3458, loss 0.263813, acc 0.9375\n",
      "2018-07-12T19:30:37.592631: step 3459, loss 0.225002, acc 0.90625\n",
      "2018-07-12T19:30:39.190331: step 3460, loss 0.0552382, acc 0.984375\n",
      "2018-07-12T19:30:40.824770: step 3461, loss 0.0876825, acc 0.953125\n",
      "2018-07-12T19:30:42.469272: step 3462, loss 0.0939995, acc 0.984375\n",
      "2018-07-12T19:30:44.110072: step 3463, loss 0.122556, acc 0.953125\n",
      "2018-07-12T19:30:45.757947: step 3464, loss 0.134722, acc 0.9375\n",
      "2018-07-12T19:30:47.412940: step 3465, loss 0.174644, acc 0.96875\n",
      "2018-07-12T19:30:49.066997: step 3466, loss 0.158885, acc 0.921875\n",
      "2018-07-12T19:30:50.725269: step 3467, loss 0.114408, acc 0.9375\n",
      "2018-07-12T19:30:52.357352: step 3468, loss 0.155441, acc 0.953125\n",
      "2018-07-12T19:30:54.049770: step 3469, loss 0.0902554, acc 0.953125\n",
      "2018-07-12T19:30:55.744070: step 3470, loss 0.040582, acc 1\n",
      "2018-07-12T19:30:57.331265: step 3471, loss 0.203825, acc 0.9375\n",
      "2018-07-12T19:30:58.971153: step 3472, loss 0.196359, acc 0.953125\n",
      "2018-07-12T19:31:00.624318: step 3473, loss 0.108006, acc 0.96875\n",
      "2018-07-12T19:31:02.203752: step 3474, loss 0.08426, acc 0.984375\n",
      "2018-07-12T19:31:03.832731: step 3475, loss 0.183016, acc 0.90625\n",
      "2018-07-12T19:31:05.481476: step 3476, loss 0.112217, acc 0.953125\n",
      "2018-07-12T19:31:07.142884: step 3477, loss 0.328165, acc 0.90625\n",
      "2018-07-12T19:31:08.756016: step 3478, loss 0.320981, acc 0.84375\n",
      "2018-07-12T19:31:10.347203: step 3479, loss 0.303888, acc 0.890625\n",
      "2018-07-12T19:31:12.008229: step 3480, loss 0.183595, acc 0.953125\n",
      "2018-07-12T19:31:13.664318: step 3481, loss 0.266212, acc 0.890625\n",
      "2018-07-12T19:31:15.294142: step 3482, loss 0.153517, acc 0.9375\n",
      "2018-07-12T19:31:16.860328: step 3483, loss 0.125846, acc 0.953125\n",
      "2018-07-12T19:31:18.468376: step 3484, loss 0.080565, acc 0.984375\n",
      "2018-07-12T19:31:20.117271: step 3485, loss 0.0963961, acc 0.953125\n",
      "2018-07-12T19:31:21.815464: step 3486, loss 0.177707, acc 0.90625\n",
      "2018-07-12T19:31:23.484174: step 3487, loss 0.212783, acc 0.90625\n",
      "2018-07-12T19:31:25.138320: step 3488, loss 0.19461, acc 0.921875\n",
      "2018-07-12T19:31:26.804854: step 3489, loss 0.212852, acc 0.921875\n",
      "2018-07-12T19:31:28.464290: step 3490, loss 0.158665, acc 0.96875\n",
      "2018-07-12T19:31:30.207035: step 3491, loss 0.196809, acc 0.9375\n",
      "2018-07-12T19:31:31.858481: step 3492, loss 0.0786496, acc 0.96875\n",
      "2018-07-12T19:31:33.520226: step 3493, loss 0.145616, acc 0.921875\n",
      "2018-07-12T19:31:35.207219: step 3494, loss 0.0754072, acc 0.984375\n",
      "2018-07-12T19:31:36.842999: step 3495, loss 0.102497, acc 0.984375\n",
      "2018-07-12T19:31:38.513345: step 3496, loss 0.140453, acc 0.96875\n",
      "2018-07-12T19:31:40.165879: step 3497, loss 0.169821, acc 0.921875\n",
      "2018-07-12T19:31:41.862488: step 3498, loss 0.147998, acc 0.953125\n",
      "2018-07-12T19:31:43.553461: step 3499, loss 0.17524, acc 0.921875\n",
      "2018-07-12T19:31:45.308452: step 3500, loss 0.180365, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:32:03.890535: step 3500, loss 0.268949, acc 0.8936\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3500\n",
      "\n",
      "2018-07-12T19:32:06.409191: step 3501, loss 0.161722, acc 0.9375\n",
      "2018-07-12T19:32:08.115700: step 3502, loss 0.214459, acc 0.9375\n",
      "2018-07-12T19:32:09.759500: step 3503, loss 0.0838372, acc 0.96875\n",
      "2018-07-12T19:32:11.356421: step 3504, loss 0.176067, acc 0.921875\n",
      "2018-07-12T19:32:12.932203: step 3505, loss 0.166181, acc 0.890625\n",
      "2018-07-12T19:32:14.590234: step 3506, loss 0.152739, acc 0.953125\n",
      "2018-07-12T19:32:16.206270: step 3507, loss 0.160225, acc 0.953125\n",
      "2018-07-12T19:32:17.888801: step 3508, loss 0.0757551, acc 0.96875\n",
      "2018-07-12T19:32:19.558676: step 3509, loss 0.121849, acc 0.96875\n",
      "2018-07-12T19:32:21.205491: step 3510, loss 0.177019, acc 0.921875\n",
      "2018-07-12T19:32:22.764359: step 3511, loss 0.126075, acc 0.953125\n",
      "2018-07-12T19:32:24.382670: step 3512, loss 0.241445, acc 0.90625\n",
      "2018-07-12T19:32:26.014214: step 3513, loss 0.154265, acc 0.9375\n",
      "2018-07-12T19:32:27.620666: step 3514, loss 0.123189, acc 0.9375\n",
      "2018-07-12T19:32:29.227036: step 3515, loss 0.0426401, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:32:30.917283: step 3516, loss 0.260262, acc 0.90625\n",
      "2018-07-12T19:32:32.613115: step 3517, loss 0.131237, acc 0.953125\n",
      "2018-07-12T19:32:34.278953: step 3518, loss 0.17721, acc 0.9375\n",
      "2018-07-12T19:32:35.881477: step 3519, loss 0.118465, acc 0.96875\n",
      "2018-07-12T19:32:36.898921: step 3520, loss 0.230175, acc 0.916667\n",
      "2018-07-12T19:32:38.529467: step 3521, loss 0.153555, acc 0.90625\n",
      "2018-07-12T19:32:40.112433: step 3522, loss 0.0467327, acc 0.96875\n",
      "2018-07-12T19:32:41.736316: step 3523, loss 0.0816384, acc 0.984375\n",
      "2018-07-12T19:32:43.401055: step 3524, loss 0.253243, acc 0.90625\n",
      "2018-07-12T19:32:45.046028: step 3525, loss 0.13338, acc 0.953125\n",
      "2018-07-12T19:32:46.686044: step 3526, loss 0.0848224, acc 0.984375\n",
      "2018-07-12T19:32:48.260419: step 3527, loss 0.153114, acc 0.921875\n",
      "2018-07-12T19:32:49.907479: step 3528, loss 0.110779, acc 0.9375\n",
      "2018-07-12T19:32:51.557573: step 3529, loss 0.0774974, acc 0.96875\n",
      "2018-07-12T19:32:53.216436: step 3530, loss 0.0567385, acc 0.984375\n",
      "2018-07-12T19:32:54.835938: step 3531, loss 0.0752082, acc 0.96875\n",
      "2018-07-12T19:32:56.446670: step 3532, loss 0.0980249, acc 0.984375\n",
      "2018-07-12T19:32:58.072536: step 3533, loss 0.0613871, acc 0.984375\n",
      "2018-07-12T19:32:59.675266: step 3534, loss 0.0610287, acc 0.984375\n",
      "2018-07-12T19:33:01.289037: step 3535, loss 0.0332212, acc 1\n",
      "2018-07-12T19:33:02.906988: step 3536, loss 0.0526402, acc 0.96875\n",
      "2018-07-12T19:33:04.490961: step 3537, loss 0.160365, acc 0.9375\n",
      "2018-07-12T19:33:06.111052: step 3538, loss 0.0741909, acc 0.96875\n",
      "2018-07-12T19:33:07.856147: step 3539, loss 0.0567856, acc 0.984375\n",
      "2018-07-12T19:33:09.580818: step 3540, loss 0.0495782, acc 0.984375\n",
      "2018-07-12T19:33:11.267646: step 3541, loss 0.120365, acc 0.953125\n",
      "2018-07-12T19:33:12.887549: step 3542, loss 0.0279097, acc 1\n",
      "2018-07-12T19:33:14.563717: step 3543, loss 0.097271, acc 0.96875\n",
      "2018-07-12T19:33:16.194521: step 3544, loss 0.095666, acc 0.953125\n",
      "2018-07-12T19:33:17.844305: step 3545, loss 0.125818, acc 0.9375\n",
      "2018-07-12T19:33:19.476435: step 3546, loss 0.0462939, acc 1\n",
      "2018-07-12T19:33:21.097561: step 3547, loss 0.103427, acc 0.96875\n",
      "2018-07-12T19:33:22.696254: step 3548, loss 0.119161, acc 0.953125\n",
      "2018-07-12T19:33:24.310415: step 3549, loss 0.126063, acc 0.9375\n",
      "2018-07-12T19:33:26.086030: step 3550, loss 0.0954169, acc 0.984375\n",
      "2018-07-12T19:33:27.699932: step 3551, loss 0.0630429, acc 0.984375\n",
      "2018-07-12T19:33:29.466608: step 3552, loss 0.157089, acc 0.90625\n",
      "2018-07-12T19:33:31.118108: step 3553, loss 0.0696091, acc 0.96875\n",
      "2018-07-12T19:33:32.703667: step 3554, loss 0.0842659, acc 0.96875\n",
      "2018-07-12T19:33:34.308786: step 3555, loss 0.106773, acc 0.953125\n",
      "2018-07-12T19:33:35.927693: step 3556, loss 0.0609549, acc 0.984375\n",
      "2018-07-12T19:33:37.528467: step 3557, loss 0.154239, acc 0.9375\n",
      "2018-07-12T19:33:39.182521: step 3558, loss 0.126945, acc 0.953125\n",
      "2018-07-12T19:33:40.900226: step 3559, loss 0.183292, acc 0.9375\n",
      "2018-07-12T19:33:42.540158: step 3560, loss 0.0735916, acc 0.984375\n",
      "2018-07-12T19:33:44.158584: step 3561, loss 0.0642757, acc 0.984375\n",
      "2018-07-12T19:33:45.832484: step 3562, loss 0.0847742, acc 0.96875\n",
      "2018-07-12T19:33:47.494406: step 3563, loss 0.192733, acc 0.9375\n",
      "2018-07-12T19:33:49.158584: step 3564, loss 0.0913227, acc 0.953125\n",
      "2018-07-12T19:33:50.827053: step 3565, loss 0.122089, acc 0.953125\n",
      "2018-07-12T19:33:52.426482: step 3566, loss 0.290248, acc 0.84375\n",
      "2018-07-12T19:33:54.086266: step 3567, loss 0.122768, acc 0.921875\n",
      "2018-07-12T19:33:55.740049: step 3568, loss 0.119818, acc 0.953125\n",
      "2018-07-12T19:33:57.408526: step 3569, loss 0.149811, acc 0.9375\n",
      "2018-07-12T19:33:59.110462: step 3570, loss 0.127839, acc 0.953125\n",
      "2018-07-12T19:34:00.710417: step 3571, loss 0.178995, acc 0.9375\n",
      "2018-07-12T19:34:02.417050: step 3572, loss 0.113167, acc 0.96875\n",
      "2018-07-12T19:34:04.039688: step 3573, loss 0.094299, acc 0.96875\n",
      "2018-07-12T19:34:05.658065: step 3574, loss 0.087807, acc 0.953125\n",
      "2018-07-12T19:34:07.392668: step 3575, loss 0.13942, acc 0.921875\n",
      "2018-07-12T19:34:09.089656: step 3576, loss 0.112798, acc 0.953125\n",
      "2018-07-12T19:34:10.746604: step 3577, loss 0.162087, acc 0.921875\n",
      "2018-07-12T19:34:12.406346: step 3578, loss 0.117437, acc 0.9375\n",
      "2018-07-12T19:34:14.041550: step 3579, loss 0.196377, acc 0.90625\n",
      "2018-07-12T19:34:15.740548: step 3580, loss 0.179084, acc 0.90625\n",
      "2018-07-12T19:34:17.448368: step 3581, loss 0.126973, acc 0.921875\n",
      "2018-07-12T19:34:19.242259: step 3582, loss 0.207022, acc 0.90625\n",
      "2018-07-12T19:34:20.973135: step 3583, loss 0.0986542, acc 0.953125\n",
      "2018-07-12T19:34:22.604003: step 3584, loss 0.129069, acc 0.9375\n",
      "2018-07-12T19:34:24.221648: step 3585, loss 0.102893, acc 0.953125\n",
      "2018-07-12T19:34:25.844364: step 3586, loss 0.121152, acc 0.96875\n",
      "2018-07-12T19:34:27.513638: step 3587, loss 0.0928266, acc 0.96875\n",
      "2018-07-12T19:34:29.183905: step 3588, loss 0.0643776, acc 0.96875\n",
      "2018-07-12T19:34:30.963770: step 3589, loss 0.212316, acc 0.90625\n",
      "2018-07-12T19:34:32.705054: step 3590, loss 0.0715563, acc 0.984375\n",
      "2018-07-12T19:34:34.419027: step 3591, loss 0.115109, acc 0.9375\n",
      "2018-07-12T19:34:36.066022: step 3592, loss 0.0546801, acc 0.984375\n",
      "2018-07-12T19:34:37.758206: step 3593, loss 0.0954196, acc 0.9375\n",
      "2018-07-12T19:34:39.387664: step 3594, loss 0.0907934, acc 0.96875\n",
      "2018-07-12T19:34:41.103515: step 3595, loss 0.0863679, acc 0.984375\n",
      "2018-07-12T19:34:42.708341: step 3596, loss 0.147941, acc 0.96875\n",
      "2018-07-12T19:34:44.420470: step 3597, loss 0.156102, acc 0.953125\n",
      "2018-07-12T19:34:46.155338: step 3598, loss 0.16074, acc 0.96875\n",
      "2018-07-12T19:34:47.813018: step 3599, loss 0.214619, acc 0.953125\n",
      "2018-07-12T19:34:49.625016: step 3600, loss 0.111156, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:35:08.027415: step 3600, loss 0.273021, acc 0.8912\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3600\n",
      "\n",
      "2018-07-12T19:35:10.661367: step 3601, loss 0.0520924, acc 1\n",
      "2018-07-12T19:35:12.287125: step 3602, loss 0.0726741, acc 0.984375\n",
      "2018-07-12T19:35:13.953282: step 3603, loss 0.104956, acc 0.953125\n",
      "2018-07-12T19:35:15.586755: step 3604, loss 0.0688111, acc 0.984375\n",
      "2018-07-12T19:35:17.221814: step 3605, loss 0.10665, acc 0.953125\n",
      "2018-07-12T19:35:18.831287: step 3606, loss 0.0403441, acc 1\n",
      "2018-07-12T19:35:20.465295: step 3607, loss 0.232521, acc 0.90625\n",
      "2018-07-12T19:35:22.071524: step 3608, loss 0.0906851, acc 0.953125\n",
      "2018-07-12T19:35:23.797911: step 3609, loss 0.107734, acc 0.953125\n",
      "2018-07-12T19:35:25.427095: step 3610, loss 0.0450764, acc 1\n",
      "2018-07-12T19:35:27.138512: step 3611, loss 0.029066, acc 1\n",
      "2018-07-12T19:35:28.759846: step 3612, loss 0.0702403, acc 0.96875\n",
      "2018-07-12T19:35:30.376409: step 3613, loss 0.197637, acc 0.9375\n",
      "2018-07-12T19:35:32.038894: step 3614, loss 0.163073, acc 0.953125\n",
      "2018-07-12T19:35:33.678296: step 3615, loss 0.159203, acc 0.921875\n",
      "2018-07-12T19:35:35.394780: step 3616, loss 0.0525554, acc 0.96875\n",
      "2018-07-12T19:35:37.078024: step 3617, loss 0.0944277, acc 0.984375\n",
      "2018-07-12T19:35:38.697113: step 3618, loss 0.0587424, acc 0.984375\n",
      "2018-07-12T19:35:40.311269: step 3619, loss 0.131461, acc 0.953125\n",
      "2018-07-12T19:35:41.982343: step 3620, loss 0.0566786, acc 0.984375\n",
      "2018-07-12T19:35:43.627351: step 3621, loss 0.127254, acc 0.953125\n",
      "2018-07-12T19:35:45.321516: step 3622, loss 0.0797044, acc 0.96875\n",
      "2018-07-12T19:35:46.975615: step 3623, loss 0.0371497, acc 0.984375\n",
      "2018-07-12T19:35:48.620980: step 3624, loss 0.0898845, acc 0.96875\n",
      "2018-07-12T19:35:50.257653: step 3625, loss 0.0912733, acc 0.953125\n",
      "2018-07-12T19:35:51.893678: step 3626, loss 0.0774337, acc 0.96875\n",
      "2018-07-12T19:35:53.540180: step 3627, loss 0.0601365, acc 0.96875\n",
      "2018-07-12T19:35:55.185450: step 3628, loss 0.173111, acc 0.9375\n",
      "2018-07-12T19:35:56.813062: step 3629, loss 0.0914721, acc 0.953125\n",
      "2018-07-12T19:35:58.549214: step 3630, loss 0.146824, acc 0.953125\n",
      "2018-07-12T19:36:00.210600: step 3631, loss 0.105784, acc 0.953125\n",
      "2018-07-12T19:36:01.915992: step 3632, loss 0.0765456, acc 0.96875\n",
      "2018-07-12T19:36:03.626288: step 3633, loss 0.109615, acc 0.953125\n",
      "2018-07-12T19:36:05.328578: step 3634, loss 0.0710936, acc 0.96875\n",
      "2018-07-12T19:36:06.937955: step 3635, loss 0.126932, acc 0.921875\n",
      "2018-07-12T19:36:08.567073: step 3636, loss 0.12783, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:36:10.259875: step 3637, loss 0.0502535, acc 0.984375\n",
      "2018-07-12T19:36:11.892845: step 3638, loss 0.0681184, acc 0.984375\n",
      "2018-07-12T19:36:13.557003: step 3639, loss 0.0815325, acc 0.96875\n",
      "2018-07-12T19:36:15.197259: step 3640, loss 0.0630341, acc 0.984375\n",
      "2018-07-12T19:36:16.848680: step 3641, loss 0.170832, acc 0.9375\n",
      "2018-07-12T19:36:18.518794: step 3642, loss 0.0759196, acc 0.96875\n",
      "2018-07-12T19:36:20.149524: step 3643, loss 0.286317, acc 0.890625\n",
      "2018-07-12T19:36:21.760390: step 3644, loss 0.116982, acc 0.9375\n",
      "2018-07-12T19:36:23.432614: step 3645, loss 0.0716173, acc 0.96875\n",
      "2018-07-12T19:36:25.067154: step 3646, loss 0.0978018, acc 0.9375\n",
      "2018-07-12T19:36:26.747841: step 3647, loss 0.0697633, acc 0.953125\n",
      "2018-07-12T19:36:28.406319: step 3648, loss 0.0327857, acc 0.984375\n",
      "2018-07-12T19:36:30.017801: step 3649, loss 0.199724, acc 0.953125\n",
      "2018-07-12T19:36:31.644575: step 3650, loss 0.199603, acc 0.890625\n",
      "2018-07-12T19:36:33.260538: step 3651, loss 0.0793399, acc 0.96875\n",
      "2018-07-12T19:36:34.864673: step 3652, loss 0.0505009, acc 0.96875\n",
      "2018-07-12T19:36:36.536822: step 3653, loss 0.0632076, acc 1\n",
      "2018-07-12T19:36:38.127196: step 3654, loss 0.113692, acc 0.953125\n",
      "2018-07-12T19:36:39.733269: step 3655, loss 0.0598285, acc 1\n",
      "2018-07-12T19:36:41.374167: step 3656, loss 0.113081, acc 0.96875\n",
      "2018-07-12T19:36:42.996241: step 3657, loss 0.0406656, acc 1\n",
      "2018-07-12T19:36:44.593336: step 3658, loss 0.0822085, acc 0.96875\n",
      "2018-07-12T19:36:46.223344: step 3659, loss 0.129595, acc 0.953125\n",
      "2018-07-12T19:36:47.835953: step 3660, loss 0.0952301, acc 0.96875\n",
      "2018-07-12T19:36:49.485013: step 3661, loss 0.117023, acc 0.953125\n",
      "2018-07-12T19:36:51.180986: step 3662, loss 0.0828353, acc 0.953125\n",
      "2018-07-12T19:36:52.833525: step 3663, loss 0.12174, acc 0.953125\n",
      "2018-07-12T19:36:54.500092: step 3664, loss 0.0856234, acc 0.96875\n",
      "2018-07-12T19:36:56.213981: step 3665, loss 0.188782, acc 0.90625\n",
      "2018-07-12T19:36:57.815386: step 3666, loss 0.113599, acc 0.953125\n",
      "2018-07-12T19:36:59.472632: step 3667, loss 0.0788582, acc 0.96875\n",
      "2018-07-12T19:37:01.127391: step 3668, loss 0.138238, acc 0.953125\n",
      "2018-07-12T19:37:02.797874: step 3669, loss 0.127666, acc 0.9375\n",
      "2018-07-12T19:37:04.425356: step 3670, loss 0.0654745, acc 0.953125\n",
      "2018-07-12T19:37:06.040139: step 3671, loss 0.0867862, acc 0.953125\n",
      "2018-07-12T19:37:07.673889: step 3672, loss 0.162613, acc 0.96875\n",
      "2018-07-12T19:37:09.309938: step 3673, loss 0.168195, acc 0.921875\n",
      "2018-07-12T19:37:10.987785: step 3674, loss 0.0921613, acc 0.953125\n",
      "2018-07-12T19:37:12.680859: step 3675, loss 0.0525267, acc 0.96875\n",
      "2018-07-12T19:37:14.392463: step 3676, loss 0.0969212, acc 0.953125\n",
      "2018-07-12T19:37:15.992826: step 3677, loss 0.138527, acc 0.921875\n",
      "2018-07-12T19:37:17.607635: step 3678, loss 0.116469, acc 0.953125\n",
      "2018-07-12T19:37:19.399277: step 3679, loss 0.192975, acc 0.890625\n",
      "2018-07-12T19:37:21.036302: step 3680, loss 0.0986424, acc 0.9375\n",
      "2018-07-12T19:37:22.677401: step 3681, loss 0.123551, acc 0.96875\n",
      "2018-07-12T19:37:24.338067: step 3682, loss 0.065971, acc 0.984375\n",
      "2018-07-12T19:37:25.915765: step 3683, loss 0.120372, acc 0.953125\n",
      "2018-07-12T19:37:27.651356: step 3684, loss 0.182279, acc 0.90625\n",
      "2018-07-12T19:37:29.299614: step 3685, loss 0.140054, acc 0.9375\n",
      "2018-07-12T19:37:30.896337: step 3686, loss 0.0239419, acc 1\n",
      "2018-07-12T19:37:32.561439: step 3687, loss 0.131873, acc 0.9375\n",
      "2018-07-12T19:37:34.159391: step 3688, loss 0.115062, acc 0.953125\n",
      "2018-07-12T19:37:35.774218: step 3689, loss 0.0829693, acc 0.96875\n",
      "2018-07-12T19:37:37.378588: step 3690, loss 0.155191, acc 0.9375\n",
      "2018-07-12T19:37:39.029764: step 3691, loss 0.0739958, acc 0.96875\n",
      "2018-07-12T19:37:40.645642: step 3692, loss 0.139238, acc 0.953125\n",
      "2018-07-12T19:37:42.251717: step 3693, loss 0.0808871, acc 0.9375\n",
      "2018-07-12T19:37:43.849734: step 3694, loss 0.0397538, acc 0.984375\n",
      "2018-07-12T19:37:45.495582: step 3695, loss 0.0660875, acc 0.984375\n",
      "2018-07-12T19:37:47.114046: step 3696, loss 0.0783279, acc 0.953125\n",
      "2018-07-12T19:37:48.755904: step 3697, loss 0.0768689, acc 0.96875\n",
      "2018-07-12T19:37:50.494765: step 3698, loss 0.0974939, acc 0.9375\n",
      "2018-07-12T19:37:52.144281: step 3699, loss 0.127195, acc 0.9375\n",
      "2018-07-12T19:37:53.762109: step 3700, loss 0.0691918, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:38:11.656268: step 3700, loss 0.278298, acc 0.8948\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3700\n",
      "\n",
      "2018-07-12T19:38:14.299443: step 3701, loss 0.106569, acc 0.953125\n",
      "2018-07-12T19:38:15.891059: step 3702, loss 0.0564958, acc 1\n",
      "2018-07-12T19:38:17.570965: step 3703, loss 0.274941, acc 0.859375\n",
      "2018-07-12T19:38:19.174244: step 3704, loss 0.114737, acc 0.953125\n",
      "2018-07-12T19:38:20.836878: step 3705, loss 0.0729446, acc 0.96875\n",
      "2018-07-12T19:38:22.453565: step 3706, loss 0.111633, acc 0.96875\n",
      "2018-07-12T19:38:24.119284: step 3707, loss 0.23849, acc 0.875\n",
      "2018-07-12T19:38:25.788134: step 3708, loss 0.0966141, acc 0.9375\n",
      "2018-07-12T19:38:27.411994: step 3709, loss 0.058091, acc 0.984375\n",
      "2018-07-12T19:38:28.991541: step 3710, loss 0.0496107, acc 1\n",
      "2018-07-12T19:38:30.643795: step 3711, loss 0.215878, acc 0.890625\n",
      "2018-07-12T19:38:32.295236: step 3712, loss 0.0963315, acc 0.96875\n",
      "2018-07-12T19:38:33.975416: step 3713, loss 0.159769, acc 0.921875\n",
      "2018-07-12T19:38:35.587636: step 3714, loss 0.114956, acc 0.953125\n",
      "2018-07-12T19:38:37.281703: step 3715, loss 0.11533, acc 0.96875\n",
      "2018-07-12T19:38:38.895650: step 3716, loss 0.11475, acc 0.953125\n",
      "2018-07-12T19:38:40.567879: step 3717, loss 0.277756, acc 0.859375\n",
      "2018-07-12T19:38:42.256206: step 3718, loss 0.143133, acc 0.953125\n",
      "2018-07-12T19:38:43.950883: step 3719, loss 0.127206, acc 0.9375\n",
      "2018-07-12T19:38:45.721859: step 3720, loss 0.0408231, acc 1\n",
      "2018-07-12T19:38:47.382586: step 3721, loss 0.0595734, acc 1\n",
      "2018-07-12T19:38:49.020352: step 3722, loss 0.0701227, acc 0.984375\n",
      "2018-07-12T19:38:50.614607: step 3723, loss 0.098397, acc 0.984375\n",
      "2018-07-12T19:38:52.255168: step 3724, loss 0.151055, acc 0.921875\n",
      "2018-07-12T19:38:53.940134: step 3725, loss 0.070398, acc 0.984375\n",
      "2018-07-12T19:38:55.598918: step 3726, loss 0.105892, acc 0.984375\n",
      "2018-07-12T19:38:57.232504: step 3727, loss 0.154549, acc 0.9375\n",
      "2018-07-12T19:38:58.925138: step 3728, loss 0.173327, acc 0.9375\n",
      "2018-07-12T19:39:00.578547: step 3729, loss 0.133034, acc 0.984375\n",
      "2018-07-12T19:39:02.280079: step 3730, loss 0.0728873, acc 0.984375\n",
      "2018-07-12T19:39:03.909875: step 3731, loss 0.129363, acc 0.9375\n",
      "2018-07-12T19:39:05.679637: step 3732, loss 0.063793, acc 0.96875\n",
      "2018-07-12T19:39:07.380613: step 3733, loss 0.225949, acc 0.9375\n",
      "2018-07-12T19:39:09.042275: step 3734, loss 0.104255, acc 0.953125\n",
      "2018-07-12T19:39:10.626799: step 3735, loss 0.110877, acc 0.953125\n",
      "2018-07-12T19:39:12.222652: step 3736, loss 0.0749672, acc 0.96875\n",
      "2018-07-12T19:39:13.851927: step 3737, loss 0.09402, acc 0.9375\n",
      "2018-07-12T19:39:15.487068: step 3738, loss 0.0906989, acc 0.96875\n",
      "2018-07-12T19:39:17.118939: step 3739, loss 0.0566329, acc 0.984375\n",
      "2018-07-12T19:39:18.783418: step 3740, loss 0.18926, acc 0.9375\n",
      "2018-07-12T19:39:20.387872: step 3741, loss 0.119242, acc 0.9375\n",
      "2018-07-12T19:39:21.994115: step 3742, loss 0.0793821, acc 0.984375\n",
      "2018-07-12T19:39:23.602550: step 3743, loss 0.178824, acc 0.953125\n",
      "2018-07-12T19:39:25.196742: step 3744, loss 0.203927, acc 0.96875\n",
      "2018-07-12T19:39:26.824524: step 3745, loss 0.118395, acc 0.96875\n",
      "2018-07-12T19:39:28.505937: step 3746, loss 0.122401, acc 0.9375\n",
      "2018-07-12T19:39:30.113603: step 3747, loss 0.0624367, acc 0.984375\n",
      "2018-07-12T19:39:31.750333: step 3748, loss 0.0373715, acc 0.984375\n",
      "2018-07-12T19:39:33.456665: step 3749, loss 0.126003, acc 0.953125\n",
      "2018-07-12T19:39:35.113620: step 3750, loss 0.0383404, acc 0.984375\n",
      "2018-07-12T19:39:36.716246: step 3751, loss 0.0471246, acc 0.984375\n",
      "2018-07-12T19:39:38.323770: step 3752, loss 0.0801852, acc 0.984375\n",
      "2018-07-12T19:39:39.911890: step 3753, loss 0.143668, acc 0.9375\n",
      "2018-07-12T19:39:41.547228: step 3754, loss 0.113995, acc 0.953125\n",
      "2018-07-12T19:39:43.205265: step 3755, loss 0.0502671, acc 0.984375\n",
      "2018-07-12T19:39:44.877169: step 3756, loss 0.153273, acc 0.9375\n",
      "2018-07-12T19:39:46.550169: step 3757, loss 0.0292688, acc 0.984375\n",
      "2018-07-12T19:39:48.173634: step 3758, loss 0.0837009, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:39:49.819293: step 3759, loss 0.134837, acc 0.9375\n",
      "2018-07-12T19:39:51.467737: step 3760, loss 0.139094, acc 0.9375\n",
      "2018-07-12T19:39:53.062644: step 3761, loss 0.0923502, acc 0.984375\n",
      "2018-07-12T19:39:54.660871: step 3762, loss 0.111937, acc 0.953125\n",
      "2018-07-12T19:39:56.279126: step 3763, loss 0.0820715, acc 0.96875\n",
      "2018-07-12T19:39:58.006661: step 3764, loss 0.0660487, acc 0.96875\n",
      "2018-07-12T19:39:59.648148: step 3765, loss 0.107239, acc 0.953125\n",
      "2018-07-12T19:40:01.256358: step 3766, loss 0.0884236, acc 0.96875\n",
      "2018-07-12T19:40:02.964441: step 3767, loss 0.143395, acc 0.9375\n",
      "2018-07-12T19:40:04.603016: step 3768, loss 0.120961, acc 0.953125\n",
      "2018-07-12T19:40:06.312503: step 3769, loss 0.10411, acc 0.96875\n",
      "2018-07-12T19:40:08.044799: step 3770, loss 0.163866, acc 0.9375\n",
      "2018-07-12T19:40:09.770473: step 3771, loss 0.0775496, acc 0.953125\n",
      "2018-07-12T19:40:11.411217: step 3772, loss 0.089556, acc 0.96875\n",
      "2018-07-12T19:40:13.109960: step 3773, loss 0.11958, acc 0.9375\n",
      "2018-07-12T19:40:14.750648: step 3774, loss 0.0650544, acc 0.984375\n",
      "2018-07-12T19:40:16.376446: step 3775, loss 0.184377, acc 0.9375\n",
      "2018-07-12T19:40:18.027959: step 3776, loss 0.15121, acc 0.9375\n",
      "2018-07-12T19:40:19.679698: step 3777, loss 0.0555978, acc 0.984375\n",
      "2018-07-12T19:40:21.319965: step 3778, loss 0.0480318, acc 0.984375\n",
      "2018-07-12T19:40:22.937565: step 3779, loss 0.0925914, acc 0.953125\n",
      "2018-07-12T19:40:24.510938: step 3780, loss 0.0719131, acc 0.984375\n",
      "2018-07-12T19:40:26.129493: step 3781, loss 0.0615866, acc 1\n",
      "2018-07-12T19:40:27.776588: step 3782, loss 0.147918, acc 0.953125\n",
      "2018-07-12T19:40:29.365294: step 3783, loss 0.143714, acc 0.96875\n",
      "2018-07-12T19:40:31.028452: step 3784, loss 0.119524, acc 0.96875\n",
      "2018-07-12T19:40:32.675807: step 3785, loss 0.0967597, acc 0.953125\n",
      "2018-07-12T19:40:34.314843: step 3786, loss 0.0618699, acc 0.984375\n",
      "2018-07-12T19:40:35.909509: step 3787, loss 0.0590168, acc 0.96875\n",
      "2018-07-12T19:40:37.604036: step 3788, loss 0.0777897, acc 0.953125\n",
      "2018-07-12T19:40:39.253781: step 3789, loss 0.18782, acc 0.953125\n",
      "2018-07-12T19:40:40.843817: step 3790, loss 0.0729431, acc 0.984375\n",
      "2018-07-12T19:40:42.434725: step 3791, loss 0.0765568, acc 0.96875\n",
      "2018-07-12T19:40:44.067600: step 3792, loss 0.168746, acc 0.96875\n",
      "2018-07-12T19:40:45.808167: step 3793, loss 0.0943254, acc 0.984375\n",
      "2018-07-12T19:40:47.419563: step 3794, loss 0.105926, acc 0.9375\n",
      "2018-07-12T19:40:49.045279: step 3795, loss 0.134457, acc 0.953125\n",
      "2018-07-12T19:40:50.652200: step 3796, loss 0.237928, acc 0.90625\n",
      "2018-07-12T19:40:52.283458: step 3797, loss 0.0692483, acc 0.953125\n",
      "2018-07-12T19:40:53.890924: step 3798, loss 0.166572, acc 0.953125\n",
      "2018-07-12T19:40:55.488981: step 3799, loss 0.139784, acc 0.9375\n",
      "2018-07-12T19:40:57.145201: step 3800, loss 0.106759, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:41:15.200605: step 3800, loss 0.282462, acc 0.892\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3800\n",
      "\n",
      "2018-07-12T19:41:17.843553: step 3801, loss 0.0698667, acc 0.953125\n",
      "2018-07-12T19:41:19.544359: step 3802, loss 0.109358, acc 0.953125\n",
      "2018-07-12T19:41:21.294099: step 3803, loss 0.0645531, acc 0.96875\n",
      "2018-07-12T19:41:22.927974: step 3804, loss 0.0852799, acc 0.953125\n",
      "2018-07-12T19:41:24.559201: step 3805, loss 0.0986208, acc 0.96875\n",
      "2018-07-12T19:41:26.179226: step 3806, loss 0.096738, acc 0.953125\n",
      "2018-07-12T19:41:27.775082: step 3807, loss 0.0413088, acc 0.984375\n",
      "2018-07-12T19:41:29.400214: step 3808, loss 0.164428, acc 0.9375\n",
      "2018-07-12T19:41:31.054572: step 3809, loss 0.17639, acc 0.890625\n",
      "2018-07-12T19:41:32.607658: step 3810, loss 0.111576, acc 0.9375\n",
      "2018-07-12T19:41:34.187648: step 3811, loss 0.0814862, acc 0.96875\n",
      "2018-07-12T19:41:35.838834: step 3812, loss 0.121779, acc 0.953125\n",
      "2018-07-12T19:41:37.547336: step 3813, loss 0.151756, acc 0.921875\n",
      "2018-07-12T19:41:39.186946: step 3814, loss 0.0509457, acc 0.984375\n",
      "2018-07-12T19:41:40.866977: step 3815, loss 0.299749, acc 0.890625\n",
      "2018-07-12T19:41:42.517109: step 3816, loss 0.0988596, acc 0.96875\n",
      "2018-07-12T19:41:44.154991: step 3817, loss 0.12167, acc 0.953125\n",
      "2018-07-12T19:41:45.851633: step 3818, loss 0.0747834, acc 0.953125\n",
      "2018-07-12T19:41:47.506597: step 3819, loss 0.0730849, acc 0.984375\n",
      "2018-07-12T19:41:49.112583: step 3820, loss 0.098303, acc 0.984375\n",
      "2018-07-12T19:41:50.794695: step 3821, loss 0.0626269, acc 0.953125\n",
      "2018-07-12T19:41:52.375311: step 3822, loss 0.209052, acc 0.921875\n",
      "2018-07-12T19:41:53.995380: step 3823, loss 0.213249, acc 0.96875\n",
      "2018-07-12T19:41:55.657698: step 3824, loss 0.050409, acc 0.984375\n",
      "2018-07-12T19:41:57.323675: step 3825, loss 0.111146, acc 0.96875\n",
      "2018-07-12T19:41:58.952879: step 3826, loss 0.116213, acc 0.953125\n",
      "2018-07-12T19:42:00.575719: step 3827, loss 0.0789774, acc 0.96875\n",
      "2018-07-12T19:42:02.289649: step 3828, loss 0.0925427, acc 0.984375\n",
      "2018-07-12T19:42:03.973055: step 3829, loss 0.122809, acc 0.9375\n",
      "2018-07-12T19:42:05.564296: step 3830, loss 0.147235, acc 0.953125\n",
      "2018-07-12T19:42:07.244497: step 3831, loss 0.0645825, acc 1\n",
      "2018-07-12T19:42:08.911994: step 3832, loss 0.0767341, acc 0.953125\n",
      "2018-07-12T19:42:10.598376: step 3833, loss 0.137097, acc 0.921875\n",
      "2018-07-12T19:42:12.211345: step 3834, loss 0.182799, acc 0.9375\n",
      "2018-07-12T19:42:13.832286: step 3835, loss 0.220276, acc 0.9375\n",
      "2018-07-12T19:42:15.472290: step 3836, loss 0.11704, acc 0.953125\n",
      "2018-07-12T19:42:17.187940: step 3837, loss 0.124661, acc 0.953125\n",
      "2018-07-12T19:42:18.897685: step 3838, loss 0.0529657, acc 1\n",
      "2018-07-12T19:42:20.744417: step 3839, loss 0.110203, acc 0.9375\n",
      "2018-07-12T19:42:22.522126: step 3840, loss 0.111394, acc 0.953125\n",
      "2018-07-12T19:42:24.420447: step 3841, loss 0.0798438, acc 0.953125\n",
      "2018-07-12T19:42:26.038434: step 3842, loss 0.170611, acc 0.953125\n",
      "2018-07-12T19:42:27.732406: step 3843, loss 0.160334, acc 0.9375\n",
      "2018-07-12T19:42:29.376615: step 3844, loss 0.0814421, acc 0.953125\n",
      "2018-07-12T19:42:31.058921: step 3845, loss 0.0917886, acc 0.984375\n",
      "2018-07-12T19:42:32.765732: step 3846, loss 0.110152, acc 0.953125\n",
      "2018-07-12T19:42:34.412672: step 3847, loss 0.19948, acc 0.90625\n",
      "2018-07-12T19:42:36.069897: step 3848, loss 0.123936, acc 0.9375\n",
      "2018-07-12T19:42:37.674140: step 3849, loss 0.0506764, acc 0.984375\n",
      "2018-07-12T19:42:39.298546: step 3850, loss 0.17987, acc 0.9375\n",
      "2018-07-12T19:42:40.959127: step 3851, loss 0.063742, acc 0.984375\n",
      "2018-07-12T19:42:42.659228: step 3852, loss 0.0404887, acc 0.984375\n",
      "2018-07-12T19:42:44.339735: step 3853, loss 0.0679189, acc 0.984375\n",
      "2018-07-12T19:42:45.998423: step 3854, loss 0.078378, acc 0.96875\n",
      "2018-07-12T19:42:47.675149: step 3855, loss 0.185563, acc 0.921875\n",
      "2018-07-12T19:42:49.332880: step 3856, loss 0.0707648, acc 0.984375\n",
      "2018-07-12T19:42:51.005014: step 3857, loss 0.0544285, acc 0.984375\n",
      "2018-07-12T19:42:52.642002: step 3858, loss 0.0873269, acc 0.953125\n",
      "2018-07-12T19:42:54.198976: step 3859, loss 0.209987, acc 0.921875\n",
      "2018-07-12T19:42:55.849395: step 3860, loss 0.083138, acc 0.96875\n",
      "2018-07-12T19:42:57.492204: step 3861, loss 0.12701, acc 0.96875\n",
      "2018-07-12T19:42:59.105512: step 3862, loss 0.0813303, acc 0.953125\n",
      "2018-07-12T19:43:00.734322: step 3863, loss 0.0600815, acc 1\n",
      "2018-07-12T19:43:02.426018: step 3864, loss 0.0467555, acc 0.984375\n",
      "2018-07-12T19:43:04.031965: step 3865, loss 0.136521, acc 0.96875\n",
      "2018-07-12T19:43:05.754551: step 3866, loss 0.0355185, acc 0.984375\n",
      "2018-07-12T19:43:07.386504: step 3867, loss 0.0659771, acc 0.96875\n",
      "2018-07-12T19:43:09.061089: step 3868, loss 0.11722, acc 0.921875\n",
      "2018-07-12T19:43:10.675105: step 3869, loss 0.166144, acc 0.90625\n",
      "2018-07-12T19:43:12.333350: step 3870, loss 0.0388845, acc 1\n",
      "2018-07-12T19:43:14.023823: step 3871, loss 0.189057, acc 0.921875\n",
      "2018-07-12T19:43:15.094097: step 3872, loss 0.0476956, acc 0.972222\n",
      "2018-07-12T19:43:16.732758: step 3873, loss 0.0693967, acc 0.96875\n",
      "2018-07-12T19:43:18.319064: step 3874, loss 0.0408304, acc 1\n",
      "2018-07-12T19:43:20.028321: step 3875, loss 0.077652, acc 0.953125\n",
      "2018-07-12T19:43:21.652190: step 3876, loss 0.0248756, acc 1\n",
      "2018-07-12T19:43:23.275162: step 3877, loss 0.108415, acc 0.96875\n",
      "2018-07-12T19:43:24.872164: step 3878, loss 0.0188952, acc 1\n",
      "2018-07-12T19:43:26.467872: step 3879, loss 0.0894596, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-12T19:43:28.139328: step 3880, loss 0.0582735, acc 0.96875\n",
      "2018-07-12T19:43:29.766966: step 3881, loss 0.036242, acc 1\n",
      "2018-07-12T19:43:31.378733: step 3882, loss 0.0866558, acc 0.953125\n",
      "2018-07-12T19:43:32.968752: step 3883, loss 0.0829065, acc 0.953125\n",
      "2018-07-12T19:43:34.586761: step 3884, loss 0.0771352, acc 0.96875\n",
      "2018-07-12T19:43:36.197991: step 3885, loss 0.0673631, acc 0.96875\n",
      "2018-07-12T19:43:37.847493: step 3886, loss 0.0371459, acc 1\n",
      "2018-07-12T19:43:39.532902: step 3887, loss 0.0654754, acc 0.96875\n",
      "2018-07-12T19:43:41.196504: step 3888, loss 0.090727, acc 0.96875\n",
      "2018-07-12T19:43:42.834148: step 3889, loss 0.0980639, acc 0.953125\n",
      "2018-07-12T19:43:44.487264: step 3890, loss 0.102703, acc 0.953125\n",
      "2018-07-12T19:43:46.108199: step 3891, loss 0.0332321, acc 1\n",
      "2018-07-12T19:43:47.744412: step 3892, loss 0.091625, acc 0.96875\n",
      "2018-07-12T19:43:49.399765: step 3893, loss 0.031875, acc 1\n",
      "2018-07-12T19:43:51.055466: step 3894, loss 0.0558781, acc 0.984375\n",
      "2018-07-12T19:43:52.654549: step 3895, loss 0.0316983, acc 0.984375\n",
      "2018-07-12T19:43:54.261810: step 3896, loss 0.119286, acc 0.96875\n",
      "2018-07-12T19:43:55.853152: step 3897, loss 0.0460517, acc 0.984375\n",
      "2018-07-12T19:43:57.491081: step 3898, loss 0.0522156, acc 0.984375\n",
      "2018-07-12T19:43:59.120270: step 3899, loss 0.0517281, acc 0.984375\n",
      "2018-07-12T19:44:00.790442: step 3900, loss 0.0625527, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-12T19:44:18.879072: step 3900, loss 0.284693, acc 0.8952\n",
      "\n",
      "Saved model checkpoint to /mnt/md1/a503wangsiqi/runs/1531388756/checkpoints/model-3900\n",
      "\n",
      "2018-07-12T19:44:21.467601: step 3901, loss 0.0869345, acc 0.984375\n",
      "2018-07-12T19:44:23.091182: step 3902, loss 0.0478069, acc 0.984375\n",
      "2018-07-12T19:44:24.685850: step 3903, loss 0.0312866, acc 1\n",
      "2018-07-12T19:44:26.225569: step 3904, loss 0.0818672, acc 0.984375\n",
      "2018-07-12T19:44:27.830602: step 3905, loss 0.0965392, acc 0.96875\n",
      "2018-07-12T19:44:29.490055: step 3906, loss 0.0974901, acc 0.953125\n",
      "2018-07-12T19:44:31.140372: step 3907, loss 0.10197, acc 0.9375\n",
      "2018-07-12T19:44:32.747819: step 3908, loss 0.0580314, acc 0.984375\n",
      "2018-07-12T19:44:34.366516: step 3909, loss 0.0661879, acc 0.984375\n",
      "2018-07-12T19:44:36.025008: step 3910, loss 0.0759669, acc 0.96875\n",
      "2018-07-12T19:44:37.611364: step 3911, loss 0.0599073, acc 0.96875\n",
      "2018-07-12T19:44:39.221105: step 3912, loss 0.0604332, acc 0.96875\n",
      "2018-07-12T19:44:40.803201: step 3913, loss 0.0944726, acc 0.96875\n",
      "2018-07-12T19:44:42.432829: step 3914, loss 0.026989, acc 1\n",
      "2018-07-12T19:44:44.148902: step 3915, loss 0.124874, acc 0.9375\n",
      "2018-07-12T19:44:45.796130: step 3916, loss 0.0503467, acc 1\n",
      "2018-07-12T19:44:47.436498: step 3917, loss 0.116254, acc 0.953125\n",
      "2018-07-12T19:44:49.013781: step 3918, loss 0.0584714, acc 0.96875\n",
      "2018-07-12T19:44:50.704647: step 3919, loss 0.0301944, acc 0.984375\n",
      "2018-07-12T19:44:52.490126: step 3920, loss 0.0617158, acc 0.96875\n",
      "2018-07-12T19:44:54.219134: step 3921, loss 0.049083, acc 0.984375\n",
      "2018-07-12T19:44:55.850404: step 3922, loss 0.0705469, acc 0.984375\n",
      "2018-07-12T19:44:57.486507: step 3923, loss 0.0611866, acc 0.96875\n",
      "2018-07-12T19:44:59.108389: step 3924, loss 0.0509611, acc 1\n",
      "2018-07-12T19:45:00.757153: step 3925, loss 0.0511824, acc 0.984375\n",
      "2018-07-12T19:45:02.395470: step 3926, loss 0.1114, acc 0.953125\n",
      "2018-07-12T19:45:04.106483: step 3927, loss 0.0395383, acc 1\n",
      "2018-07-12T19:45:05.757159: step 3928, loss 0.0488405, acc 0.984375\n",
      "2018-07-12T19:45:07.364747: step 3929, loss 0.107796, acc 0.953125\n",
      "2018-07-12T19:45:08.898916: step 3930, loss 0.0371923, acc 0.984375\n",
      "2018-07-12T19:45:10.552028: step 3931, loss 0.0834617, acc 0.96875\n",
      "2018-07-12T19:45:12.142665: step 3932, loss 0.0311214, acc 1\n",
      "2018-07-12T19:45:13.765956: step 3933, loss 0.0180352, acc 1\n",
      "2018-07-12T19:45:15.398756: step 3934, loss 0.0343043, acc 0.984375\n"
     ]
    }
   ],
   "source": [
    "train(x_train, y_train, vocab_processor, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\n",
    "tf.flags.DEFINE_boolean(\"eval_train\", False, \"Evaluate on all training data\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "# CHANGE THIS: Load data. Load your own data here\n",
    "if FLAGS.eval_train:\n",
    "    #x_raw, y_test = load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    x_raw, y_test = load_data_and_labels(pos_reviews, neg_reviews)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    x_raw = [\"a masterpiece four years in the making\", \"everything is off.\"]\n",
    "    y_test = [1, 0]\n",
    "\n",
    "# Map data into vocabulary\n",
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
    "\n",
    "print(\"\\nEvaluating...\\n\")\n",
    "\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))\n",
    "\n",
    "# Save the evaluation to a csv\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
